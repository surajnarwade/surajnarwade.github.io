[{"content":"One of the best way to learn language is to try recreating familiar linux command in it. In one of the previous post, we have seen how we can implement simple which command, you can check the blog post here.\nIn this blog post, we will try to implement mkdir command, Letâ€™s get started.\nmkdir is one of the important commands in Linux which helps to create directories.\nwe will cover following in the blog post:\nmkdir command mkdir with -p flag mkdir with -m flag | Here\u0026rsquo;s the man page for the mkdir command: https://linux.die.net/man/1/mkdir\nmkdir command Letâ€™s try running mkdir command and understand what it does,\nmkdir mydirectory letâ€™s see what it has created,\ndrwxr-xr-x 2 suraj.narwade staff 64 25 Oct 14:22 mydirectory it has created the mydirectory directory with 755 permissions.\nLetâ€™s implement this in Go,\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) func main() { // Check if the user provided a directory name if len(os.Args) \u0026lt; 2 { fmt.Println(\u0026#34;Usage: mkdir directory_name ...\u0026#34;) return } dirName := os.Args[1] // Create the directory err := os.Mkdir(dirName, 0755) if err != nil { fmt.Printf(\u0026#34;Error creating directory: %s\\n\u0026#34;, err) return } } mkdir command with -p flag when -p flag is specified, it will create all the intermediate directories provided as well, for example,\nmkdir -p mydirectory/a/b/c it will create all the directories, mydirectory, a, b, c\nletâ€™s implement this,\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) func main() { if len(os.Args) \u0026lt; 2 { fmt.Println(\u0026#34;Usage: mkdir [-p] directory_name ...\u0026#34;) return } // Check if the -p flag is passed var dirName string createParents := false if os.Args[1] == \u0026#34;-p\u0026#34; { createParents = true if len(os.Args) \u0026lt; 3 { fmt.Println(\u0026#34;Usage: mkdir [-p] directory_name ...\u0026#34;) return } dirName = os.Args[2] } else { dirName = os.Args[1] } // Create the directory (with or without parent directories) var err error if createParents { err = os.MkdirAll(dirName, 0755) // MkdirAll creates parents if necessary } else { err = os.Mkdir(dirName, 0755) } if err != nil { fmt.Printf(\u0026#34;Error creating directory: %s\\n\u0026#34;, err) return } } mkdir command with -m flag Till now, default file permission were 755 but what if we want to set different permissions. we can do so with -m flag. Letâ€™s try it out.\nmkdir -m 700 mydirectory it has created mydirectory with 700 permission.\ndrwx------ 2 suraj.narwade wheel 64 25 Oct 14:37 mydirectory okay, now letâ€™s -m flag and finalise our code for mkdir .\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; ) func main() { if len(os.Args) \u0026lt; 2 { fmt.Println(\u0026#34;Usage: mkdir [-p] [-m mode] directory_name ...\u0026#34;) return } // Default mode and flags mode := os.FileMode(0755) // Default permissions createParents := false dirName := \u0026#34;\u0026#34; // Parsing arguments for flags for i := 1; i \u0026lt; len(os.Args); i++ { arg := os.Args[i] if arg == \u0026#34;-p\u0026#34; { createParents = true } else if arg == \u0026#34;-m\u0026#34; \u0026amp;\u0026amp; i+1 \u0026lt; len(os.Args) { // Parse mode from next argument modeArg := os.Args[i+1] parsedMode, err := strconv.ParseUint(modeArg, 8, 32) if err != nil { fmt.Printf(\u0026#34;Invalid mode: %s\\n\u0026#34;, modeArg) return } mode = os.FileMode(parsedMode) i++ // Skip next argument since it\u0026#39;s the mode } else { dirName = arg } } // Ensure directory name is provided if dirName == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;Error: Directory name is required.\u0026#34;) return } // Create the directory var err error if createParents { err = os.MkdirAll(dirName, mode) // MkdirAll with custom mode } else { err = os.Mkdir(dirName, mode) } if err != nil { fmt.Printf(\u0026#34;Error creating directory: %s\\n\u0026#34;, err) return } } Hope you learnt something new, try it out and let me know if you have any suggestions or any comments.\nTill then, happy coding ðŸ˜Š\n","permalink":"http://localhost:1313/blog/building-mkdir-in-go/","summary":"One of the best way to learn language is to try recreating familiar linux command in it. In one of the previous post, we have seen how we can implement simple which command, you can check the blog post here.\nIn this blog post, we will try to implement mkdir command, Letâ€™s get started.\nmkdir is one of the important commands in Linux which helps to create directories.\nwe will cover following in the blog post:","title":"Building mkdir (linux) command in Go"},{"content":"Iota is predefined identifier which is used in constant declaration in Go. It is used to simplify incrementing value of constants.\niota value always start at 0 and increments automatically by 1 for next constant.\nhereâ€™s the syntax for the iota,\nconst ( a = iota // 0 b = iota // 1 c = iota // 2 ) or you can simply write it as,\nconst ( a = iota b c ) Letâ€™s look at simple example,\npackage main import \u0026#34;fmt\u0026#34; func main() { const ( a = iota // 0 b // 1 c // 2 ) fmt.Println(a, b, c) } output will be,\n0 1 2 Custom increment\nIn some use cases, we might need different kind of increment or start at 1.\nconst ( A = iota * 2 // 0 B // 2 C // 4 ) const ( X = iota + 1 // 1 Y // 2 Z // 3 If you want to skip the particular value, you can do so by putting _ for example,\npackage main import \u0026#34;fmt\u0026#34; func main() { const ( a = iota // 0 _ c // 2 ) fmt.Println(a, c) } output will be,\n0 2 Use cases Enumerated Constants In cases, where we want to assign integer value to constants, enumerations, it is used.\nfor example,\nconst ( Sunday = iota // 0 Monday Tuesday Wednesday Thursday Friday Saturday ) File permissions const ( Read = 1 \u0026lt;\u0026lt; iota // 1 (001) Write // 2 (010) Execute // 4 (100) ) permissions := Read | Execute // You can check if the permission includes \u0026#39;Write\u0026#39;: if permissions\u0026amp;Write != 0 { fmt.Println(\u0026#34;Write permission granted\u0026#34;) } Log levels const ( Debug = iota // 0 Info // 1 Warn // 2 Error // 3 ) logLevel := Warn if logLevel \u0026gt;= Error { fmt.Println(\u0026#34;Critical issue detected!\u0026#34;) } Units of Measurements const ( _ = iota // ignore first value by assigning to blank identifier KB = 1 \u0026lt;\u0026lt; (10 * iota) // 1024 MB // 1024 * 1024 GB // 1024 * 1024 * 1024 ) HTTP codes const ( OK = iota + 200 // 200 Created // 201 Accepted // 202 ) Using iota makes your code cleaner, more maintainable, and less error-prone by eliminating the need to manually assign values in such scenarios.\nIf you know any other use cases which is not mentioned in the blog, do let me know in the comments.\nthank you for reading :)\n","permalink":"http://localhost:1313/blog/understanding-iota-function-in-go/","summary":"Iota is predefined identifier which is used in constant declaration in Go. It is used to simplify incrementing value of constants.\niota value always start at 0 and increments automatically by 1 for next constant.\nhereâ€™s the syntax for the iota,\nconst ( a = iota // 0 b = iota // 1 c = iota // 2 ) or you can simply write it as,\nconst ( a = iota b c ) Letâ€™s look at simple example,","title":"Understanding iota function in Go"},{"content":"As Terraform codebase grows, there are chances where you may need to refactor or rename resources in your codebase which can result into deletion and recreation of resources.\nThis happens because terraform thinks that you want to delete and create new resource and it allocates new id to the resource.\nInstead, you should let terraform know that you want to simply move the resource instead of replacing it.\nTo mimic this we will take simple example where we will have AWS instance with name a and we want to change it to b\nTo avoid that, Terraform already has functionality where you can perform terrraform commands to rename and move the resource inside the state.\nFor example,\nterraform state mv aws_instance.a aws_instance.b But Running terraform commands sometimes can be daunting or may not be available because of security requirement of the organisations.\nEnter moved block The \u0026ldquo;moved\u0026rdquo; configuration block facilitates tracking of resource relocations directly within your configuration. This feature allows you to plan, preview, and confirm resource relocations, ensuring a secure and efficient reorganization of your configuration.\nFor example,\nmoved { from = aws_instance.a to = aws_instance.b } moved block only has two fields:\nfrom : id of the resource to : intended id of the resource once you add this configuration, you can actually plan the changes and safely apply the changes.\nReference https://developer.hashicorp.com/terraform/tutorials/configuration-language/move-config https://developer.hashicorp.com/terraform/cli/state/move https://developer.hashicorp.com/terraform/language/modules/develop/refactoring ","permalink":"http://localhost:1313/blog/refactoring-terraform-easily-with-moved-block/","summary":"As Terraform codebase grows, there are chances where you may need to refactor or rename resources in your codebase which can result into deletion and recreation of resources.\nThis happens because terraform thinks that you want to delete and create new resource and it allocates new id to the resource.\nInstead, you should let terraform know that you want to simply move the resource instead of replacing it.\nTo mimic this we will take simple example where we will have AWS instance with name a and we want to change it to b","title":"Refactoring Terraform easily with moved block"},{"content":"In a previous blog post, we learned how to create and run commands in Go. Now, let\u0026rsquo;s explore how to recreate the following command:\nls -l | grep hello Let\u0026rsquo;s create the first command:\nlsCmd := exec.Command(\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;) Next, create the second command:\ngrepCmd := exec.Command(\u0026#34;grep\u0026#34;, \u0026#34;hello\u0026#34;) To connect them, we need to understand what\u0026rsquo;s happening in the Linux command. Essentially, we are taking the stdout of the first command ls -l and passing it to the stdin of the second command grep hello. To achieve this in Go, follow these steps:\nCreate a pipe for the stdout of the first command: lsOutput, err := lsCmd.StdoutPipe() if err != nil { fmt.Println(\u0026#34;Error creating pipe for ls:\u0026#34;, err) return } Start the first command: if err := lsCmd.Start(); err != nil { fmt.Println(\u0026#34;Error starting ls:\u0026#34;, err) return } Set the stdin of the second command to the stdout of the first: grepCmd.Stdin = lsOutput Define the second command and start it: grepOutput, err := grepCmd.StdoutPipe() if err != nil { fmt.Println(\u0026#34;Error creating pipe for grep:\u0026#34;, err) return } if err := grepCmd.Start(); err != nil { fmt.Println(\u0026#34;Error starting grep:\u0026#34;, err) return } Add wait statements to ensure both commands are complete: lsCmd.Wait() grepCmd.Wait() Now, let\u0026rsquo;s read the output and print it.\ngrepResult, err := io.ReadAll(grepOutput) if err != nil { fmt.Printf(\u0026#34;Error reading command output: %v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;%s\\n\u0026#34;, grepResult) Now let\u0026rsquo;s take a look at the complete program and run it,\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os/exec\u0026#34; ) func main() { // Create the first command: ls -l lsCmd := exec.Command(\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;) // Create the second command: grep hello grepCmd := exec.Command(\u0026#34;grep\u0026#34;, \u0026#34;hello\u0026#34;) // Create a pipe for the stdout of the first command lsOutput, err := lsCmd.StdoutPipe() if err != nil { fmt.Println(\u0026#34;Error creating pipe for ls:\u0026#34;, err) return } // Start the first command if err := lsCmd.Start(); err != nil { fmt.Println(\u0026#34;Error starting ls:\u0026#34;, err) return } // Set the stdin of the second command to the stdout of the first grepCmd.Stdin = lsOutput // Create a pipe for the stdout of the second command grepOutput, err := grepCmd.StdoutPipe() if err != nil { fmt.Println(\u0026#34;Error creating pipe for grep:\u0026#34;, err) return } // Start the second command if err := grepCmd.Start(); err != nil { fmt.Println(\u0026#34;Error starting grep:\u0026#34;, err) return } // Wait for both commands to complete lsCmd.Wait() grepCmd.Wait() // Read and print the output of the second command (grep) grepResult, err := io.ReadAll(grepOutput) if err != nil { fmt.Printf(\u0026#34;Error reading command output: %v\\n\u0026#34;, err) return } fmt.Println(string(grepResult)) } the output will be:\ngo run hello.go -rw-r--r-- 1 surajnarwade staff 805 10 Sep 07:27 hello.go Hope you learnt something interesting, Let me know in the comments :)\nHappy Coding!!!\n","permalink":"http://localhost:1313/blog/piping-command-output-in-go/","summary":"In a previous blog post, we learned how to create and run commands in Go. Now, let\u0026rsquo;s explore how to recreate the following command:\nls -l | grep hello Let\u0026rsquo;s create the first command:\nlsCmd := exec.Command(\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;) Next, create the second command:\ngrepCmd := exec.Command(\u0026#34;grep\u0026#34;, \u0026#34;hello\u0026#34;) To connect them, we need to understand what\u0026rsquo;s happening in the Linux command. Essentially, we are taking the stdout of the first command ls -l and passing it to the stdin of the second command grep hello.","title":"Piping Command Output in Go"},{"content":"Sometimes, there are certain use cases where you may have to run shell commands through a Golang app. This is where the os/exec package comes in handy.\nIf you want to run a command like sleep 1, you can simply do the following:\ncmd := exec.Command(\u0026#34;sleep\u0026#34;, \u0026#34;1\u0026#34;) err := cmd.Run() This will run the command and wait until the execution finishes, but it doesn\u0026rsquo;t capture the stdout.\nIf you intend to capture both stdout and stderr, you can use the following code:\ncmd := exec.Command(\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;) stdoutStderr, err := cmd.CombinedOutput() if err != nil { log.Fatal(err) } It\u0026rsquo;s called \u0026ldquo;combined output\u0026rdquo; because it returns both stdout and stderr.\nIf you are interested in only the stdout, you can do the following:\nout, err := exec.Command(\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;).Output() if err != nil { log.Fatal(\u0026#34;Error:\u0026#34;, err) } fmt.Println(string(out)) Additionally, you can print the command itself (useful for debugging purposes):\ncmd := exec.Command(\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;) fmt.Println(cmd.String()) Happy coding!\n","permalink":"http://localhost:1313/blog/running-shell-commands-in-go/","summary":"Sometimes, there are certain use cases where you may have to run shell commands through a Golang app. This is where the os/exec package comes in handy.\nIf you want to run a command like sleep 1, you can simply do the following:\ncmd := exec.Command(\u0026#34;sleep\u0026#34;, \u0026#34;1\u0026#34;) err := cmd.Run() This will run the command and wait until the execution finishes, but it doesn\u0026rsquo;t capture the stdout.\nIf you intend to capture both stdout and stderr, you can use the following code:","title":"Running Shell Commands in Go using os/exec"},{"content":"One of the best ways to learn a language is via building equivalent Linux commands using the language of your choice.\nwhich is my all-time favourite command to see if the binary is present on the server or not. You can see which usage as shown below,\n$ which kubectl-eks /Users/surajnarwade/.krew/bin/kubectl-eks We simply pass the binary name to which command and it will give us the absolute path to the binary.\n| Here\u0026rsquo;s the man page for the which command: https://linux.die.net/man/1/which\nNow let\u0026rsquo;s try to write it in Golang,\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/exec\u0026#34; ) func main() { path, err := exec.LookPath(os.Args[1]) if err != nil { log.Fatalf(\u0026#34;Error: %s\u0026#34;, err) } fmt.Printf(\u0026#34;%s is available at %s\\n\u0026#34;, os.Args[1], path) } Let\u0026rsquo;s run the program,\n$ go run main.go kubectl-eks kubectl-eks is available at /Users/surajnarwade/.krew/bin/kubectl-eks Let\u0026rsquo;s understand the program step by step,\nIn the following step, we are passing an argument to the program and we are using it as an input to exec.Lookpath the function, Here\u0026rsquo;s the doc link for the function. path, err := exec.LookPath(os.Args[1]) then we simply handle the error and print the path variable. Now, in a similar way, you can try other commands using standard Golang libraries. Let me know in the comments if you tried any.\nHappy Coding!!!\n","permalink":"http://localhost:1313/blog/building-which-linux-command-in-go/","summary":"One of the best ways to learn a language is via building equivalent Linux commands using the language of your choice.\nwhich is my all-time favourite command to see if the binary is present on the server or not. You can see which usage as shown below,\n$ which kubectl-eks /Users/surajnarwade/.krew/bin/kubectl-eks We simply pass the binary name to which command and it will give us the absolute path to the binary.","title":"Building which (Linux) command in Go"},{"content":"Encrypting and decrypting strings using AWS Key Management Service (KMS) and the AWS Command Line Interface (CLI) is a crucial part of securing sensitive data in your AWS environment. In this blog post, we\u0026rsquo;ll walk through the process of encrypting and decrypting a string using KMS and the AWS CLI. This is a fundamental skill for protecting your data in AWS services like S3, RDS, and Lambda.\nRequirement You will need AWS CLI, You can follow this doc to install it, https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html You can follow this guide to create the KMS key, https://docs.aws.amazon.com/kms/latest/developerguide/create-keys.html Encrypting String Let\u0026rsquo;s create a simple plaintext string for encryption purposes. echo \u0026#34;Hello World\u0026#34; \u0026gt; hello.txt Let\u0026rsquo;s run the following command to encrypt our string. If you notice we are outputting it to encrypted.base64 because AWS CLI will encrypt it and then encode it into base64. aws kms encrypt --key-id \u0026lt;KMS_ID\u0026gt; \\ --plaintext fileb://hello.txt \\ --output text --query CiphertextBlob \u0026gt; encrypted.bas64 Now this string is safe to store as only the KMS key can decrypt this.\nDecrypting String For decrypting the string, let\u0026rsquo;s first decode the base64 string. If you check the content of encrypted, it will be a binary blob. cat encrypted.base64 | base64 --decode \u0026gt; encrypted Let\u0026rsquo;s AWS CLI to decrypt the base64 decoded encrypted. You will get another base64 string aws kms decrypt --ciphertext-blob fileb://encrypted --output text --query Plaintext --key-id \u0026lt;KMS_ID\u0026gt; --region eu-west-2 SGVsbG8gV29ybGQK Now we just need to decode the string to get our data string. echo \u0026#34;SGVsbG8gV29ybGQK\u0026#34; | base64 --decode Hello World You can combine the above 2 commands in one command and output the text into a file. aws kms decrypt --ciphertext-blob fileb://encrypted --output text --query Plaintext --key-id \u0026lt;KMS_ID\u0026gt; --region eu-west-2 | base64 --decode \u0026gt; decrypted.tx In this blog post, we\u0026rsquo;ve learned how to encrypt and decrypt a string using AWS KMS and the AWS CLI.\nRemember to manage your KMS keys carefully, control access with IAM policies, and follow AWS security best practices to maintain a robust security posture in your AWS environment.\n","permalink":"http://localhost:1313/blog/encrypting-text-with-aws-kms/","summary":"Encrypting and decrypting strings using AWS Key Management Service (KMS) and the AWS Command Line Interface (CLI) is a crucial part of securing sensitive data in your AWS environment. In this blog post, we\u0026rsquo;ll walk through the process of encrypting and decrypting a string using KMS and the AWS CLI. This is a fundamental skill for protecting your data in AWS services like S3, RDS, and Lambda.\nRequirement You will need AWS CLI, You can follow this doc to install it, https://docs.","title":"Encrypting text with AWS KMS"},{"content":"AWS has more than 200 instance types under EC2. It\u0026rsquo;s very tricky to select appropriate ec2-instance types and surely AWS docs can be daunting.\nYou can check the official doc here: https://aws.amazon.com/ec2/instance-types/\nec2-instance-selector is an open-source project by AWS. It is a CLI tool to helps you select compatible instance types.\nCheck out the repo here,\nhttps://github.com/aws/amazon-ec2-instance-selector\nFeatures You can filter AWS Instance types using criteria like vcpu, memory, network performance, etc. You can use this project as a Go library to embed this functionality into your Golang projects, I will surely embed this in awsctl. Installing ec2-instance-selector On Mac brew tap aws/tap brew install ec2-instance-selector On Linux \u0026amp; Windows, you can download the binary from the release page. verify installation ec2-instance-selector --version v2.4.1 Usage Lookup all the instance type details interactively ec2-instance-selector -o interactive Not all regions have all the instance types, so you can pass the region of your choice via --region flag ec2-instance-selector --region eu-west-2 If you are looking for a specific vcpu or memory count, you can pass flags as shown below, ec2-instance-selector --vcpus 4 --memory 16 g4ad.xlarge g4dn.xlarge g5.xlarge im4gn.xlarge ... ... you can also print in tabular format, âžœ ec2-instance-selector --vcpus 4 --memory 16 -o table Instance Type VCPUs Mem (GiB) ------------- ----- --------- g4ad.xlarge 4 16 g4dn.xlarge 4 16 g5.xlarge 4 16 im4gn.xlarge 4 16 ... ... âžœ ec2-instance-selector --vcpus 4 --memory 16 -o table-wide Instance Type VCPUs Mem (GiB) Hypervisor Current Gen Hibernation Support CPU Arch Network Performance ENIs GPUs GPU Mem (GiB) GPU Info On-Demand Price/Hr Spot Price/Hr (30d avg) ------------- ----- --------- ---------- ----------- ------------------- -------- ------------------- ---- ---- ------------- -------- ------------------ ----------------------- g4ad.xlarge 4 16 nitro true false x86_64 Up to 10 Gigabit 2 1 8 AMD Radeon Pro V520 $0.44271 $0.25325 g4dn.xlarge 4 16 nitro true false x86_64 Up to 25 Gigabit 3 1 16 NVIDIA T4 $0.615 $0.21569 g5.xlarge 4 16 nitro true false x86_64 Up to 10 Gigabit 4 1 24 NVIDIA A10G $1.277 $0.3831 im4gn.xlarge 4 16 nitro true false arm64 Up to 25 Gigabit 4 0 0 none $0.42209 $0.24382 m4.xlarge 4 16 xen false true x86_64 High 4 0 0 none $0.232 $0.11175 ... ... These are just a few examples of ec2-instance-selector but you can do much more such as finding instances by prices, CPU architecture, EBS, etc.\nYou can find this information in the help section,\nec2-instance-selector --help If you find any useful commands with ec2-instance-selector , do let me know in the comment section.\nHave a lovely day :)\n","permalink":"http://localhost:1313/blog/making-a-wise-choice-with-ec2-instance-selector/","summary":"AWS has more than 200 instance types under EC2. It\u0026rsquo;s very tricky to select appropriate ec2-instance types and surely AWS docs can be daunting.\nYou can check the official doc here: https://aws.amazon.com/ec2/instance-types/\nec2-instance-selector is an open-source project by AWS. It is a CLI tool to helps you select compatible instance types.\nCheck out the repo here,\nhttps://github.com/aws/amazon-ec2-instance-selector\nFeatures You can filter AWS Instance types using criteria like vcpu, memory, network performance, etc.","title":"Making a wise choice with ec2-instance-selector"},{"content":"Up until now, we\u0026rsquo;ve been using the DefaultServeMux, but it\u0026rsquo;s time to wield more control and precision by explicitly using the http.ServeMux. Plus, we\u0026rsquo;ll look at the powerful frameworks available, like Gorilla Mux and Echo, that take web server functionality to new heights.\nCheck out previous posts:\nPart 1: https://surajincloud.com/understanding-http-server-in-go-basic Part 2: https://surajincloud.com/understanding-http-server-in-go-using-httpserver-struct Part 3: https://surajincloud.com/understanding-http-server-in-go-handlers Part 4: https://surajincloud.com/understanding-http-server-in-go-multiple-handlers Part 5: https://surajincloud.com/understanding-http-server-in-go-handlefunc Why do we need this? While the DefaultServeMux In Golang does a great job of handling basic routing, there will come a point where you\u0026rsquo;ll want more control over your application\u0026rsquo;s routes. This is where the http.ServeMux steps in. It empowers you to explicitly define how your application responds to different routes, giving you fine-grained control over your server\u0026rsquo;s behaviour.\nSome of the benefits of using custom mux over the default one,\nExplicit Control: When you create your own http.ServeMux, you have complete control over how routes are defined and handled. This allows for more explicit and organized route management, making it easier to understand and maintain your code. Isolation: By using a custom http.ServeMux, you isolate your routes and handlers from other parts of your application. This can help prevent naming clashes and ensure that your route-handling logic is contained within the relevant context. Reusability: Custom muxes enable you to define route handlers as separate functions that can be reused across different applications or parts of your codebase. This promotes modularity and code reuse, as you can easily swap or reuse handlers as needed. Middleware Support: While the default http.ServeMux lacks built-in middleware support, using a custom mux allows you to implement middleware at the mux level. This can include authentication, logging, or any other pre-processing or post-processing logic you need. Fine-Grained Control: With a custom mux, you can design advanced routing scenarios. You might want to conditionally route requests based on headers, query parameters, or other contextual information. This level of control isn\u0026rsquo;t readily achievable with the default mux. Framework Integration: If you plan to use third-party web frameworks like Gorilla Mux or Echo, understanding and working with custom muxes provides a solid foundation. These frameworks often require you to set up custom muxes for enhanced routing and middleware capabilities. Let\u0026rsquo;s dive into the code and see how we can use http.ServeMux to create explicit routes for our web server:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func homeHandler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello World!\u0026#34;) } func aboutHandler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;About Us Page - Coming Soon\u0026#34;) } func main() { mux := http.NewServeMux() // Creating a new ServeMux instance mux.HandleFunc(\u0026#34;/\u0026#34;, helloHandler) mux.HandleFunc(\u0026#34;/about\u0026#34;, aboutHandler) // Creating the server with our custom ServeMux s := http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: mux, // Using our custom ServeMux } s.ListenAndServe() } This modular approach not only makes the code cleaner and more readable, but it also allows for better organization and potential reuse of route handlers.\nbut there are multiple implementations that give extra capabilities. Some of the examples are:\nhttprouter: https://github.com/julienschmidt/httprouter echo: https://github.com/labstack/echo gorilla mux: https://github.com/gorilla/mux We will see more advanced concepts in an upcoming post in our quest to learn more about golang webservers.\nStay curious and keep coding!\n","permalink":"http://localhost:1313/blog/understanding-http-server-in-go-mux/","summary":"Up until now, we\u0026rsquo;ve been using the DefaultServeMux, but it\u0026rsquo;s time to wield more control and precision by explicitly using the http.ServeMux. Plus, we\u0026rsquo;ll look at the powerful frameworks available, like Gorilla Mux and Echo, that take web server functionality to new heights.\nCheck out previous posts:\nPart 1: https://surajincloud.com/understanding-http-server-in-go-basic Part 2: https://surajincloud.com/understanding-http-server-in-go-using-httpserver-struct Part 3: https://surajincloud.com/understanding-http-server-in-go-handlers Part 4: https://surajincloud.com/understanding-http-server-in-go-multiple-handlers Part 5: https://surajincloud.com/understanding-http-server-in-go-handlefunc Why do we need this? While the DefaultServeMux In Golang does a great job of handling basic routing, there will come a point where you\u0026rsquo;ll want more control over your application\u0026rsquo;s routes.","title":"Understanding HTTP Server in Go #6 - Mux"},{"content":"Welcome back to the fifth instalment of our in-depth exploration of creating dynamic web servers using Golang! In this post, we\u0026rsquo;re diving into the fascinating world of HandlerFunc. As you\u0026rsquo;ve come to expect, GoLang provides us with an elegant and powerful way to handle requests through this mechanism. Let\u0026rsquo;s embark on this journey of discovery.\nCheck out previous posts:\nPart 1: https://surajincloud.com/understanding-http-server-in-go-basic\nPart 2: https://surajincloud.com/understanding-http-server-in-go-using-httpserver-struct\nPart 3: https://surajincloud.com/understanding-http-server-in-go-handlers\nPart 4: https://surajincloud.com/understanding-http-server-in-go-multiple-handlers\nLet\u0026rsquo;s dive into the code to uncover the magic behind HandleFunc:\nfunc HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { DefaultServeMux.HandleFunc(pattern, handler) } | You can find the code here: https://pkg.go.dev/net/http#HandleFunc\nAs you can see, HandleFunc takes in a URL pattern and a handler function. It then registers this handler function with the DefaultServeMux, allowing it to be invoked when the corresponding URL is accessed.\nTaking it a step further, let\u0026rsquo;s peek into the HandleFunc method itself:\nfunc (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { if handler == nil { panic(\u0026#34;http: nil handler\u0026#34;) } mux.Handle(pattern, HandlerFunc(handler)) } | You can find the code here: https://pkg.go.dev/net/http#ServeMux.HandleFunc\nHere, HandleFunc within a custom ServeMux instance ensures that the provided handler function is not nil. Once validated, it invokes the underlying Handle method, associating the pattern with an HandlerFunc instance that wraps the provided handler function.\nLet\u0026rsquo;s understand this with the help of example,\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func Handler1(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello World - 1\u0026#34;) } func main() { s := http.Server{ Addr: \u0026#34;:8080\u0026#34;, } http.HandleFunc(\u0026#34;/hello1\u0026#34;, Handler1) s.ListenAndServe() } In this example, we define a Handler1 function that writes \u0026ldquo;Hello World - 1\u0026rdquo; to the response writer. We then use http.HandleFunc to associate this handler with the /test URL path.\nLet\u0026rsquo;s test the code,\n$ curl localhost:8080/hello1 Hello World - 1 We can now assign multiple handlers to paths quite easily.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func Handler1(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello World - 1\u0026#34;) } func Handler2(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello World - 2\u0026#34;) } func main() { s := http.Server{ Addr: \u0026#34;:8080\u0026#34;, } http.HandleFunc(\u0026#34;/hello1\u0026#34;, Handler1) http.HandleFunc(\u0026#34;/hello2\u0026#34;, Handler2) s.ListenAndServe() } With HandlerFunc, handling multiple routes becomes a breeze. By leveraging the power of this mechanism, you can create cleaner, more organized, and easily maintainable code.\nStay tuned for the next part of our journey, where we\u0026rsquo;ll unravel the intricacies of dynamic routing using http.ServeMux. The world of GoLang web servers is evolving before our eyes, and we\u0026rsquo;re equipping you with the knowledge to master it.\nHappy coding!\n","permalink":"http://localhost:1313/blog/understanding-http-server-in-go-handlefunc/","summary":"Welcome back to the fifth instalment of our in-depth exploration of creating dynamic web servers using Golang! In this post, we\u0026rsquo;re diving into the fascinating world of HandlerFunc. As you\u0026rsquo;ve come to expect, GoLang provides us with an elegant and powerful way to handle requests through this mechanism. Let\u0026rsquo;s embark on this journey of discovery.\nCheck out previous posts:\nPart 1: https://surajincloud.com/understanding-http-server-in-go-basic\nPart 2: https://surajincloud.com/understanding-http-server-in-go-using-httpserver-struct\nPart 3: https://surajincloud.com/understanding-http-server-in-go-handlers\nPart 4: https://surajincloud.","title":"Understanding HTTP Server in Go #5 - HandleFunc"},{"content":"In the previous blog post, we made our webserver useful for the first time where we printed Hello World instead of 404.\nCheck out previous blog posts:\nPart 1: https://surajincloud.com/understanding-http-server-in-go-basic\nPart 2: https://surajincloud.com/understanding-http-server-in-go-using-httpserver-struct\nPart 3: https://surajincloud.com/understanding-http-server-in-go-handlers\nIt was still useless as it was printing the same response on all the paths.\nNow in real real-world scenario, we wonâ€™t have just one URL right, We will need multiple paths and handlers and we still donâ€™t have mux so now in this case we can still have multiple handlers\nWe will need to define multiple structs along with their multiple serveHTTP methods\nHere instead of passing one handler in the server struct, we will use the standard function called http.Handle\nLetâ€™s look at http.Handle function\nfunc Handle(pattern string, handler Handler) { DefaultServeMux.Handle(pattern, handler) } | You can find the code for this function here: https://pkg.go.dev/net/http#Handle\nhttp.Handle function will then attach a handler to DefaultServeMux\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) type THandler struct{} func (t *THandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello World\u0026#34;) } func main() { handler := THandler{} s := http.Server{ Addr: \u0026#34;:8080\u0026#34;, } http.Handle(\u0026#34;/hello\u0026#34;, \u0026amp;handler) s.ListenAndServe() } and now you can add more handlers,\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) type Handler1 struct{} func (h *Handler1) ServeHTTP(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello World - 1\u0026#34;) } type Handler2 struct{} func (h *Handler2) ServeHTTP(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello World - 2\u0026#34;) } func main() { handler1 := Handler1{} handler2 := Handler2{} s := http.Server{ Addr: \u0026#34;:8080\u0026#34;, } http.Handle(\u0026#34;/hello1\u0026#34;, \u0026amp;handler1) http.Handle(\u0026#34;/hello2\u0026#34;, \u0026amp;handler2) s.ListenAndServe() } Now let\u0026rsquo;s run the server and check the output,\n$ curl localhost:8080/hello1 Hello World - 1 $ curl localhost:8080/hello2 Hello World - 2 and if we try any other paths, we will receive 404 now,\n$ curl localhost:8080/hello3 404 page not found Now our server is kind of useful and working as intended, but we can further optimise it and make it more advanced, we will see that in upcoming blog posts.\nTill then, Happy Coding !!!\n","permalink":"http://localhost:1313/blog/understanding-http-server-in-go-multiple-handlers/","summary":"In the previous blog post, we made our webserver useful for the first time where we printed Hello World instead of 404.\nCheck out previous blog posts:\nPart 1: https://surajincloud.com/understanding-http-server-in-go-basic\nPart 2: https://surajincloud.com/understanding-http-server-in-go-using-httpserver-struct\nPart 3: https://surajincloud.com/understanding-http-server-in-go-handlers\nIt was still useless as it was printing the same response on all the paths.\nNow in real real-world scenario, we wonâ€™t have just one URL right, We will need multiple paths and handlers and we still donâ€™t have mux so now in this case we can still have multiple handlers","title":"Understanding HTTP Server in Go #4 - multiple handlers"},{"content":"Welcome to the exciting third instalment of our journey into creating web servers using Golang. In our previous articles, we covered the basics of creating a web server and explored the concept of custom servers using the http.Server struct.\nCheck out the previous blog posts:\n\\ Part 1:* https://surajincloud.com/understanding-http-server-in-go-basic\n\\ part 2:* https://surajincloud.com/understanding-http-server-in-go-using-httpserver-struct\nHowever, as you might have noticed, our server was, until now, simply returning a 404 error. Fear not, because in this article, we\u0026rsquo;re about to breathe life into our server by diving into the realm of handlers.\nThe Missing Piece: Handlers So far, our web server setup has been impressive, but it\u0026rsquo;s been missing a crucial element - a way to handle incoming requests effectively. This is where handlers come into play. In GoLang, a handler is an object that implements the ServeHTTP method with the following signature:\nServeHTTP(http.ResponseWriter, *http.Request) Let\u0026rsquo;s write our first handler,\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) type handler struct{} func (h *handler) ServeHTTP(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello World\u0026#34;) } func main() { h := handler{} s := http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: \u0026amp;h, } s.ListenAndServe() } Inside the method, we\u0026rsquo;re using the fmt.Fprintf function to write \u0026ldquo;Hello World\u0026rdquo; to the response writer, effectively sending this text as the response.\nNow if you run this program and try to access it,\n$ curl localhost:8080 Hello World But here\u0026rsquo;s where the real fun begins - the power of routing! If you try accessing /hi or /bye, you\u0026rsquo;ll notice that you still receive the \u0026ldquo;Hello World\u0026rdquo; response. This is because we haven\u0026rsquo;t implemented specific routes yet.\nFor example,\n$ curl localhost:8080/hello Hello World But we didn\u0026rsquo;t define the hello endpoint, ideally, it should have been 404 isn\u0026rsquo;t it, What could be the reason for this, it makes our webserver again useless.\nThis is because we haven\u0026rsquo;t implemented specific routes yet and effectively we are not using any mux here.\nIn the next part of our series, we will see how we can have multiple handlers on our webserver. till then,\nHappy Coding !!!\n","permalink":"http://localhost:1313/blog/understanding-http-server-in-go-handlers/","summary":"Welcome to the exciting third instalment of our journey into creating web servers using Golang. In our previous articles, we covered the basics of creating a web server and explored the concept of custom servers using the http.Server struct.\nCheck out the previous blog posts:\n\\ Part 1:* https://surajincloud.com/understanding-http-server-in-go-basic\n\\ part 2:* https://surajincloud.com/understanding-http-server-in-go-using-httpserver-struct\nHowever, as you might have noticed, our server was, until now, simply returning a 404 error. Fear not, because in this article, we\u0026rsquo;re about to breathe life into our server by diving into the realm of handlers.","title":"Understanding HTTP Server in Go #3 - Handlers"},{"content":"Welcome back to the second part of our exploration into building web servers using GoLang! In the previous blog post, we discussed creating a basic web server using the http.ListenAndServe function.\nHowever, this does not give us more control over the server config and to gain more control and flexibility over the server\u0026rsquo;s behaviour, you\u0026rsquo;re in for a treat. In this article, we\u0026rsquo;ll delve into the world of custom servers by utilizing the http.Server struct.\nThe code is very straightforward,\npackage main import \u0026#34;net/http\u0026#34; func main() { // Creating an instance of http.Server s := http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: nil, // You can replace this with your own handler logic } // Starting the server s.ListenAndServe() } You can find the server struct here: https://pkg.go.dev/net/http#Server\nand the ListenAndServe method on the server struct can be found here: https://pkg.go.dev/net/http#Server.ListenAndServe\nThis is the exact same method that we saw in the previous blog post.\nBasically instead of passing Addr and Handler as an input and then implicitly creating Server struct, we are defining the struct beforehand.\nLet\u0026rsquo;s take a look at this server struct,\ntype Server struct { Addr string Handler Handler DisableGeneralOptionsHandler bool TLSConfig *tls.Config ReadTimeout time.Duration ReadHeaderTimeout time.Duration WriteTimeout time.Duration IdleTimeout time.Duration MaxHeaderBytes int TLSNextProto map[string]func(*Server, *tls.Conn, Handler) ConnState func(net.Conn, ConnState) ErrorLog *log.Logger BaseContext func(net.Listener) context.Context ConnContext func(ctx context.Context, c net.Conn) context.Context inShutdown atomic.Bool disableKeepAlives atomic.Bool nextProtoOnce sync.Once nextProtoErr error mu sync.Mutex listeners map[*net.Listener]struct{} activeConn map[*conn]struct{} onShutdown []func() listenerGroup sync.WaitGroup } Along with Addr and Handler You can now set various timeouts, context, logger, etc\nSimilar to a previous blog post, as you run this code, you will still get 404 As we have not defined any handler as such this server is technically useless but a good example to understand the internals.\nIn the upcoming blog post, we will see how to define a handler and make this server more usable.\nTill then, Happy Coding!!!\n","permalink":"http://localhost:1313/blog/understanding-http-server-in-go-using-httpserver-struct/","summary":"Welcome back to the second part of our exploration into building web servers using GoLang! In the previous blog post, we discussed creating a basic web server using the http.ListenAndServe function.\nHowever, this does not give us more control over the server config and to gain more control and flexibility over the server\u0026rsquo;s behaviour, you\u0026rsquo;re in for a treat. In this article, we\u0026rsquo;ll delve into the world of custom servers by utilizing the http.","title":"Understanding HTTP Server in Go #2 - using http.Server struct"},{"content":"The net/http package in Go offers a powerful set of tools for constructing HTTP servers to handle incoming requests and send responses. In this article, we\u0026rsquo;ll dive into setting up a basic HTTP server.\nBefore we delve into the specifics, let\u0026rsquo;s familiarize ourselves with a couple of key terms:\nMultiplexer (or Mux): This is an essential component of an HTTP server that routes incoming requests to the appropriate handlers based on the requested paths or URLs. we will see this in detail in upcoming posts. With this foundation in mind, let\u0026rsquo;s explore how the net/http package simplifies the process of creating an HTTP server.\nStarting a Basic HTTP Server The net/http package makes it remarkably easy to create an HTTP server. The http.ListenAndServe function serves as a gateway to launching a server. Here\u0026rsquo;s a simple example of its usage:\ngoCopy code package main import ( \u0026#34;net/http\u0026#34; ) func main() { http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } In this example, we use http.ListenAndServe to create a server that listens on port 8080 for incoming requests. The second argument nil signifies that we are using the default multiplexer, known as DefaultServeMux, to handle the incoming requests.\n(second argument is usually a handler but we will see how that works in upcoming blog posts)\nDemystifying ListenAndServe The ListenAndServe function is a convenient way to initiate an HTTP server, but let\u0026rsquo;s explore what\u0026rsquo;s happening under the hood.\nHere\u0026rsquo;s a closer look at the ListenAndServe function:\nRef: https://pkg.go.dev/net/http#ListenAndServe\nfunc ListenAndServe(addr string, handler Handler) error { server := \u0026amp;Server{Addr: addr, Handler: handler} return server.ListenAndServe() } This function constructs a Server struct with the provided address and handler. It then invokes the ListenAndServe method on this struct.\nInside ListenAndServe The ListenAndServe method orchestrates the process of setting up a server:\nRef: https://cs.opensource.google/go/go/+/refs/tags/go1.21.0:src/net/http/server.go;l=2973\nfunc (srv *Server) ListenAndServe() error { if srv.shuttingDown() { return ErrServerClosed } addr := srv.Addr if addr == \u0026#34;\u0026#34; { addr = \u0026#34;:http\u0026#34; } ln, err := net.Listen(\u0026#34;tcp\u0026#34;, addr) if err != nil { return err } return srv.Serve(ln) } This method checks for a shutdown state and determines the address to listen on. If no address is provided, the default is set to port 80 (HTTP). The net.Listen function is then used to create a listener interface for incoming connections on all the network interfaces.\nThe Serving Process The Serve method is pivotal in the server setup:\nRef: https://cs.opensource.google/go/go/+/refs/tags/go1.21.0:src/net/http/server.go;l=3026\nfunc (srv *Server) Serve(l net.Listener) error { // ... (error handling and graceful shutdown code omitted) for { rw, e := l.Accept() // ... (error handling code omitted) go srv.ServeHTTP(rw, req) } } The Serve method continuously accepts incoming connections. Each connection is assigned to a goroutine and the ServeHTTP method is invoked to handle the incoming request.\nNow if you run the simple webserver, you will get 404 what makes this webserver pretty useless but a really good example to understand the internals.\nIn upcoming posts, we will learn further interesting stuff about net/http and running a web server in Golang and also make this webserver more usable.\nHappy Coding !!!\n","permalink":"http://localhost:1313/blog/understanding-http-server-in-go-basic/","summary":"The net/http package in Go offers a powerful set of tools for constructing HTTP servers to handle incoming requests and send responses. In this article, we\u0026rsquo;ll dive into setting up a basic HTTP server.\nBefore we delve into the specifics, let\u0026rsquo;s familiarize ourselves with a couple of key terms:\nMultiplexer (or Mux): This is an essential component of an HTTP server that routes incoming requests to the appropriate handlers based on the requested paths or URLs.","title":"Understanding HTTP Server in Go #1 - Basic"},{"content":"While working with route53, I came across a couple of DNS records which were new to me.\nThere are several types of DNS (Domain Name System) records, each with a specific purpose. Here are some of the most common types:\nA record (Address record) Maps a hostname to an IPv4 address.\nexample.com IN A 192.0.2.1 AAAA record (IPv6 Address record) Maps a hostname to an IPv6 address.\nexample.com IN AAAA 2001:db8::1 CNAME record (Canonical Name record) Maps an alias hostname to the true hostname (canonical name).\nwww.example.com IN CNAME example.com MX record (Mail Exchange record) Specifies the mail servers that should be used to handle email for a particular domain.\nexample.com IN MX 10 mailserver.example.com NS record (Name Server record) Specifies the authoritative DNS servers for a particular domain.\nexample.com IN NS ns1.example.com PTR record (Pointer record) Maps an IP address to a hostname (reverse DNS lookup).\n1.2.3.4.in-addr.arpa IN PTR example.com SOA record (Start of Authority record) Specifies a domain\u0026rsquo;s primary authoritative DNS server and provides information about the zone.\nexample.com IN SOA ns1.example.com admin.example.com 2019032101 7200 3600 1209600 3600 TXT record (Text record) Stores arbitrary text data associated with a domain.\nexample.com IN TXT \u0026#34;v=spf1 mx -all\u0026#34; SRV records it is also known as Service records. it is useful for services that operate on non-standard ports or for load balancing between multiple servers providing the same service. it is used to define the hostname, port number, priority, and weight of a server that provides a particular service for a domain.\n_Service._Proto.Name TTL Class Priority Weight Port Target _imaps._tcp.example.com. 3600 IN SRV 10 5 993 mailserver.example.com. Hope this was helpful, Happy Hacking :)\n","permalink":"http://localhost:1313/blog/types-of-dns-records/","summary":"While working with route53, I came across a couple of DNS records which were new to me.\nThere are several types of DNS (Domain Name System) records, each with a specific purpose. Here are some of the most common types:\nA record (Address record) Maps a hostname to an IPv4 address.\nexample.com IN A 192.0.2.1 AAAA record (IPv6 Address record) Maps a hostname to an IPv6 address.\nexample.com IN AAAA 2001:db8::1 CNAME record (Canonical Name record) Maps an alias hostname to the true hostname (canonical name).","title":"Types of DNS records"},{"content":"fmt is one of the essential packages in Golang. fmt stands for \u0026ldquo;format.\u0026rdquo; This package allows us to format strings, print output, and interact with the standard input/output streams.\nIn this blog post, we will explore the key functionalities of the fmt package.\nyou can import the package as shown:\nimport \u0026#34;fmt\u0026#34; Printing to standard output There are various functions available for printing such as fmt.Println , fmt.Print and fmt.Print\nPrintln This will print a line with a new line at the end\nfmt.Println(\u0026#34;Hello World\u0026#34;) output:\nHello, World! Print This will print but no newline at the end\noutput:\nfmt.Print(\u0026#34;Hello World\u0026#34;) Printf This will format as per the format specifier and print without a newline at the end\nfmt.Printf(\u0026#34;My name is %s.\\\\n\u0026#34;, name) Here are some examples of format specifiers,\n%s: string %d: integer %f: float %t: boolean %v: any value based on its type Note: One of the noticeable differences between print and println is that print adds Spaces between operands and println doesn\u0026rsquo;t.\nfor example,\nfmt.Println(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) fmt.Print(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) the output will be:\nhello world helloworld Formatting without printing using Sprint you can format the string and store it in a variable without printing to standard output, some of the available options are:\nSprint This will store the string in the given variable\nvalue := fmt.Sprint(\u0026#34;Hello World\u0026#34;) Sprintf This will format the string as per the format specifier and store it in the given variable\nvalue := fmt.Sprintf(\u0026#34;My name is %s\u0026#34;, name) Sprintln This will format the string as per the format specifier, add a new line at the end and store it in the given variable\nvalue := fmt.Sprintln(\u0026#34;My name is %s\u0026#34;, name) Reading input For reading user input from the standard input or console, we can use Scanf , Scan , Scanln from the fmt package\nScan this reads space-separated values until a whitespace is encountered\nfmt.Print(\u0026#34;Enter your name: \u0026#34;) fmt.Scan(\u0026amp;name) Scanln this reads a line of text until a newline is encountered\nfmt.Print(\u0026#34;Enter your age: \u0026#34;) fmt.Scanln(\u0026amp;age) Scanf this reads formatted input based on the format specifier\nfmt.Print(\u0026#34;Enter your age: \u0026#34;) fmt.Scanf(\u0026#34;%d\u0026#34;, \u0026amp;age) Writing to the specific output stream using Fprint, we can format and write data to a specific output stream, such as a file or an io.Writer interface (for example, write formatted data to the response writer (http.ResponseWriter) when handling HTTP requests)\nFprint fmt.Fprint(os.Stdout, \u0026#34;Hello world\u0026#34;) Fprintln fmt.Fprintln(os.Stdout, \u0026#34;Hello world\u0026#34;) Fprintf func testHandler(w http.ResponseWriter, r *http.Request) { message := \u0026#34;Hello World\u0026#34; fmt.Fprintf(w, \u0026#34;%s\u0026#34;, message) } There is much more interesting stuff in fmt, but we will see that in upcoming blogs. Till then,\nHappy Coding :)\n","permalink":"http://localhost:1313/blog/fmt-in-golang/","summary":"fmt is one of the essential packages in Golang. fmt stands for \u0026ldquo;format.\u0026rdquo; This package allows us to format strings, print output, and interact with the standard input/output streams.\nIn this blog post, we will explore the key functionalities of the fmt package.\nyou can import the package as shown:\nimport \u0026#34;fmt\u0026#34; Printing to standard output There are various functions available for printing such as fmt.Println , fmt.Print and fmt.Print","title":"fmt in Golang - formatting I/O"},{"content":"working with open-source Go modules is more straightforward, you add them in Go mod files and run Go mod commands and it just works.\nRecently, I came across private Go modules usage in one of my projects at work and thought of writing about it so it can help someone :)\nif you have only one repo then you can do\nexport GOPRIVATE=github.com/yourorg/yourrepo If you have more than one private module, you can run the following:\nexport GOPRIVATE=github.com/yourorg/* If you are using the SSH key to connect to Github, you will have to add the following snippet in ~/.gitconfig\n[url \u0026#34;ssh://git@github.com/\u0026#34;] insteadOf = https://github.com/ once everything is set,\ngo get -u github.com/yourorg/yourrepo if you want to pull a specific package, you can do,\ngo get -u github.com/yourrepo/yourorg/yourdir Happy Coding !!!\n","permalink":"http://localhost:1313/blog/using-go-modules-with-a-private-github-repository/","summary":"working with open-source Go modules is more straightforward, you add them in Go mod files and run Go mod commands and it just works.\nRecently, I came across private Go modules usage in one of my projects at work and thought of writing about it so it can help someone :)\nif you have only one repo then you can do\nexport GOPRIVATE=github.com/yourorg/yourrepo If you have more than one private module, you can run the following:","title":"Using Go modules with a Private GitHub repository"},{"content":"Unlike other Languages, Golang does not have multiple looping constructs. It has only one for.\nBut, I think that\u0026rsquo;s enough to cover all the use cases, let\u0026rsquo;s see different variations of loops.\nBasic Loop In basic for loop, there are 3 elements,\ninitialization: i := 1 condition: i\u0026lt;=5 post: i++ loop will continue to run until the postcondition is true.\nfor i := 1; i \u0026lt;= 5; i++ { fmt.Println(i) } output:\n1 2 3 4 5 While loop a loop will continue to run until the condition (in this case n\u0026lt;5 ) is true\nn := 1 for n \u0026lt; 5 { n = n + 1 } fmt.Println(n) output:\n5 Infinite loop n := 0 for { n = n + 1 } fmt.Println(\u0026#34;hello\u0026#34;,n) there will be no output as the loop will never end.\nLoop over Slice or Array when looping over a slice or array, n is an index (starting from 0) of the element i\na := []string{\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;} for n, i := range a { fmt.Println(i, s) } output:\n0 hello 1 world in some use cases, you may not need the index, then you can simply ignore it as shown,\na := []string{\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;} for _, i := range a { fmt.Println(s) } output:\nhello world Loop over Maps m := map[string]int{ \u0026#34;london\u0026#34;: 1, \u0026#34;newyork\u0026#34;: 2, \u0026#34;dublin\u0026#34;: 3, } for k, v := range m { fmt.Println(k, v) } output:\nlondon 1 newyork 2 dublin 3 Breaking the loop In all the for loops we have seen above (especially infinite loop) sometimes there can be use cases where we want to break out of the loop if a certain condition is met.\nwe can do this with the help of break a statement. Let\u0026rsquo;s take the same infinite loop example to understand this, but you can use this in any for loop variations.\nn := 0 for { n = n + 1 if n == 5 { break } } fmt.Println(\u0026#34;hello\u0026#34;,n) now there will be output,\nhello 5 Skipping the iteration sometimes, you may want to skip the given iteration based on some condition, you can do that with the help of continue statement, for example,\nfor i := 1; i \u0026lt;= 5; i++ { if i == 4 { continue } fmt.Println(i) } output:\n1 2 3 5 Conclusion \u0026lsquo;For\u0026rsquo; loops are an essential component of any programming language, including Go. In this blog post, we covered the basic syntax of \u0026lsquo;for\u0026rsquo; loops, infinite loops, iteration over arrays and slices, using \u0026lsquo;continue\u0026rsquo; to skip iterations, and utilizing \u0026lsquo;break\u0026rsquo; to exit a loop prematurely. With this knowledge, you will be able to confidently use \u0026lsquo;for\u0026rsquo; loops to solve various use cases in Go.\nHappy coding!\n","permalink":"http://localhost:1313/blog/for-loop-in-golang/","summary":"Unlike other Languages, Golang does not have multiple looping constructs. It has only one for.\nBut, I think that\u0026rsquo;s enough to cover all the use cases, let\u0026rsquo;s see different variations of loops.\nBasic Loop In basic for loop, there are 3 elements,\ninitialization: i := 1 condition: i\u0026lt;=5 post: i++ loop will continue to run until the postcondition is true.\nfor i := 1; i \u0026lt;= 5; i++ { fmt.Println(i) } output:","title":"For loop in Golang"},{"content":"In this post, we will see different Kubernetes clients, the client is something that you can use to talk to the Kubernetes cluster.\nMainly, there are 3 ways to talk to a cluster:\nkubectl Console/Dashboard programmatically using clients The Kubernetes community maintains clients in various languages in this repo.\nSince Kubernetes is in Golang, a client library known as client-go is widely used. But there are clients available in other interesting languages :)\nPython Ruby C Sharp Perl Haskell Javascript Java for Java, there\u0026rsquo;s one more client known as fabric8 kubernetes client which is also widely used and famous too :) (it works with OpenShift too)\nIn the upcoming blog post, we will see how we can leverage client-go effectively to talk to the cluster and create some tools using it.\n","permalink":"http://localhost:1313/blog/introduction-to-kubernetes-clients/","summary":"In this post, we will see different Kubernetes clients, the client is something that you can use to talk to the Kubernetes cluster.\nMainly, there are 3 ways to talk to a cluster:\nkubectl Console/Dashboard programmatically using clients The Kubernetes community maintains clients in various languages in this repo.\nSince Kubernetes is in Golang, a client library known as client-go is widely used. But there are clients available in other interesting languages :)","title":"Introduction to Kubernetes Clients"},{"content":"Terraform is a popular tool for Infrastructure as a Code practice to manage Cloud Infrastructure using declarative ways.\nAs Infrastructure grows, the number of projects grows, and there\u0026rsquo;s a fair chance that you may not have the same terraform versions among your projects.\nThere can be various reasons for it. As a result, you may need multiple terraform versions installed on your machine, which can get a little tricky.\nWorry not. There\u0026rsquo;s an open-source project to solve this problem tfenv.\nWhat is tfenv? Tfenv is a simple command-line tool that helps you manage multiple versions of Terraform. It allows you to easily switch between different versions of Terraform, depending on your requirements.\nInstalling tfenv On Mac\nInstall using brew brew install tfenv Manual Installation\nYou can use following\n$ git clone --depth=1 https://github.com/tfutils/tfenv.git ~/.tfenv $ echo \u0026#39;export PATH=$PATH:$HOME/.tfenv/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc Restart the Terminal, and you are set to use tfenv\nVerify the Installation Run the following command to verify the installation. It should also show the output as shown below. $ tfenv tfenv 3.0.0 Usage: tfenv \u0026lt;command\u0026gt; [\u0026lt;options\u0026gt;] Commands: install Install a specific version of Terraform use Switch a version to use uninstall Uninstall a specific version of Terraform list List all installed versions list-remote List all installable versions version-name Print current version init Update environment to use tfenv correctly. pin Write the current active version to ./.terraform-version Usage Once tfenv is installed, you are all set to manage multiple terraform versions.\nInstalling specific versions of Terraform $ tfenv install 1.3.6 Installing Terraform v1.3.6 Downloading release tarball from https://releases.hashicorp.com/terraform/1.3.6/terraform_1.3.6_linux_amd64.zip ####################################################################################################################### 100.0% Downloading SHA hash file from https://releases.hashicorp.com/terraform/1.3.6/terraform_1.3.6_SHA256SUMS Not instructed to use Local PGP (/home/suraj/.tfenv/use-{gpgv,gnupg}) \u0026amp; No keybase install found, skipping OpenPGP signature verification terraform_1.3.6_linux_amd64.zip: OK Archive: /tmp/tfenv_download.Xvaw0x/terraform_1.3.6_linux_amd64.zip inflating: /home/suraj/.tfenv/versions/1.3.6/terraform Installation of terraform v1.3.6 successful. To make this your default version, run \u0026#39;tfenv use 1.3.6\u0026#39; List all the available installed terraform versions $ tfenv list 1.3.9 1.3.8 * 1.3.7 (set by /home/suraj/.tfenv/version) It will show you all the available versions and * will show the current terraform version.\nYou can also print the current terraform version using the following command, $ tfenv version-name 1.3.7 If you want to switch to another version, you can do as shown below, $ tfenv use 1.3.8 Switching default version to v1.3.8 Default version (when not overridden by .terraform-version or TFENV_TERRAFORM_VERSION) is now: 1.3.8 uninstalling specific versions of terraform $ tfenv uninstall 1.3.6 Uninstall Terraform v1.3.6 Terraform v1.3.6 is successfully uninstalled I hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/how-to-manage-multiple-versions-of-terraform/","summary":"Terraform is a popular tool for Infrastructure as a Code practice to manage Cloud Infrastructure using declarative ways.\nAs Infrastructure grows, the number of projects grows, and there\u0026rsquo;s a fair chance that you may not have the same terraform versions among your projects.\nThere can be various reasons for it. As a result, you may need multiple terraform versions installed on your machine, which can get a little tricky.\nWorry not.","title":"How to manage multiple versions of Terraform?"},{"content":"In a previous blog post, we have seen how to enable SSM Session for EC2 instances to ditch SSH and enable safe and secure shell access.\nIn this blog post, we will go one step further towards security and encrypting our SSM sessions from EC2-managed nodes and the local machines of users with the help of the KMS key.\nCreating the KMS key To encrypt the sessions in SSM, we will first need to have the KMS key. aws kms create-key --description \u0026#34;SSM Session Manager Key\u0026#34; --origin AWS_KMS This will create a KMS key for us.\nOr you can use Terraform to create KMS keys.\nAdding appropriate permissions to the EC2 instance. You must create an IAM policy with the following permissions and attach it to the IAM role attached to the EC2 instance.\nkms:Encrypt Enable KMS encryption in SSM Session Manager. Go to the AWS Systems Manager console, https://console.aws.amazon.com/systems-manager/. Select Session Manager from the navigation. Choose the Preferences tab, and then choose Edit. Select the check box next to Enable KMS encryption. Select the existing KMS key from the list or fill the box with KMS Key ARN (you can also click on create a key, which will take you to the KMS Console) Choose Save. I hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/encrypting-ec2-sessions-with-kms-and-ssm-session-managers/","summary":"In a previous blog post, we have seen how to enable SSM Session for EC2 instances to ditch SSH and enable safe and secure shell access.\nIn this blog post, we will go one step further towards security and encrypting our SSM sessions from EC2-managed nodes and the local machines of users with the help of the KMS key.\nCreating the KMS key To encrypt the sessions in SSM, we will first need to have the KMS key.","title":"Encrypting EC2 Sessions with KMS and SSM Session Managers"},{"content":"To access EC2 instances, SSH has been the preferred way for many years. But this way has its downsides, such as managing the keypairs, etc.\nRecently, I came across a new safe and secure way to access instances, even from a Web browser and from the terminal, which is SSM. Let\u0026rsquo;s learn more about SSM and how to set it up.\nWhat is SSM?\nSSM stands for Systems Manager, a set of tools such as Session manager, State manager, patch manager, etc.\nAmong all of them, we are going to focus on Session Manager as a replacement for SSH.\nWhat is SSM Session Manager?\nSession Manager is a feature of AWS Systems Manager (SSM) that allows you to establish secure, bi-directional connectivity between your local computer and an Amazon Elastic Compute Cloud (EC2) instance using a web-based shell or the AWS CLI.\nKey features of SSM Session Manager\nSome of the key features of Session Manager are as follows:\nSecure communication using IAM and SSL. Fine-grained access control: With the IAM policy\u0026rsquo;s help, you can control who has access to start sessions and require MFA for added security. Audit: It logs all session activity, providing a complete audit trail of who accessed an instance and what actions were performed. Supported Operating Systems:\nAmazon Linux 2 Amazon Linux Ubuntu Server Red Hat Enterprise Linux (RHEL) SUSE Linux Enterprise Server (SLES) How does it work?\nYou will need to install the SSM agent on your EC2 machine. Amazon Linux or EKS Optimised AMIs comes with pre-installed SSM agents.\nRefer to this documentation to install SSM Agent: https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-manual-agent-install.html.\nVerify that the agent is installed,\nsudo systemctl status amazon-ssm-agent Now that agent is installed, we will need to give appropriate IAM permissions to the EC2 instance.\nWe will need to attach the default IAM Policy AmazonSSMManagedInstanceCore to the IAM role of the EC2 instance.\nIf you cannot use default policies, follow this documentation and manually create the policy: https://docs.aws.amazon.com/systems-manager/latest/userguide/getting-started-add-permissions-to-existing-profile.html.\nAccessing the instance via Browser To access the instance using SSM, head over to the EC2 console. Select the EC2 instance you wish to access. Click on Connect and now select Session Manager and then click on Connect It will open a new tab in your browser with shell access to your instance. Accessing the instance via AWS CLI To access the instance using AWS CLI, you will need a session manager plugin for AWS CLI Follow this documentation to install the plugin: https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html Verify that the plugin is installed by running the following command, $ session-manager-plugin The Session Manager plugin is installed successfully. Use the AWS CLI to start a session. * Now you can run the following command to connect to the instance aws ssm start-session \u0026ndash;target In This way, you can completely ditch SSH and start using SSM to access your EC2 instances. I hope you learned something new from this blog post. Click [**here**](https://surajincloud.com/about) to learn about me and how you can support my work, Thank you. ","permalink":"http://localhost:1313/blog/lets-ssm-not-ssh-on-ec2-instances/","summary":"To access EC2 instances, SSH has been the preferred way for many years. But this way has its downsides, such as managing the keypairs, etc.\nRecently, I came across a new safe and secure way to access instances, even from a Web browser and from the terminal, which is SSM. Let\u0026rsquo;s learn more about SSM and how to set it up.\nWhat is SSM?\nSSM stands for Systems Manager, a set of tools such as Session manager, State manager, patch manager, etc.","title":"Let's SSM, not SSH, on EC2 instances"},{"content":"In a previous blog post, we have seen how to enable SSM Session for EC2 instances to ditch SSH and enable safe and secure shell access.\nIn this blog post, we will go one step further towards security and encrypting our SSM sessions from EC2-managed nodes and the local machines of users with the help of the KMS key.\nCreating the KMS key To encrypt the sessions in SSM, we will first need to have the KMS key. aws kms create-key --description \u0026#34;SSM Session Manager Key\u0026#34; --origin AWS_KMS This will create a KMS key for us.\nOr you can use Terraform to create KMS keys.\nAdding appropriate permissions to the EC2 instance. You must create an IAM policy with the following permissions and attach it to the IAM role attached to the EC2 instance.\nkms:Encrypt Enable KMS encryption in SSM Session Manager. Go to the AWS Systems Manager console, https://console.aws.amazon.com/systems-manager/. Select Session Manager from the navigation. Choose the Preferences tab, and then choose Edit. Select the check box next to Enable KMS encryption. Select the existing KMS key from the list or fill the box with KMS Key ARN (you can also click on create a key, which will take you to the KMS Console) Choose Save. I hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/encrypting-ec2-sessions-with-kms-and-ssm-session-managers--deleted/","summary":"In a previous blog post, we have seen how to enable SSM Session for EC2 instances to ditch SSH and enable safe and secure shell access.\nIn this blog post, we will go one step further towards security and encrypting our SSM sessions from EC2-managed nodes and the local machines of users with the help of the KMS key.\nCreating the KMS key To encrypt the sessions in SSM, we will first need to have the KMS key.","title":"Encrypting EC2 Sessions with KMS and SSM Session Managers"},{"content":"As we know, Terraform supports S3 as a backend to store the state in AWS. In GCS and Azure, there are equivalent solutions for object storage available.\nWhat if we want to store terraform state in our environment or on any other cloud provider?\nThe good news is that we can do that, as all we need is S3-compliant storage.\nAn example can be:\nMinio: minio is S3 compatible Opensource Object storage Civo Object store: I have already shared a blog post for the same here For this example, we will use minio:\nLet\u0026rsquo;s run minio in a docker container locally mkdir -p ${HOME}/minio/data docker run \\ -p 9000:9000 \\ -p 9090:9090 \\ --user $(id -u):$(id -g) \\ --name minio1 \\ -e \u0026#34;MINIO_ROOT_USER=ADMIN\u0026#34; \\ -e \u0026#34;MINIO_ROOT_PASSWORD=PASSWORD\u0026#34; \\ -v ${HOME}/minio/data:/data \\ quay.io/minio/minio server /data --console-address \u0026#34;:9090\u0026#34; * Once it is running, go to [http://127\\.0\\.0\\.1:9090](http://127.0.0.1:9090/access-keys) and log in using the credentials * create access key [http://127\\.0\\.0\\.1:9090/access\\-keys](http://127.0.0.1:9090/access-keys) * Once you have the access keys, you can now define the backend as shown below: terraform { backend \u0026ldquo;s3\u0026rdquo; { bucket = \u0026ldquo;state\u0026rdquo; key = \u0026ldquo;terraform.tfstate\u0026rdquo; region = \u0026ldquo;myregion\u0026rdquo; endpoint = \u0026ldquo;http://127.0.0.1:9090\u0026rdquo; skip_region_validation = true skip_credentials_validation = true skip_metadata_api_check = true force_path_style = true } }\nWhat\u0026#39;s happening here? * Setting `skip_credentials_validation` to true will disables validation of the credentials when communicating with the S3 service. This is needed as we are using non\\-AWS S3\\-compatible storage. * Setting `skip_metadata_api_check` to true will disable the metadata API check (specific to AWS) when communicating with the S3 service. * With `force_path_style` we are telling terraform to use a path style for the URL * Setting `skip_region_validation` to true will disable region validation as it is again specific to AWS. You can also export credentials in the environment variables as shown instead of keeping them in the code export AWS_S3_ENDPOINT= export AWS_ACCESS_KEY_ID= export AWS_SECRET_ACCESS_KEY=\nThat\u0026#39;s it. You are now ready to use non\\-AWS S3 compliant Storage for Terraform backend and do, terraform init\nI hope you learned something new from this blog post. Click [**here**](https://surajincloud.com/about) to learn about me and how you can support my work, Thank you. ","permalink":"http://localhost:1313/blog/how-to-use-non-aws-s3-compatible-storage-for-terraform-backend/","summary":"As we know, Terraform supports S3 as a backend to store the state in AWS. In GCS and Azure, there are equivalent solutions for object storage available.\nWhat if we want to store terraform state in our environment or on any other cloud provider?\nThe good news is that we can do that, as all we need is S3-compliant storage.\nAn example can be:\nMinio: minio is S3 compatible Opensource Object storage Civo Object store: I have already shared a blog post for the same here For this example, we will use minio:","title":"How to use non-AWS S3 Compatible storage for Terraform Backend?"},{"content":"A few days back, I came up with the idea of awsctl CLI which will be kubectl style and will be easy to generate information about aws resources.\nI decided to live stream the development of the project so that it will help beginners to understand the process and lifecycle of the OpenSource project and will help the audience to learn how to write the CLI tool.\nhere\u0026rsquo;s the summary of first(16th Feb 2023) stream:\nI created the repo for awsctl: https://github.com/surajincloud/awsctl we selected Apache Open Source License for the project using https://choosealicense.com/ we then Installed the cobra-cli tool to initialize the commands. initialized the skeleton of the CLI and implemented version command :D we also added ec2 \u0026amp; s3 commands but we didn\u0026rsquo;t add any logic for the same. Check out the stream recording for more:\nhttps://www.youtube.com/live/W9AmkQrNLqw?feature=share\nIn the next stream, we will define, AWS Client and run awsctl get ec2 command.\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/building-awsctl-using-golang-1/","summary":"A few days back, I came up with the idea of awsctl CLI which will be kubectl style and will be easy to generate information about aws resources.\nI decided to live stream the development of the project so that it will help beginners to understand the process and lifecycle of the OpenSource project and will help the audience to learn how to write the CLI tool.\nhere\u0026rsquo;s the summary of first(16th Feb 2023) stream:","title":"Building awsctl using Golang #1"},{"content":"In Golang, implementing basic authentication in an HTTP API request is relatively straightforward.\nOnce we construct the request, then we have to call the SetBasicAuth() method and pass username \u0026amp; password\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { client := \u0026amp;http.Client{} req, err := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;http://google.com\u0026#34;, nil) if err != nil { log.Fatal(err) } req.SetBasicAuth(\u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;) resp, err := client.Do(req) if err != nil { log.Fatal(err) } defer resp.Body.Close() bodyText, err := ioutil.ReadAll(resp.Body) if err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;%s\\n\u0026#34;, bodyText) } It is equivalent of,\ncurl -u username:password http://example.com or\ncurl -H \u0026#34;Authorization: Basic \u0026lt;base64 encoded username:password \u0026gt;\u0026#34; localhost:8080 Let\u0026rsquo;s understand what SetBasicAuth is the method done behind the scene?\nRef: https://github.com/golang/go/blob/fd0c0db4a411eae0483d1cb141e801af401e43d3/src/net/http/request.go#L988\nfunc (r *Request) SetBasicAuth(username, password string) { r.Header.Set(\u0026#34;Authorization\u0026#34;, \u0026#34;Basic \u0026#34;+basicAuth(username, password)) } func basicAuth(username, password string) string { auth := username + \u0026#34;:\u0026#34; + password return base64.StdEncoding.EncodeToString([]byte(auth)) } it is adding Auth headers in the request, where username and password and encode to base64\nThis is a simple example of implementing basic authentication in an HTTP API request in Golang. Keep in mind that in production, it\u0026rsquo;s not recommended to hardcode the credentials in the code. And it\u0026rsquo;s best practice to use HTTPS for added security.\nNote: Basic Authentication is not the only way to authenticate a user, and it\u0026rsquo;s not recommended to use it in case of sensitive information or data.\nI hope you learnt something new from this blog post. Click here to learn about me and how you can support my work, Thank you!\n","permalink":"http://localhost:1313/blog/basic-authentication-in-http-api-requests-in-golang/","summary":"In Golang, implementing basic authentication in an HTTP API request is relatively straightforward.\nOnce we construct the request, then we have to call the SetBasicAuth() method and pass username \u0026amp; password\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { client := \u0026amp;http.Client{} req, err := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;http://google.com\u0026#34;, nil) if err != nil { log.Fatal(err) } req.SetBasicAuth(\u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;) resp, err := client.Do(req) if err != nil { log.","title":"Basic Authentication in HTTP API requests in Golang"},{"content":"Recently, Civo Cloud launched an object store that is object Storage and S3-compatible.\nRead more about it here: https://www.civo.com/learn/using-civo-object-stores\nIn my Cloud Heist - Civo series on youtube, one of the viewers asked how we can store terraform state in the bucket, similar to how we do in AWS space.\nHere\u0026rsquo;s the solution for the same,\nLet\u0026rsquo;s create the object store resource \u0026#34;civo_object_store\u0026#34; \u0026#34;statefile\u0026#34; { name = \u0026#34;state\u0026#34; max_size_gb = 500 region = \u0026#34;LON1\u0026#34; } Now apply this config. It will create the bucket and local state file. $ terraform apply now let\u0026rsquo;s verify from the console that the bucket is created (make sure you are in the correct region) we will use civo CLI to get the information about the object store. # state is the name of the bucket/objectstore $ civo objectstore show state ID : 9177e5e5-f04f-4a9b-bc25-6e028ca54187 Name : state Size : 500 Object Store Endpoint : objectstore.lon1.civo.com Region : LON1 Access Key : \u0026lt;ACCESS_KEY\u0026gt; Status : ready To access the secret key run: civo objectstore credential secret --access-key=\u0026lt;ACCESS_KEY\u0026gt; \\* note down the access key from the final output and run the following command to get the required environment variables. since we are using `s3` backend, which is mainly used for AWS, we will have to use environment variables with the prefix `AWS` * ``` $ civo objectstore credential export --access-key=\u0026lt;ACCESS_KEY\u0026gt; # Tip: You can redirect output with (\u0026gt;\u0026gt; ~/.zshrc) to add these to Zsh\u0026#39;s startup automatically export AWS_ACCESS_KEY_ID=\u0026lt;ACCESS_KEY\u0026gt; export AWS_SECRET_ACCESS_KEY=\u0026lt;SECRET_ACCESS_KEY\u0026gt; export AWS_DEFAULT_REGION=LON1 export AWS_HOST=https://objectstore.lon1.civo.com Now export those variables or store them in the bashrc or zshrc. Let\u0026rsquo;s define the terraform backend. As mentioned earlier, we are using the S3 backend, which is used for AWS. We will need to skip the region checks and metadata API checks. terraform { backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;state\u0026#34; key = \u0026#34;terraform.tfstate\u0026#34; region = \u0026#34;LON1\u0026#34; endpoint = \u0026#34;https://objectstore.lon1.civo.com\u0026#34; skip_region_validation = true skip_credentials_validation = true skip_metadata_api_check = true force_path_style = true } } Now Run terraform init.\n$ terraform init After this existing state is copied to the new backend, the state for whatever resources we create will be stored in the new state backend.\nNote: As there is no dynamoDB equivalent in the civo cloud, we could not implement State locking mechanism, Hence, If you are using approach mentioned in this blog, be careful.\nalso follow the issue: https://github.com/hashicorp/terraform/issues/27070\nI hope you learnt something new from this blog post. Click here to learn about me and how you can support my work, Thank you!\n","permalink":"http://localhost:1313/blog/civo-object-store-as-a-terraform-backend/","summary":"Recently, Civo Cloud launched an object store that is object Storage and S3-compatible.\nRead more about it here: https://www.civo.com/learn/using-civo-object-stores\nIn my Cloud Heist - Civo series on youtube, one of the viewers asked how we can store terraform state in the bucket, similar to how we do in AWS space.\nHere\u0026rsquo;s the solution for the same,\nLet\u0026rsquo;s create the object store resource \u0026#34;civo_object_store\u0026#34; \u0026#34;statefile\u0026#34; { name = \u0026#34;state\u0026#34; max_size_gb = 500 region = \u0026#34;LON1\u0026#34; } Now apply this config.","title":"Civo Object Store as a Terraform Backend"},{"content":"When you are using data sources for aws_instance probably to fetch the IP address or something else,\ndata \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;instance-name\u0026#34;] } } You must have faced the following issues while using Terraform for EC2 instances.\nâ”‚ Error: multiple EC2 Instances matched; use additional constraints to reduce matches to a single EC2 Instance Why this issue occurs? There\u0026rsquo;s a fair chance that the previous instance with the same name is destroyed, but there\u0026rsquo;s still an entry with the terminated state.\nResolution In this case, you will need to add another filter that will only retrieve running instances, not the terminated ones.\ndata \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;instance-name\u0026#34;] } filter { name = \u0026#34;instance-state-name\u0026#34; values = [\u0026#34;running\u0026#34;] } } Note: instance_state field won\u0026rsquo;t work in this case.\nI hope you learnt something new from this blog post. Click here to learn about me and how you can support my work, Thank you!\n","permalink":"http://localhost:1313/blog/solved-error-multiple-ec2-instances-matched-use-additional-constraints-to-reduce-matches-to-a-single-ec2-instance/","summary":"When you are using data sources for aws_instance probably to fetch the IP address or something else,\ndata \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;instance-name\u0026#34;] } } You must have faced the following issues while using Terraform for EC2 instances.\nâ”‚ Error: multiple EC2 Instances matched; use additional constraints to reduce matches to a single EC2 Instance Why this issue occurs? There\u0026rsquo;s a fair chance that the previous instance with the same name is destroyed, but there\u0026rsquo;s still an entry with the terminated state.","title":"[SOLVED] Error: multiple EC2 Instances matched; use additional constraints to reduce matches to a single EC2 Instance"},{"content":"Golang, also known as Go, is a programming language developed by Google. Today, it is used by many organisations for their web applications and CLI tools. Many OpenSource projects in cloud-native spaces, such as Kubernetes and Docker, were written using Go.\nGolang is known for its simplicity, efficiency, and scalability. This blog post will walk you through installing and setting up Golang on your computer.\nDownloading the Go\nGo to this link https://go.dev/dl/ and select the build depending on your operating system, as shown in the following image. At the time of writing this blog, 1.19.5 was the latest stable version of Golang.\nInstalling Golang\nOnce the binary is downloaded, you can install it on your computer by following the instructions on the link https://go.dev/doc/install. The installation process is different for each operating system, so follow the instructions specific to your OS.\nVerify the Installation\nTo verify that Golang has been successfully installed, open a terminal and run the following command:\n$ go version This will display the version of Golang that is currently installed on your computer.\nIn my case, it showed,\n$ go version go version go1.19.5 linux/amd64 Let\u0026rsquo;s write Code\nOnce Golang is installed and set up, you can start writing code.\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello, World!\u0026#34;) } You can run the program by using the following command,\n$ go run main.go Hello, World! In conclusion, installing and setting up Golang is a straightforward process that can be completed in just a few steps. With Golang installed, you can start writing efficient, scalable, and simple code in no time.\nI hope you learnt something new from this blog post. Click here to learn about me and how you can support my work, Thank you!\n","permalink":"http://localhost:1313/blog/installing-setting-up-golang/","summary":"Golang, also known as Go, is a programming language developed by Google. Today, it is used by many organisations for their web applications and CLI tools. Many OpenSource projects in cloud-native spaces, such as Kubernetes and Docker, were written using Go.\nGolang is known for its simplicity, efficiency, and scalability. This blog post will walk you through installing and setting up Golang on your computer.\nDownloading the Go\nGo to this link https://go.","title":"Installing \u0026 Setting up Golang"},{"content":"While working with APIs, headers are an essential aspect. Sometimes we set them, or sometimes we consume them and make decisions.\nWhen I was exploring headers in Golang, I came across two methods. Headers.Set() \u0026amp; Headers.Add() Initially, I thought they were the same. But then I wondered if they are the same, then why two methods?\nLet\u0026rsquo;s understand.\nI will be using a proxy which will use Add \u0026amp; Set on headers, as shown below,\nfunc (p Proxy) handle(w http.ResponseWriter, r *http.Request) { r.Header.Add(\u0026#34;Header1\u0026#34;, \u0026#34;head1\u0026#34;) r.Header.Set(\u0026#34;Header2\u0026#34;, \u0026#34;head2\u0026#34;) p.proxy.ServeHTTP(w, r) } Let\u0026rsquo;s try to curl the server along with Headers,\n$ curl -i http://localhost:8080 -H \u0026#39;header1: foo\u0026#39; -H \u0026#39;header2: bar\u0026#39; HTTP/1.1 200 OK Content-Length: 34 Content-Type: text/plain; charset=utf-8 Date: Tue, 17 Jan 2023 20:46:34 GMT header1: [head1 foo], header2: [bar] What happened here?\nFor header1, we were using Add method. When we tried passing value to the same header, it returned an array. So basically, it is appended to the existing value. Official documentation reference: https://pkg.go.dev/net/http#Header.Add\nFor header2, we were using Set method. When we tried passing value to the same header, it didn\u0026rsquo;t return as Array with old and newly appended values. Instead, it replaced the old value. Official documentation reference: https://pkg.go.dev/net/http#Header.Set\nReference https://tachingchen.com/blog/pitfall-of-golang-header-operation/ I hope you learnt something new from this blog post. Click here to learn about me and how you can support my work, Thank you!\n","permalink":"http://localhost:1313/blog/difference-between-setting-adding-the-headers-in-http-api-in-golang/","summary":"While working with APIs, headers are an essential aspect. Sometimes we set them, or sometimes we consume them and make decisions.\nWhen I was exploring headers in Golang, I came across two methods. Headers.Set() \u0026amp; Headers.Add() Initially, I thought they were the same. But then I wondered if they are the same, then why two methods?\nLet\u0026rsquo;s understand.\nI will be using a proxy which will use Add \u0026amp; Set on headers, as shown below,","title":"Difference between Setting \u0026 Adding the Headers in HTTP API in Golang"},{"content":"In the previous article, we saw how to make the HTTP GET API request in Golang. While going through the example, we discussed closing the response body.\nIn this article, Let\u0026rsquo;s discuss why it is essential to close the response body.\nWhat is the response body? The response body is a stream of data that is read from the server.\nWhy should I close it? Ensure all data has been read and the resources associated with it are freed up. What happens if I don\u0026rsquo;t close it? It can lead to resource leaks. It can cause the program to consume more memory and resources than necessary, leading to performance issues. It can also cause issues with the underlying connection as transport may not reuse HTTP/1.x â€œkeep-aliveâ€ TCP connections if the Body is not read to completion and closed. From the official docs,\nLink: https://pkg.go.dev/net/http#Client\n// The http Client and Transport guarantee that Body is always // non-nil, even on responses without a body or responses with // a zero-length body. It is the caller\u0026#39;s responsibility to // close Body. The default HTTP client\u0026#39;s Transport may not // reuse HTTP/1.x \u0026#34;keep-alive\u0026#34; TCP connections if the Body is // not read to completion and closed. What should I do? Always use defer statement and write down the statement immediately. This will ensure that the response body gets closed even in case of runtime error during the reading and parsing of the response. Here\u0026rsquo;s the complete example,\nresp, err := http.Get(\u0026#34;http://localhost:8080\u0026#34;) if err != nil { log.Fatal(err) } defer resp.Body.Close() Thanks for reading :)\nThis blog post has a more detailed explanation for the same.\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/why-close-the-http-api-response-body-in-golang-what-if-you-dont/","summary":"In the previous article, we saw how to make the HTTP GET API request in Golang. While going through the example, we discussed closing the response body.\nIn this article, Let\u0026rsquo;s discuss why it is essential to close the response body.\nWhat is the response body? The response body is a stream of data that is read from the server.\nWhy should I close it? Ensure all data has been read and the resources associated with it are freed up.","title":"Why close the HTTP API response body in Golang? What if you don't..."},{"content":"Golang is one of the widely used languages for designing API Clients. While you design the client, one of the important aspects of the client is fetching the data from the API in some format using a GET request.\nHere\u0026rsquo;s the curl command example, which mimics the GET request which we will write code for in Golang,\n$ curl -XGET localhost:8080 {\u0026#34;message\u0026#34;:\u0026#34;hello world !!!\u0026#34;} Note: I already have a basic webserver running, Hence I can get the output on localhost.\nNow we know the curl command, Let\u0026rsquo;s write the Go code for the same,\npackage main import ( \u0026#34;io\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { resp, err := http.Get(\u0026#34;http://localhost:8080\u0026#34;) if err != nil { log.Fatal(err) } defer resp.Body.Close() body, err := io.ReadAll(resp.Body) fmt.Println(string(body)) } Let\u0026rsquo;s run this program,\n$ go run main.go {\u0026#34;message\u0026#34;:\u0026#34;hello world !!!\u0026#34;} we get a similar output, which means we achieved what we wanted.\nBut let\u0026rsquo;s understand what\u0026rsquo;s happening here line by line,\nWe are importing net/http the package consists of HTTP Client and Server implementations and many other HTTP-related functionalities, which we will discuss in upcoming blogs. import \u0026#34;net/http\u0026#34; Now we will use http.Get() the function to make the API call. This function takes url as input. resp, err := http.Get(\u0026#34;http://localhost:8080\u0026#34;) This http.Get returns the Response struct and an error.\nWe should defer the response body closure. It can cause issues if we don\u0026rsquo;t close it, so this is an important step. defer resp.Body.Close() Once we have the response, we can use the response.Body field to read the response data. The response.Body field is an io.ReadCloser, which means it implements the io.Reader interface. body, err := io.ReadAll(resp.Body) Now, the body is of type []byte so we will convert it to a string and then print the response.\nfmt.Println(string(data)) This is a basic example of making a GET request in Golang. Many other options are available in the package, such as setting headers, specifying timeouts, and more, which we will see in upcoming blog posts.\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/how-to-make-an-http-get-request-in-golang/","summary":"Golang is one of the widely used languages for designing API Clients. While you design the client, one of the important aspects of the client is fetching the data from the API in some format using a GET request.\nHere\u0026rsquo;s the curl command example, which mimics the GET request which we will write code for in Golang,\n$ curl -XGET localhost:8080 {\u0026#34;message\u0026#34;:\u0026#34;hello world !!!\u0026#34;} Note: I already have a basic webserver running, Hence I can get the output on localhost.","title":"How to make an HTTP GET request in Golang?"},{"content":"Golang is a widely used language for building scalable and reliable web applications.\nWhen it comes to Storing data (relational), MySQL is one of the old and preferred ways as a database for many organisations.\nThis blog post will look at how to use Golang with MySQL by establishing the connection and printing the version using a simple query.\nIn upcoming blog posts, we will see advanced operations.\nPrerequisites To follow along with this tutorial, you should have the following installed on your machine:\nGolang MySQL Installing the MySQL driver\nThe first step is to install the MySQL driver for Golang. This driver lets us connect to a MySQL database from our Golang application. To install the MySQL driver, open a terminal and run the following command:\ngo get -u github.com/go-sql-driver/mysql Here\u0026rsquo;s the sample code. Take a minute and go through the code. Afterwards, we will discuss one line at a time.\npackage main import ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; ) func main() { db, err := sql.Open(\u0026#34;mysql\u0026#34;, \u0026#34;username:password@tcp(127.0.0.1:3306)/testdb\u0026#34;) defer db.Close() if err != nil { log.Fatal(err) } pingErr := db.Ping() if pingErr != nil { log.Fatal(pingErr) } fmt.Println(\u0026#34;Connected!\u0026#34;) var version string scanEerr := db.QueryRow(\u0026#34;SELECT VERSION()\u0026#34;).Scan(\u0026amp;version) if scanErr != nil { log.Fatal(scanErr) } fmt.Println(version) } Let\u0026rsquo;s understand one by one we need to connect to the MySQL database from our Golang application. To do this, we will use the database/sql package in the Go standard library. This package provides a common interface for working with different SQL databases.\nunderscore alias _ will import the package but will use it explicitly. It will be indirectly used for initialising the database drivers. import ( ... \u0026#34;database/sql\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; ... ) Now, we need to open a connection to the MySQL database. We can do this by calling the sql.Open() function and passing in the MySQL driver name and a connection string.\nThe first field is the name of the driver (mysql in this case) The second field is the DSN (Data source name)format which consists of username, password, protocol, address and name of the database, etc. db, err := sql.Open(\u0026#34;mysql\u0026#34;, \u0026#34;username:password@tcp(127.0.0.1:3306)/testdb\u0026#34;) Ensure that the connection is properly closed when the application exits with the help of the following defer statement. defer db.Close() Let\u0026rsquo;s ping the database to verify that our program can connect to the database. If this function doesn\u0026rsquo;t return any error (nil), then we can establish the connection successfully. pingErr := db.Ping() The QueryRow the function executes a query that will return at most one row. The Scan function will store output from the matched row into the version variable. var version string scanErr := db.QueryRow(\u0026#34;SELECT VERSION()\u0026#34;).Scan(\u0026amp;version) The output of the program will be as shown below:\n$ go run main.go Connected! 5.7.38 Thank you for reading. If you have any questions, do let me know in the comments.\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/how-to-use-golang-with-mysql/","summary":"Golang is a widely used language for building scalable and reliable web applications.\nWhen it comes to Storing data (relational), MySQL is one of the old and preferred ways as a database for many organisations.\nThis blog post will look at how to use Golang with MySQL by establishing the connection and printing the version using a simple query.\nIn upcoming blog posts, we will see advanced operations.\nPrerequisites To follow along with this tutorial, you should have the following installed on your machine:","title":"How to use Golang with MySQL?"},{"content":"Last year, I started exploring and studying terraform for work. While studying, I mostly relied on Terraform plan command to check the output plan.\nAs terraform module I was writing got bigger, the plan got bigger too, and I needed to store the plan in a file and then read \u0026amp; analyse the file later.\nFor example, I want to read the plan for the following resource \u0026amp; store it in a file,\nresource \u0026#34;null_resource\u0026#34; \u0026#34;hello_world\u0026#34;{ provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;echo \\\u0026#34;hello world\\\u0026#34;\u0026#34; } } Like any Linux user, I ran the following command,\nterraform plan \u0026gt; planfile and then\nbecause look at this file,\nAn execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: \u001b[32m+\u001b[0m create \u001b[0m Terraform will perform the following actions:\u001b [1m # null_resource.hello_world\u001b[0m will be created\u001b[0m\u001b[0m \u001b[0m \u001b[32m+\u001b[0m\u001b[0m resource \u0026#34;null_resource\u0026#34; \u0026#34;hello_world\u0026#34; { \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mid\u001b[0m\u001b[0m = (known after apply) } \u001b[0m\u001b[1mPlan:\u001b[0m 1 to add, 0 to change, 0 to destroy.\u001b[0m ------------------------------------------------------------------------ Note: You didn\u0026#39;t specify an \u0026#34;-out\u0026#34; parameter to save this plan, so Terraform can\u0026#39;t guarantee that exactly these actions will be performed if \u0026#34;terraform apply\u0026#34; is subsequently run. After a lot of googling, I came across this life-changing flag,\nterraform plan -no-color \u0026gt; planfile and my life became easy and simple as it was before:\nAn execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # null_resource.hello_world will be created + resource \u0026#34;null_resource\u0026#34; \u0026#34;hello_world\u0026#34; { + id = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ Note: You didn\u0026#39;t specify an \u0026#34;-out\u0026#34; parameter to save this plan, so Terraform can\u0026#39;t guarantee that exactly these actions will be performed if \u0026#34;terraform apply\u0026#34; is subsequently run. I hope this helps and makes your life a bit easy.\nHappy Terraforming!!!\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/read-terraforms-plan-the-better-way/","summary":"Last year, I started exploring and studying terraform for work. While studying, I mostly relied on Terraform plan command to check the output plan.\nAs terraform module I was writing got bigger, the plan got bigger too, and I needed to store the plan in a file and then read \u0026amp; analyse the file later.\nFor example, I want to read the plan for the following resource \u0026amp; store it in a file,","title":"Read Terraform's plan the better way"},{"content":"If you are Terraform user, you are pretty sure you know what the module is and must have used at least one community Terraform module. For those who donâ€™t know what Terraform module is, Read more here\nIf you are a pro user or part of the platform engineering or DevOps team, you probably must have written one. When you write the module for yourself or other groups, itâ€™s essential to have documentation on how to use the module, input or output parameters, etc.\nThe good news is that you donâ€™t need to write documentation. Instead, you can generate it in seconds with the help of terraform-docs.\nNote: provided that you have added appropriate descriptions at respective parameters.\nInstalling terraform-docs terraform-docs It is written in Golang. Hence it is easy to install on any Operating system you are using. Follow this guide to install the terraform-docs utility on your respective operating system. Verify the installation Once you have installed the tool, letâ€™s verify it by running the following command, it should show the version and the operating system along with the architecture.\n$ terraform-docs version terraform-docs version v0.16.0 darwin/arm64 Note: your version may differ.\nLetâ€™s try it out We will be using the terraform-civo-kubernetes module. For example, I wrote this module to manage my Kubernetes clusters on Civo Cloud. Clone this repo and inside the repo directory, run the following command.\nterraform-docs markdown table . It will now generate documentation in table format and print on stdout. You can either output this to a file or copy and paste it into the README file. It looks something like this when you visualise it in the markdown file.\nNow we know the basic functionality of terraform-docs, you can further explore official documentation for the project and use the project like a PRO\nReferences website: terraform-docs.io GitHub: github.com/terraform-docs/terraform-docs I hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/how-to-generate-terraform-module-docs/","summary":"If you are Terraform user, you are pretty sure you know what the module is and must have used at least one community Terraform module. For those who donâ€™t know what Terraform module is, Read more here\nIf you are a pro user or part of the platform engineering or DevOps team, you probably must have written one. When you write the module for yourself or other groups, itâ€™s essential to have documentation on how to use the module, input or output parameters, etc.","title":"How to generate Terraform module docs"},{"content":"Hello everyone, Itâ€™s my pleasure to announce the v0.1.0 release of the Kubectl EKS Plugin. I had this of the kubectl plugin for EKS from the day I started using Amazon EKS. But I finally made it into reality and cut the first release.\nThe mission of the kubectl EKS plugin is to simplify operations and provide easy access to cluster-related information.\nThe first release of kubectl-eks has few but convenient functionalities. I am excited about this project and hope to see more features in the upcoming months.\nInstalling the plugin Go to the release page here and download the respective binary. For example, On Linux, $ wget https://github.com/surajincloud/kubectl-eks/releases/download/v0.1.0/kubectl-eks_0.1.0_linux_amd64.tar.gz $ tar -xvf kubectl-eks_0.1.0_linux_amd64.tar.gz $ mv kubectl-eks ~/.local/bin # you can move to any $PATH of your choice Build it from the source $ git clone https://github.com/surajincloud/kubectl-eks $ cd kubectl-eks $ make $ mv kubectl-eks ~/.local/bin # you can move to any $PATH of your choice https://surajincloud.gumroad.com/l/own-kubectl-command\nLetâ€™s Explore List EKS nodes but differently, and it shows more information. $ kubectl eks nodes NAME ARCH INSTANCE-TYPE OS CAPACITY-TYPE REGION AGE ip-1-5-1-4.eu-west-1.compute.internal amd64 r5n.2xlarge linux SPOT eu-west-1a 14h ip-1-5-1-9.eu-west-1.compute.internal amd64 r5.2xlarge linux ON_DEMAND eu-west-1a 2d5h List all IRSA. It shows the list of all serviceaccount with their IAM Role and Token expiration time. $ kubectl eks irsa -n cloud NAMESPACE SERVICEACCOUNT IAM-ROLE TOKEN-EXPIRATION cloud test-1 arn:aws:iam::111111111111:role/test-1 86400 cloud test-2 arn:aws:iam::222222222222:role/test-2 86400 Note: If you donâ€™t pass the flag, it will list from all namespaces.\nThis feature is most exciting for me as I no longer need to install the AWS SSM plugin or search for instance id. I can use the following command for SSM access to EKS Node. $ kubectl eks ssm ip-1-5-1-4.eu-west-1.compute.internal SSM into node i-022357dcxxxxx sh-4.2$ We will see more exciting stuff in this kubectl eks plugin in upcoming releases.\nCheck out the GitHub repo here.\nPlease give it a go, and I hope you find it helpful. If you like the project, consider Starring it.\nIf you have any feature requests, Feel free to fill out a GitHub issue and contribute.\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/announcing-kubectl-eks-plugin-v0-1-0/","summary":"Hello everyone, Itâ€™s my pleasure to announce the v0.1.0 release of the Kubectl EKS Plugin. I had this of the kubectl plugin for EKS from the day I started using Amazon EKS. But I finally made it into reality and cut the first release.\nThe mission of the kubectl EKS plugin is to simplify operations and provide easy access to cluster-related information.\nThe first release of kubectl-eks has few but convenient functionalities.","title":"Announcing Kubectl EKS Plugin v0.1.0"},{"content":"Sometimes, When IAM user tries to register a new Virtual MFA device using Google Authenticator or similar tools, they notice the following error,\nEntity Already exists This entity already exists. MFADevice entity at the same path and name already exists. Before adding a new virtual MFA device, ask your administrator to delete the existing device using the CLI or API. This happens because the MFA device was created but not enabled for the users. It may happen because of various reasons. If you are a system administrator or platform engineer, this can be little pain to figure out the issue.\nLuckily, the solution is straightforward, as shown below,\nList all Virtual MFA devices and find out the MFA serial number, aws iam list-virtual-mfa-devices For example, if your username is suraj, you can try the following command,\n$ aws iam list-virtual-mfa-devices | grep suraj arn:aws:iam::0123456789:mfa/suraj Once you know the ARN of MFA, now we have to delete this dangling MFA using the following command,\n$ aws iam delete-virtual-mfa-device --serial-number arn:aws:iam::0123456789:mfa/suraj Now users should be able to register new MFA devices with their IAM users.\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/solved-mfa-entity-already-exists-error-in-aws-iam/","summary":"Sometimes, When IAM user tries to register a new Virtual MFA device using Google Authenticator or similar tools, they notice the following error,\nEntity Already exists This entity already exists. MFADevice entity at the same path and name already exists. Before adding a new virtual MFA device, ask your administrator to delete the existing device using the CLI or API. This happens because the MFA device was created but not enabled for the users.","title":"[SOLVED] MFA Entity already exists Error in AWS IAM"},{"content":"I regularly use *terraform plan -no-color* command to check the plan in a more readable way. I wrote a blog post regarding the same here. Give it a read if you havenâ€™t already.\nRunning this command every time is quite cumbersome. Hence I was looking for options and came across these terraform environment variables features.\nI can export TF_CLI_ARGS as shown below with the arg.\nexport TF_CLI_ARGS=\u0026#34;-no-color\u0026#34; Now, If I run terraform plan it will be equivalent of terraform plan -no-color\nbut If I run terraform apply it starts to fail as -no-color is not a valid arg for terraform apply.\nTo solve this problem, we can also mention command-specific args, such as TF_CLI_ARGS_plan or TF_CLI_ARGS_apply , In my case, as shown below,\nexport TF_CLI_ARGS_plan=\u0026#34;-no-color\u0026#34; I added the above statement in my bashrc, and life is a little easier with terraform now.\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/terraform-cli-args-define-once-use-again-and-again/","summary":"I regularly use *terraform plan -no-color* command to check the plan in a more readable way. I wrote a blog post regarding the same here. Give it a read if you havenâ€™t already.\nRunning this command every time is quite cumbersome. Hence I was looking for options and came across these terraform environment variables features.\nI can export TF_CLI_ARGS as shown below with the arg.\nexport TF_CLI_ARGS=\u0026#34;-no-color\u0026#34; Now, If I run terraform plan it will be equivalent of terraform plan -no-color","title":"Terraform CLI Args, define once, use again and again"},{"content":"A few days ago, I was working on an application which consisted of a multi-container pod where I had to send a signal from container A to a process from container B within the same Pod.\nWe know containers inside the pod already share the network namespace by default which means they technically have the same IP address.\nApart from this, most things, such as other namespaces, are isolated. For example, process namespace. Containers within the same pod run their process namespace and donâ€™t share any information with another container.\nAnd I wanted precisely the opposite of that to run my application.\nFor this use case, I crawled through Kubernetes documentation and came across this field called. shareProcessName: true.\nThis feature was added to Kubernetes 1.17\nLetâ€™s try it out Hereâ€™s the example where two containers are in a given pod, but the process namespace is not shared.\napiVersion: v1 kind: Pod metadata: name: pod spec: containers: - name: container1 image: envoyproxy/envoy:v1.19-latest - name: container2 image: busybox args: - /bin/sh - -c - echo hello;sleep 3600 Now, letâ€™s exec into the pod and see what process we see:\n# container 1 $ kubectl exec -it pod -c container1 -- bash root@pod:/# ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND envoy 1 0.5 0.5 2284636 47188 ? Ssl 21:39 0:00 envoy -c /etc/envoy/envoy.yaml root 82 0.0 0.0 3796 3072 pts/0 Ss 21:42 0:00 bash root 92 0.0 0.0 5296 2404 pts/0 R+ 21:42 0:00 ps aux # container 2 $ kubectl exec -it pod -c container2 -- sh / # ps aux PID USER TIME COMMAND 1 root 0:00 sleep 3600 27 root 0:00 sh 33 root 0:00 ps aux Process namespace is entirely isolated.\nLetâ€™s share the process namespace in the pod.\napiVersion: v1 kind: Pod metadata: name: pod spec: shareProcessNamespace: true containers: - name: container1 image: envoyproxy/envoy:v1.19-latest - name: container2 image: busybox args: - /bin/sh - -c - echo hello;sleep 3600 Now, if we exec into the second container of our pod, we will now see processes from container 1 as well.\n$ kubectl exec -it pod -c container2 -- sh / # ps aux PID USER TIME COMMAND 1 65535 0:00 /pause 7 101 0:00 envoy -c /etc/envoy/envoy.yaml 29 root 0:00 sleep 3600 35 root 0:00 sh 41 root 0:00 ps aux Have you noticed that container processes are no longer PID 1, though\nRemember this Once you share the process namespace, Containerâ€™s process will not run as PID 1. This may not work for some applications that need to run with PID 1, such as systemd.\nProcesses are visible among all the containers in a given pod. This means all the information from /proc is visible, which may contain sensitive information.\nLetâ€™s explore /proc.\n# ls /proc/7 attr coredump_filter gid_map mountinfo oom_score_adj sessionid status wchan autogroup cpuset io mounts pagemap setgroups syscall auxv cwd limits mountstats personality smaps task cgroup environ loginuid net projid_map smaps_rollup timens_offsets clear_refs exe map_files ns root stack timers cmdline fd maps oom_adj sched stat timerslack_ns comm fdinfo mem oom_score schedstat statm uid_map One thing to note here is that even if /proc is shared; content is protected by filesystem permissions. As you can see, if we try to access content for process 7, which is our envoy process, we are getting access denied, but if there are no proper filesystem permissions in place, youâ€™ll be able to see the content for the process as well.\n$ ls /proc/7/root ls: /proc/7/root: Permission denied Where can I use it? Send signals to the process to restart or kill the process Separate container with a set of tools either for troubleshooting or as a sidecar References https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/ I hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/share-process-namespace-among-containers-in-a-kubernetes-pod/","summary":"A few days ago, I was working on an application which consisted of a multi-container pod where I had to send a signal from container A to a process from container B within the same Pod.\nWe know containers inside the pod already share the network namespace by default which means they technically have the same IP address.\nApart from this, most things, such as other namespaces, are isolated. For example, process namespace.","title":"How to share process namespace among containers in a Kubernetes Pod"},{"content":"Often we find it difficult to visualize the Kubernetes secrets as they are base64 encoded. You will need to manually copy the encoded data and then decode it or write small custom shell scripts for the same.\nFortunately, thereâ€™s a kubectl plugin to simplify this process. view-secret The plugin allows users to view the contents of a secret without having to decode it manually.\nCheck out the project on GitHub:\nhttps://github.com/elsesiy/kubectl-view-secret\nIf you find the tool useful, make sure you â­ï¸ the repo and show your love for the project :)\nTo use the kubectl view-secret plugin, you first need to install it on your system. You can do this by running the following command:\n$ kubectl krew install view-secret Note: above command will only work if you have Krew installed.\nYou can also install the binary from the release page. Once the plugin is installed, you can use it by running the following command:\n$ kubectl view-secret [SECRET_NAME] This will display the contents of the secret in plain text. For example, if you have a secret named. mysecret,\n$ kubectl get secret mysecret -o yaml apiVersion: v1 data: foo: YmFy kind: Secret metadata: name: my-secret namespace: default type: Opaque Ideally, you would have to decode the value manually and view the secret. For example,\n$ echo \u0026#34;YmFy\u0026#34; | base64 -d bar with view-secret the plugin, you can view its contents by running the following command:\n$ kubectl view-secret mysecret Choosing key: foo bar This will print the contents of the secret in plain text, allowing you to easily view and manage the secret without having to decode it manually.\nanother interesting use case can be if a secret has more than one key value in it,\n$ kubectl get secret mysecret -o yaml apiVersion: v1 data: bar: Zm9v foo: YmFy kind: Secret metadata: name: mysecret namespace: default type: Opaque you can now explore secret using the plugin as follow,\n$ kubectl view-secret mysecret Multiple sub keys found. Specify another argument, one of: -\u0026gt; bar -\u0026gt; foo $ kubectl view-secret mysecret foo bar $ kubectl view-secret mysecret bar foo In conclusion, the kubectl view-secret a plugin is a useful tool for viewing secrets in Kubernetes clusters. It allows users to view the contents of a secret in plain text.\nhttps://surajincloud.gumroad.com/l/own-kubectl-command?layout=profile\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\nCheck out the video format of this blog.\n","permalink":"http://localhost:1313/blog/view-kubernetes-secrets-easily/","summary":"Often we find it difficult to visualize the Kubernetes secrets as they are base64 encoded. You will need to manually copy the encoded data and then decode it or write small custom shell scripts for the same.\nFortunately, thereâ€™s a kubectl plugin to simplify this process. view-secret The plugin allows users to view the contents of a secret without having to decode it manually.\nCheck out the project on GitHub:","title":"View Kubernetes Secrets easily"},{"content":"Have you ever wondered about running kubectl commands in parallel for some of your clusters? You must have written custom shell scripts or Golang scripts to do so.\nFortunately, a tool to run commands in parallel now exists, Thanks to Ahmet Alp Balkan ðŸ™\nCheck out the GitHub repo, and donâ€™t forget to â­ï¸ the repo :)\nhttps://github.com/ahmetb/kubectl-foreach\nkubectl-foreach Is a tool that allows users to run a kubectl command in one or more contexts (clusters) in parallel. This can be very useful when you need to perform an action on all of the resources in given contexts/clusters, such as gathering information, scaling them up or down, rolling out an update, or applying a configuration change.\nInstalling kubectl-foreach Download the binary from the release pages of the project based on your Operating system and architecture. Install using the Krew kubectl plugin manager kubectl krew install foreach Build from scratch go install github.com/ahmetb/kubectl-foreach@latest Let\u0026rsquo;s try it out Check versions on all the clusters at once kubectl foreach -- version Basically, whatever you write -- will be your ARGS for kubectl commands\nIt will now show you the following prompt,\nWill run command in context(s): - civo-1 - civo-2 - docker-desktop Continue? [Y/n]: Once you type Yyou will see the equivalent kubectl version output for all the clusters,\nContinue? [Y/n]: Y docker-desktop | WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. docker-desktop | Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;25\u0026#34;, GitVersion:\u0026#34;v1.25.2\u0026#34;, GitCommit:\u0026#34;5835544ca568b757a8ecae5c153f317e5736700e\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2022-09-21T14:33:49Z\u0026#34;, GoVersion:\u0026#34;go1.19.1\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/arm64\u0026#34;} docker-desktop | Kustomize Version: v4.5.7 docker-desktop | Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;24\u0026#34;, GitVersion:\u0026#34;v1.24.0\u0026#34;, GitCommit:\u0026#34;4ce5a8954017644c5420bae81d72b09b735c21f0\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2022-05-03T13:38:19Z\u0026#34;, GoVersion:\u0026#34;go1.18.1\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/arm64\u0026#34;} civo-2 | Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;25\u0026#34;, GitVersion:\u0026#34;v1.25.2\u0026#34;, GitCommit:\u0026#34;5835544ca568b757a8ecae5c153f317e5736700e\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2022-09-21T14:33:49Z\u0026#34;, GoVersion:\u0026#34;go1.19.1\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/arm64\u0026#34;} civo-2 | Kustomize Version: v4.5.7 civo-2 | Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;23\u0026#34;, GitVersion:\u0026#34;v1.23.6+k3s1\u0026#34;, GitCommit:\u0026#34;418c3fa858b69b12b9cefbcff0526f666a6236b9\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2022-04-28T22:16:18Z\u0026#34;, GoVersion:\u0026#34;go1.17.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} civo-2 | WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. civo-1 | Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;25\u0026#34;, GitVersion:\u0026#34;v1.25.2\u0026#34;, GitCommit:\u0026#34;5835544ca568b757a8ecae5c153f317e5736700e\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2022-09-21T14:33:49Z\u0026#34;, GoVersion:\u0026#34;go1.19.1\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/arm64\u0026#34;} civo-1 | Kustomize Version: v4.5.7 civo-1 | Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;23\u0026#34;, GitVersion:\u0026#34;v1.23.6+k3s1\u0026#34;, GitCommit:\u0026#34;418c3fa858b69b12b9cefbcff0526f666a6236b9\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2022-04-28T22:16:18Z\u0026#34;, GoVersion: civo-2 | WARNING: version difference between client (1.25) and server (1.23) exceeds the supported minor version skew of +/-1 \u0026#34;go1.17.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} civo-1 | WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. civo-1 | WARNING: version difference between client (1.25) and server (1.23) exceeds the supported minor version skew of +/-1 Note: you can disable the prompt by passing the -q flag, you can also set the environment variable KUBECTL_FOREACH_DISABLE_PROMPTS=true to true so that the command won\u0026rsquo;t prompt again\nthe previous command showed output from docker-desktop ,civo-1 , civo-2 What if I only want to see the outputs of civo clusters? For that, you can supply a regex of your choice, for example,\n$ kubectl foreach /civo/ -- get ns Will run command in context(s): - civo-1 - civo-2 civo-1 | NAME STATUS AGE civo-1 | default Active 44m civo-1 | kube-system Active 44m civo-1 | kube-public Active 44m civo-1 | kube-node-lease Active 44m civo-2 | NAME STATUS AGE civo-2 | kube-system Active 34m civo-2 | default Active 34m civo-2 | kube-public Active 34m civo-2 | kube-node-lease Active 34m I hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/run-commands-for-all-the-clusters-in-your-kubeconfig-in-parallel/","summary":"Have you ever wondered about running kubectl commands in parallel for some of your clusters? You must have written custom shell scripts or Golang scripts to do so.\nFortunately, a tool to run commands in parallel now exists, Thanks to Ahmet Alp Balkan ðŸ™\nCheck out the GitHub repo, and donâ€™t forget to â­ï¸ the repo :)\nhttps://github.com/ahmetb/kubectl-foreach\nkubectl-foreach Is a tool that allows users to run a kubectl command in one or more contexts (clusters) in parallel.","title":"Run Commands for All the clusters in your Kubeconfig in parallel"},{"content":"Kubernetes and Terraform are both powerful tools for managing cloud infrastructure, but they use different configuration languages. Kubernetes uses YAML, while Terraform uses HashiCorp Configuration Language (HCL). This can make it difficult to use the two tools together, as you may need to convert YAML files to HCL in order to use them with Terraform.\nSometimes when you are spinning up managed Kubernetes such as GKE, EKS and AKS, you may need to deploy Kubernetes resources via terraform.\nNow, if you already have YAML but you donâ€™t have time to rewrite terraform for it, what should you do?\nFortunately, thereâ€™s an amazing open-source tool available to help you convert Kubernetes YAML to Terraform HCL. In this blog post, weâ€™ll walk you through the steps to do this so that you can take advantage of the best features of this tool.\nGitHub - sl1pm4t/k2tf: Kubernetes YAML to Terraform HCL converter\n*A tool for converting Kubernetes API Objects (in YAML format) into HashiCorp\u0026rsquo;s Terraform configuration language. Theâ€¦*github.com\nInstallation on Mac brew install k2tf on Linux \u0026amp; Windows\n- Grab the latest release binaries from the Github release page Letâ€™s take a pod definition\n--- apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 Letâ€™s convert it to HCL\n$ k2tf -f pod.yaml resource \u0026#34;kubernetes_pod\u0026#34; \u0026#34;nginx\u0026#34; { metadata { name = \u0026#34;nginx\u0026#34; } spec { container { name = \u0026#34;nginx\u0026#34; image = \u0026#34;nginx:1.14.2\u0026#34; port { container_port = 80 } } } } you can save the terraform HCL output into the file using the following command,\nk2tf -f pod.yaml -o pod.tf you can pass a file consisting of multiple resources as input to k2tf tool.\nfor example,\n$ cat manifest.yaml --- apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app.kubernetes.io/name: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 we can run the following command to convert the above file:\nk2tf -f manifest.yaml -o manifests.tf one of the interesting features in k2tf is, you can fetch the resource definition from the cluster and then convert it to HCL\nkubectl get deployments -o yaml | k2tf -o resources.tf default value for -f flag of k2tf isâ€Šâ€”â€Šwhich means read from the STDIN\nIf you find the tool useful, make sure you â­ï¸ the repo and show your love for the project :)\nIâ€™ve also made a video for the same:\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/kubernetes-and-terraform-converting-yaml-to-hcl-for-better-automation/","summary":"Kubernetes and Terraform are both powerful tools for managing cloud infrastructure, but they use different configuration languages. Kubernetes uses YAML, while Terraform uses HashiCorp Configuration Language (HCL). This can make it difficult to use the two tools together, as you may need to convert YAML files to HCL in order to use them with Terraform.\nSometimes when you are spinning up managed Kubernetes such as GKE, EKS and AKS, you may need to deploy Kubernetes resources via terraform.","title":"Kubernetes and Terraform: Converting YAML to HCL for Better Automation"},{"content":"When we work around Kubernetes, we often have to reference the documentation for a few things to save time from the journey from terminal to browser and back. Kubectl offers a great set of help in itself.\nLearning about the resources\nThe command shows all fields from the pod resource along with its type and information about it. You can also look for a subfield.\nfor example,\n$ kubectl explain pod $ kubectl explain pod.spec.containers Sometimes what happens is you know exactly what you want in resource definition, but you forget the field or misspell it, recursive the flag comes in handy when it shows just the fields along with their type, whether it\u0026rsquo;s a string, map, list, etc.\n$ kubectl explain pod --recursive while kubectl explainis pretty useful to know all fields and their type and usage; when you are building some cool stuff like some CLI tool, plugin or maybe operator, you might have to explore and learn about APIs and JSON structures. You can do that with kubectlright from the terminal, raw flag for kubectl get comes in handy for it.\nhttps://surajincloud.gumroad.com/l/own-kubectl-command\nLearning about APIs\nkubectl get --raw / We can also explore the subpath, as shown in the above gif.\nThereâ€™s one more command which is quite handy to see what resources are registered in a given kubernetes cluster. It also shows whether a given resource is a namespace or not, and itâ€™s the short names. I covered this blog post in my keynote talk at Kubernetes Community Days Bangalore keynote as well. For more, watch my talk here:\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/learning-kubernetes-with-kubectl/","summary":"When we work around Kubernetes, we often have to reference the documentation for a few things to save time from the journey from terminal to browser and back. Kubectl offers a great set of help in itself.\nLearning about the resources\nThe command shows all fields from the pod resource along with its type and information about it. You can also look for a subfield.\nfor example,\n$ kubectl explain pod $ kubectl explain pod.","title":"Learning Kubernetes with Kubectl"},{"content":"Source: https://unsplash.com/photos/pZld9PiPDno\nWhen associating IAM roles with pods instead of assigning a super role to worker nodes, Kiam is a fairly popular project in Community; kiam runs as an agent on each node in the Kubernetes cluster, allowing cluster users to associate IAM roles to Pods.\nTo enhance security, Kiam uses certificates for servers \u0026amp; agents. To simplify the certificate management for Kiam, we will utilise cert-manager,\ncert-manager is a native Kubernetes certificate management controller that helps issue certificates from various sources or self-signed ones.\nFor Kiam, we will need the following:\nSelf-signed CA Server Certificate (Generated from above CA) Agent Certificate (Generated from above CA) Following Code shows the issuer \u0026amp; Certificate for the initial Self-signed CA. isCA option in the certificate spec specifies that itâ€™s a CA certificate.\n--- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: kiam-ca-selfsigned-issuer spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: kiam-ca-selfsigned spec: secretName: kiam-ca-selfsigned commonName: \u0026#34;kiam-ca\u0026#34; isCA: true issuerRef: name: kiam-ca-selfsigned-issuer usages: - \u0026#34;any\u0026#34; The above CA certificate will be stored in Kiam-ca-self-signed secret.\nBased on this CA certificate, we will create an Issuer of CA type and pass the secret name.\napiVersion: cert-manager.io/v1 kind: Issuer metadata: name: kiam-ca-issuer spec: ca: secretName: kiam-ca-selfsigned Now that we have an issuer, we are ready to generate server \u0026amp; agent certificates,\nGenerating Server \u0026amp; Agent Certificates For Server apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: kiam-server spec: secretName: kiam-server-tls issuerRef: name: kiam-ca-issuer usages: - \u0026#34;any\u0026#34; dnsNames: - \u0026#34;localhost\u0026#34; - \u0026#34;kiam-server\u0026#34; ipAddresses: - \u0026#34;127.0.0.1\u0026#34; For Agent apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: kiam-agent spec: secretName: kiam-agent-tls commonName: agent issuerRef: name: kiam-ca-issuer usages: - \u0026#34;any\u0026#34; Renewing Certificates The only caveat here is if the main Self-signed CA issuer gets renewed, the following server \u0026amp; agent certificates donâ€™t get renewed; this issue is already reported on cert-manager repositories. Youâ€™ll have to renew them by following commands manually,\nkubectl cert-manager renew kiam-server --namespace=kube-system kubectl cert-manager renew kiam-agent --namespace=kube-system I hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\nReferences https://github.com/uswitch/kiam/blob/master/docs/TLS.md https://github.com/jetstack/cert-manager/issues/2478 ","permalink":"http://localhost:1313/blog/chaining-kiam-certificates-using-cert-manager/","summary":"Source: https://unsplash.com/photos/pZld9PiPDno\nWhen associating IAM roles with pods instead of assigning a super role to worker nodes, Kiam is a fairly popular project in Community; kiam runs as an agent on each node in the Kubernetes cluster, allowing cluster users to associate IAM roles to Pods.\nTo enhance security, Kiam uses certificates for servers \u0026amp; agents. To simplify the certificate management for Kiam, we will utilise cert-manager,\ncert-manager is a native Kubernetes certificate management controller that helps issue certificates from various sources or self-signed ones.","title":"Chaining Kiam Certificates using Cert-manager"},{"content":"Last week, while going through some of the Kubernetes manifest files, I stumbled upon this label:\nkubernetes.io/cluster-service: â€œtrueâ€ I searched about this in Documentation but could not find much information, so I thought of writing a small blog post about it.\nhttps://surajincloud.gumroad.com/l/own-kubectl-command\nWhen you add this label to any resource of type Kind: Service , it means it is part of cluster service, and the user needs to know when they are looking for cluster information. Hence it appears in the following command:\nkubectl cluster-info By default, this command shows:\n$ kubectl cluster-info Kubernetes master is running at https://kubernetes.docker.internal:6443 KubeDNS is running at https://kubernetes.docker.internal:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use â€˜kubectl cluster-info dumpâ€™. Now, we will deploy a sample Nginx deployment and will try to add this to cluster info:\nkubectl run nginx --image=nginx -n kube-system kubectl expose deployment nginx --port 80 -n kube-system \\ --labels kubernetes.io/cluster-service=true Now you can see nginx service in kubectl cluster-info output,\n$ kubectl cluster-info Kubernetes master is running at https://kubernetes.docker.internal:6443 KubeDNS is running at https://kubernetes.docker.internal:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy nginx is running at https://kubernetes.docker.internal:6443/api/v1/namespaces/kube-system/services/nginx/proxy Note: This will work only for services in ***kube-system*** namespace.\nYou can check out the video tutorial for the same here:\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/adding-own-service-to-kubernetes-cluster-info/","summary":"Last week, while going through some of the Kubernetes manifest files, I stumbled upon this label:\nkubernetes.io/cluster-service: â€œtrueâ€ I searched about this in Documentation but could not find much information, so I thought of writing a small blog post about it.\nhttps://surajincloud.gumroad.com/l/own-kubectl-command\nWhen you add this label to any resource of type Kind: Service , it means it is part of cluster service, and the user needs to know when they are looking for cluster information.","title":"Adding own service to Kubernetes Cluster Info"},{"content":"For one of my terraform tasks, I needed all the possible combinations of elements from all given sets. For example, there are two sets.\nset1 = [\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;] set2 = [1,2] The expected result is,\n[[\u0026#34;a\u0026#34;,1], [\u0026#34;a\u0026#34;,2],[\u0026#34;b\u0026#34;,1],[\u0026#34;b\u0026#34;,2],[\u0026#34;c\u0026#34;,1],[\u0026#34;c\u0026#34;,2]] After searching about this, I learned that such a concept is called the Cartesian Product. From the Wikipedia definition,\nthe Cartesian product of two sets A and B, denoted A Ã— B, is the set of all ordered pairs (a, b) where a is in A and b is in B.\nIn my mind, I thought there had to be a nested for looping \u0026amp; tried something and failed miserably.\nBut we think from the Programmer\u0026rsquo;s perspective, but Terraform is not a Programming Language. Hence, After searching a lot, I came across this amazing function called setproduct, which basically provides Cartesian product out of the box,\nlocals{ b = [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;] a = [1, 2] } output \u0026#34;example\u0026#34; { value = setproduct(local.b, local.a)} and the output is amazing and lifesaving,\n[ [ \u0026#34;a\u0026#34;, 1, ], [ \u0026#34;a\u0026#34;, 2, ], [ \u0026#34;b\u0026#34;, 1, ], [ \u0026#34;b\u0026#34;, 2, ], [ \u0026#34;c\u0026#34;, 1, ], [ \u0026#34;c\u0026#34;, 2, ], ] If you have successfully created a cartesian product without using setproduct(), do comment here :)\nI hope you learned something new from this blog post. Click here to learn about me and how you can support my work, Thank you.\n","permalink":"http://localhost:1313/blog/creating-a-cartesian-product-in-terraform/","summary":"For one of my terraform tasks, I needed all the possible combinations of elements from all given sets. For example, there are two sets.\nset1 = [\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;] set2 = [1,2] The expected result is,\n[[\u0026#34;a\u0026#34;,1], [\u0026#34;a\u0026#34;,2],[\u0026#34;b\u0026#34;,1],[\u0026#34;b\u0026#34;,2],[\u0026#34;c\u0026#34;,1],[\u0026#34;c\u0026#34;,2]] After searching about this, I learned that such a concept is called the Cartesian Product. From the Wikipedia definition,\nthe Cartesian product of two sets A and B, denoted A Ã— B, is the set of all ordered pairs (a, b) where a is in A and b is in B.","title":"Creating a Cartesian Product in Terraform"},{"content":"It\u0026rsquo;s always bit of confusing about OpenShift 4 :D here\u0026rsquo;s how we can add user in OpenShift 4.\nCreate htpasswd file as below and users and their passwords, $ htpasswd -cb users.htpasswd user1 user1pass $ htpasswd -b users.htpasswd user2 user2pass Through console, login with kube:admin user, navigate to Administration \u0026gt; Cluster Settings \u0026gt; Global Configuration\nClick on Edit YAML in front of Oauth\nClick on Overview, under Identity Providers section, Click on Add and select HTPasswd\nput name as htpasswd, mapping method as add and select file which we created earlier using htpasswd.\nCheck Yaml section and verify that block as following should appear under spec:\nidentityProviders: - htpasswd: fileData: name: htpasswd-qbwc4 mappingMethod: add name: htpasswd type: HTPasswd Verify users, $ oc login -u system:admin $ oc get users NAME UID FULL NAME IDENTITIES user1 5699e114-5e0d-11e9-9a49-0a580a800045 htpasswd:avni user2 8c932519-5e0d-11e9-9a49-0a580a800045 htpasswd:developer Now you can login with your new users, for example,\noc login -u user1 -p user1pass Happy Hacking :) ","permalink":"http://localhost:1313/blog/user-on-os4/","summary":"It\u0026rsquo;s always bit of confusing about OpenShift 4 :D here\u0026rsquo;s how we can add user in OpenShift 4.\nCreate htpasswd file as below and users and their passwords, $ htpasswd -cb users.htpasswd user1 user1pass $ htpasswd -b users.htpasswd user2 user2pass Through console, login with kube:admin user, navigate to Administration \u0026gt; Cluster Settings \u0026gt; Global Configuration\nClick on Edit YAML in front of Oauth\nClick on Overview, under Identity Providers section, Click on Add and select HTPasswd","title":"Creating users on OpenShift 4"},{"content":"A NodeJS developer named Warlock, recently joined his dream company as an Enterprise Developer.\nOn day 1, his boss gave him a Code Repository \u0026amp; his OpenShift Platform Credentials to work his magic.\nWarlock got his code,\ngit clone https://github.com/sclorg/nodejs-ex But was unclear about how to proceed as he had no idea about OpenShift. He then referred to the OpenShift docs but got confused and overwhelmed by all the new terminologies, DeploymentConfigs, Pods, Services, and Routes mentioned in it. He could barely login into the cluster using oc and thus got frustrated.\nThen, one of his friend suggested that he take a look at odo, Warlock instantly took to it, as it had simple, easy to understand concepts like Application, Components, and URLs.\nHe found it interesting, read the docs further, and started using it.\nHe logged into the cluster with the credentials given by his boss, odo login -u warlock@dream.company -p xxxxxxx Warlock read the concept of an application in odo \u0026amp; created an application: odo app create myapp He referred the odo catalog to check on the availability of NodeJS, odo catalog list components NAME PROJECT TAGS dotnet openshift 2.0,latest httpd openshift 2.4,latest nginx openshift 1.10,1.12,1.8,latest nodejs openshift 0.10,10,4,6,8,8-RHOAR,latest perl openshift 5.16,5.20,5.24,5.26,latest php openshift 5.5,5.6,7.0,7.1,latest python openshift 2.7,3.3,3.4,3.5,3.6,latest ruby openshift 2.0,2.2,2.3,2.4,2.5,latest wildfly openshift 10.0,10.1,11.0,12.0,13.0,8.1,9.0,latest Woo, Warlock was happy because NodeJS was available in the cluster.\nSince he had only one component of the type nodejs, he created the application,\nodo create nodejs Now what? Warlock needed an URL to access his application, he tried: odo url create There was nothing on the URL, warlock got confused, then he realized, he hadnâ€™t pushed the code. So he pushed his code: odo push Tada, Now he could access his application, everytime he made some changes he did odo push and woah :)\nAfter a while, he got bored of pushing again and again. On further exploration of the hallowed odo docs he found the watch feature, and performed odo watch in his repo and wooooot !!!! odo watch Now everytime he changed his code, the changes were automatically reflected in the URL, he did not need to manually push each time.\nIn this way, Warlock the newbie developed his NodeJS application on OpenShift Platform.\nNow, Warlock always uses odo, Warlock is Happy, Be like Warlock :D ","permalink":"http://localhost:1313/blog/warlock-odo/","summary":"A NodeJS developer named Warlock, recently joined his dream company as an Enterprise Developer.\nOn day 1, his boss gave him a Code Repository \u0026amp; his OpenShift Platform Credentials to work his magic.\nWarlock got his code,\ngit clone https://github.com/sclorg/nodejs-ex But was unclear about how to proceed as he had no idea about OpenShift. He then referred to the OpenShift docs but got confused and overwhelmed by all the new terminologies, DeploymentConfigs, Pods, Services, and Routes mentioned in it.","title":"Story of Noob Developer on OpenShift"},{"content":" While exploring, I came across ko tool by google \u0026amp; found interesting since it buids and deploy golang applications to kubernetes easily. This post is for minikube only since I am focussing on local development.\nPre-Flight Checks Installing ko go get github.com/google/go-containerregistry/cmd/ko That\u0026rsquo;s it :)\nVerify your installation by which ko\nWe can mention any docker registry (local or remote) using KO_DOCKER_REPO env variable, but as we are focussing on local development, we will publish images to minikube\u0026rsquo;s docker daemon\neval $(minikube docker-env) Let\u0026rsquo;s do it take sample go web application: package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hi there\u0026#34;) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } Now we need to write small Deployment (config.yml) file, but here\u0026rsquo;s the magical part, instead of image name, we will be mentioning import path of go code. Cick here for allowed paths. apiVersion: apps/v1beta1 kind: Deployment metadata: name: hello-world spec: selector: matchLabels: foo: bar replicas: 1 template: metadata: labels: foo: bar spec: containers: - name: hello-world # This is the import path for the Go binary to build and run. image: github.com/surajnarwade/webapp ports: - containerPort: 8080 we also need to expose this deployment to access it kubectl expose deployment hello-world --type=NodePort all set, now magic will begin, ko apply -L -f config.yml -L indicates publishing images locally\ncheck minikube service hello-world, you will see the output Hi there, isn\u0026rsquo;t it cool ?\nNow you can make changes to code \u0026amp; again do ko apply -L -f config.yml, you will see changes reflected :)\nIn this way, we can do local development of golang code with minikube, this looks cool but there\u0026rsquo;s scope of more improvement too :P\nWhat happens behind the scene ? ko takes the import path of go code from the deployment, it builds the go binary. then, it creates new docker image with distroless as a base image and copies binary into it. it updates the deployment with this new image :) Reference: https://github.com/google/go-containerregistry/blob/master/cmd/ko/README.md Happy Hacking :)\n","permalink":"http://localhost:1313/blog/develop-go-minikube/","summary":"While exploring, I came across ko tool by google \u0026amp; found interesting since it buids and deploy golang applications to kubernetes easily. This post is for minikube only since I am focussing on local development.\nPre-Flight Checks Installing ko go get github.com/google/go-containerregistry/cmd/ko That\u0026rsquo;s it :)\nVerify your installation by which ko\nWe can mention any docker registry (local or remote) using KO_DOCKER_REPO env variable, but as we are focussing on local development, we will publish images to minikube\u0026rsquo;s docker daemon","title":"Local Development of golang app with minikube \u0026 ko"},{"content":" What\u0026rsquo;s client-go ? Where\u0026rsquo;s client-go ? you can find it in kubernetes/kubernetes/staging directory and published by bot to k8s.io/client-go, kubernetes also uses client-go. it\u0026rsquo;s interesting if you see client-go from kubernetes vendor, it has symlinks to kubernetes/kubernetes/staging In this Adventure, very first step is to see how to connect to API Server :) Connecting to API Server There are two ways to talk to cluster using any go program: outside cluster: if your program is standalone and if you have either kubeconfig file or master URL.\ninside cluster: if your program is supposed to run on kubernetes (for example, controller)\nWhen your program runs on kubernetes, it uses service account token inside the pods /var/run/secrets/kubernetes.io/serviceaccount for auth and tls stuff. Connecting to cluster using client-go clientcmd holds the required code to configuring connection to the cluster using kubeconfig as well as about modification to kubeconfig. Code reference here. import \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; if you have master URL and kubeconfig both: config, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig) if you have only have kubeconfig file: config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, kubeconfig) If you are running program on kubernetes, you don\u0026rsquo;t need to provided parameters, it will automatically fetch details using serviceaccount tokens: config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, \u0026#34;\u0026#34;) If you don\u0026rsquo;t provide any parameters, it fallbacks to InClusterConfig, even if it fails, it fallbacks to default configs which is either KUBERNETES_MASTER environment variable or simply http://localhost:8080, code reference here.\nDirectly use this (even above way internally uses this), Code Ref here.\nimport \u0026#34;k8s.io/client-go/rest\u0026#34; ... config,err := rest.InClusterConfig() This method basically gets KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT environment variables (which are injected into every pod) and service token to generate the config.\nconfig has all necessary information to talk to the cluster from anywhere :) If you check struct defination here it has fields like Host, APIPath, Username, Password, BearerToken, etc which are needed to talk to cluster\nIf you explore more, you will see clientcmd/api has Config (Kind: Config) struct which is API resource for kubeconfig, code is here TGIF, Enough for Today :) Happy Weekend :) Stay Tuned :) ","permalink":"http://localhost:1313/blog/exploring-k8s-part1/","summary":"What\u0026rsquo;s client-go ? Where\u0026rsquo;s client-go ? you can find it in kubernetes/kubernetes/staging directory and published by bot to k8s.io/client-go, kubernetes also uses client-go. it\u0026rsquo;s interesting if you see client-go from kubernetes vendor, it has symlinks to kubernetes/kubernetes/staging In this Adventure, very first step is to see how to connect to API Server :) Connecting to API Server There are two ways to talk to cluster using any go program: outside cluster: if your program is standalone and if you have either kubeconfig file or master URL.","title":"Exploring Kubernetes: Client-Go - part-1"},{"content":"If you have kubeconfig file already in place,\nkubeconfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(clientcmd.NewDefaultClientConfigLoadingRules(), \u0026amp;clientcmd.ConfigOverrides{}) There is one more function clientcmd.NewInteractiveDeferredLoadingClientConfig but as per code, we can use it only when password is allowed\nNamespace given by current context, namespace, _, err := kubeconfig.Namespace() REST config needed to operations restconfig, err := kubeconfig.ClientConfig() if you want kubeconfig as a struct clientcmdapi.Config, err := kubeconfig.RawConfig() Here, kubeconfig is interface of following type:\n// ClientConfig is used to make it easy to get an api server client type ClientConfig interface { // RawConfig returns the merged result of all overrides RawConfig() (clientcmdapi.Config, error) // ClientConfig returns a complete client config ClientConfig() (*restclient.Config, error) // Namespace returns the namespace resulting from the merged // result of all overrides and a boolean indicating if it was // overridden Namespace() (string, bool, error) // ConfigAccess returns the rules for loading/persisting the config. ConfigAccess() ConfigAccess } structs NewNonInteractiveDeferredLoadingClientConfig \u0026amp; inClusterClientConfig implements above interface and can be used as per requirements whether inside of cluster or outside.\n","permalink":"http://localhost:1313/blog/exploring-k8s-part2/","summary":"If you have kubeconfig file already in place,\nkubeconfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(clientcmd.NewDefaultClientConfigLoadingRules(), \u0026amp;clientcmd.ConfigOverrides{}) There is one more function clientcmd.NewInteractiveDeferredLoadingClientConfig but as per code, we can use it only when password is allowed\nNamespace given by current context, namespace, _, err := kubeconfig.Namespace() REST config needed to operations restconfig, err := kubeconfig.ClientConfig() if you want kubeconfig as a struct clientcmdapi.Config, err := kubeconfig.RawConfig() Here, kubeconfig is interface of following type:\n// ClientConfig is used to make it easy to get an api server client type ClientConfig interface { // RawConfig returns the merged result of all overrides RawConfig() (clientcmdapi.","title":"Exploring Kubernetes: Client-Go - part-2"},{"content":"we got restConfig and namespace, connection is set up Yay\nFrom this restconfig, Now we need to create respective clients for further operations\nwe will need following package:\nimport k8s.io/client-go/kubernetes create basic kubernetes client: kubeClient, err := kubernetes.NewForConfig(restConfig) it returns clients for APIgroups like:\nApps CoreV1 Batch StorageV1 If you have some custom controller, generated code will have NewForConfig function which you have call separately,\nFor example, In case of service catalog,\nserviceCatalogClient, err := servicecatalogclienset.NewForConfig(restConfig) Let\u0026rsquo;s take an example for pod:\nkubeClient.CoreV1().Pods(namespace).List\nkubeClient is struct consist of all other apigroup clients with all structs unexported so we will call method CoreV1() which returns interface which has struct c.coreV1 and interface ","permalink":"http://localhost:1313/blog/exploring-k8s-part3/","summary":"we got restConfig and namespace, connection is set up Yay\nFrom this restconfig, Now we need to create respective clients for further operations\nwe will need following package:\nimport k8s.io/client-go/kubernetes create basic kubernetes client: kubeClient, err := kubernetes.NewForConfig(restConfig) it returns clients for APIgroups like:\nApps CoreV1 Batch StorageV1 If you have some custom controller, generated code will have NewForConfig function which you have call separately,\nFor example, In case of service catalog,","title":"Exploring Kubernetes: Client-Go - part-3"},{"content":" To check devices: nmcli device status Overall status of NetworkManager: nmcli general status Display connections: nmcli connection show display only active connections: nmcli connection show --active WI-FI Check wifi status nmcli radio wifi Turn on the wifi nmcli radio wifi on Turn off the wifi nmcli radio wifi off List available wifi access points nmcli device wifi list Refresh the access point list nmcli device wifi rescan Connect to wifi nmcli device wifi connect \u0026lt;SSID\u0026gt; Connect to password protected access point nmcli device wifi connect \u0026lt;SSID\u0026gt; password \u0026lt;password\u0026gt; ","permalink":"http://localhost:1313/blog/nmcli101/","summary":" To check devices: nmcli device status Overall status of NetworkManager: nmcli general status Display connections: nmcli connection show display only active connections: nmcli connection show --active WI-FI Check wifi status nmcli radio wifi Turn on the wifi nmcli radio wifi on Turn off the wifi nmcli radio wifi off List available wifi access points nmcli device wifi list Refresh the access point list nmcli device wifi rescan Connect to wifi nmcli device wifi connect \u0026lt;SSID\u0026gt; Connect to password protected access point nmcli device wifi connect \u0026lt;SSID\u0026gt; password \u0026lt;password\u0026gt; ","title":"nmcli 101"},{"content":"Hello Folks,\nI appeared for the exam on 19th October 2018 and yesterday(20th Oct 2018), I got the result \u0026amp; I scored 92%, Yay :)\nI got so many request about How to prepare for the exam, Here\u0026rsquo;s my experience with preparation for the exam.\nA bit about Exam: Duration: 3 hours (which is fair time if you do lot of practice, I completed in 2 hours 15 mins) Questions: 24 Clusters: 6 After clearing 9 certs to become RHCA (Red Hat Certified Architect), exam environment was kind of familiar to me.\nto be specific, it\u0026rsquo;s browser based exam which is monitored by a proctor, where right half of the screen is your terminal and left half is your question pane.\nLike Red Hat Exams, CKA is also hands-on exam which test your practical, troubleshooting knowledge with kubernetes.\nBeginning: Register for the exam here. Read Candidate Handbook and exam tips carefully. Preparing for the Exam: Phase 1 (Kubernetes Knowledge): Walid Shaari\u0026rsquo;s github repo is extremely useful entrypoint for getting starting around syllabus and many useful links and guide :)\nCreate mind map of Kubernetes docs, it will help you to find something quickly since you can use docs for the reference.\nGet your hands dirty with Kubernetes tasks.\nGo through Kubernetes Concepts and Kubernetes Reference at least twice.\nTo utilise time wisely, familiar yourself with kubectl command the imperative way, that means, instead of writing yamls, try to generate it, Refer Kubectl Cheatsheet and Managing Kubernetes Objects Using Imperative Commands for the same. You might also like to check out blog post about kubectl which I wrote last year.\nFor example,\nTo generate pod defination, I will do, kubectl run mypod --image=busybox --generator=run-pod/v1 Kubernetes Cluster: Kubernetes the hard way by Kelsey Hightower, Please do these at least 5 times. This tutorial is performed on GCP, but you can do it locally as well.\nKinvolk\u0026rsquo;s Kubernetes The Hard way on vagrant is also useful to study.\nAfter learning the hard way, for practicing other topics, you can use same cluster or you can use:\nminikube katacoda kubernetes playground set up with kubeadm (it\u0026rsquo;s so simple) Books: Specifically for the exam, I didn\u0026rsquo;t refer any books, but if you are newbie, it will be awesome if you go through following books: Kubernetes: Up and Running: Dive into the Future of Infrastructure Kubernetes in Action Preparing for the Exam: Phase 2 (Supporting Knowledge): Learn tmux. Since, exam provides only one terminal window, tmux will help to split screen into panes and more windows, here\u0026rsquo;s my cheatsheet for tmux.\nLearn how to work with systemd files,\n# reloading the systemd daemon systemctl daemon-reload # starting and enabling the service systemctl restart \u0026lt;service\u0026gt; systemctl enable \u0026lt;service\u0026gt; Debugging systemd services: systemctl status \u0026lt;service\u0026gt; journalctl -u \u0026lt;service\u0026gt; Before the Exam: Since you can give exam on chrome only, you have to be careful about Ctrl+w which closes the current tab in chrome. To avoid this, My friend Suraj Deshmukh (who also cleared CKA on the same time and his blog on experience with CKA is here ) wrote a blog about disabling Ctrl+w which you can find here. First Minute of Exam: Make alias of few commands as per your convenience and put it in bashrc file, mine were,\nk = \u0026#39;kubectl\u0026#39; kgp = \u0026#39;kubectl get pods\u0026#39; kgs = \u0026#39;kubectl get svc\u0026#39; kgc = \u0026#39;kubectl get componentstatus\u0026#39; kubectl autocompletion, it really helps :) source \u0026lt;(kubectl completion bash) Finally\u0026hellip; Thanks to @kubernauts community for all the guidance and help :) If you are preparing for CKA or if you deal with kubernetes, please join Kubernauts community slack, there are really lovely kubernauts who are ready to help you out :)\nIf you have any further query, feel free to reach out to me via @red_suraj on twitter or Suraj Narwade on linkedin.\nHere\u0026rsquo;s my certificate:\nAll the Best :) ","permalink":"http://localhost:1313/blog/journey-to-cka/","summary":"Hello Folks,\nI appeared for the exam on 19th October 2018 and yesterday(20th Oct 2018), I got the result \u0026amp; I scored 92%, Yay :)\nI got so many request about How to prepare for the exam, Here\u0026rsquo;s my experience with preparation for the exam.\nA bit about Exam: Duration: 3 hours (which is fair time if you do lot of practice, I completed in 2 hours 15 mins) Questions: 24 Clusters: 6 After clearing 9 certs to become RHCA (Red Hat Certified Architect), exam environment was kind of familiar to me.","title":"Journey to the CKA"},{"content":"Check for latest release on github: Check here: https://github.com/minishift/minishift/releases Download latest binary wget https://github.com/minishift/minishift/releases/download/v1.24.0/minishift-1.24.0-linux-amd64.tgz tar -xvf minishift-1.24.0-linux-amd64.tgz cp minishift ~/.local/bin/minishift NOTE: Create ~/.local/bin if it\u0026rsquo;s not present, it\u0026rsquo;s already set in PATH variable.\nSetting up Virtualization environment sudo dnf install libvirt qemu-kvm -y sudo usermod -a -G libvirt $USER newgrp libvirt Setting the KVM driver sudo curl -L https://github.com/dhiltgen/docker-machine-kvm/releases/download/v0.7.0/docker-machine-driver-kvm -o /usr/local/bin/docker-machine-driver-kvm sudo chmod +x /usr/local/bin/docker-machine-driver-kvm Reference: https://docs.okd.io/latest/minishift/getting-started/installing.html https://docs.okd.io/latest/minishift/getting-started/setting-up-virtualization-environment.html ","permalink":"http://localhost:1313/blog/install-minishift-fedora/","summary":"Check for latest release on github: Check here: https://github.com/minishift/minishift/releases Download latest binary wget https://github.com/minishift/minishift/releases/download/v1.24.0/minishift-1.24.0-linux-amd64.tgz tar -xvf minishift-1.24.0-linux-amd64.tgz cp minishift ~/.local/bin/minishift NOTE: Create ~/.local/bin if it\u0026rsquo;s not present, it\u0026rsquo;s already set in PATH variable.\nSetting up Virtualization environment sudo dnf install libvirt qemu-kvm -y sudo usermod -a -G libvirt $USER newgrp libvirt Setting the KVM driver sudo curl -L https://github.com/dhiltgen/docker-machine-kvm/releases/download/v0.7.0/docker-machine-driver-kvm -o /usr/local/bin/docker-machine-driver-kvm sudo chmod +x /usr/local/bin/docker-machine-driver-kvm Reference: https://docs.okd.io/latest/minishift/getting-started/installing.html https://docs.okd.io/latest/minishift/getting-started/setting-up-virtualization-environment.html ","title":"Installing Minishift on Fedora"},{"content":" Configure your shell to use docker daemon of Minishift eval $(minishift docker-env) export user token registry URL export TOKEN=$(oc whoami -t) export REGISTRY_URL=$(minishift openshift registry) Logging in to Registry docker login -u developer -p $TOKEN $REGISTRY_URL OR\nwe can use directly as, docker login -u developer -p $(oc whoam -t) $(minishift openshift registry) Now you can tag images, push it to respective project to create imagestream\ndocker tag mynodejs $(minishift openshift registry)/myproject/nodejs Push the docker image to registry so that it will create imagestream automatically, docker push $(minishift openshift registry)/myproject/nodejs Now you can create application using the new imagestream, oc new-app --image-stream=nodejs --name app ","permalink":"http://localhost:1313/blog/access-minishift-registry/","summary":" Configure your shell to use docker daemon of Minishift eval $(minishift docker-env) export user token registry URL export TOKEN=$(oc whoami -t) export REGISTRY_URL=$(minishift openshift registry) Logging in to Registry docker login -u developer -p $TOKEN $REGISTRY_URL OR\nwe can use directly as, docker login -u developer -p $(oc whoam -t) $(minishift openshift registry) Now you can tag images, push it to respective project to create imagestream\ndocker tag mynodejs $(minishift openshift registry)/myproject/nodejs Push the docker image to registry so that it will create imagestream automatically, docker push $(minishift openshift registry)/myproject/nodejs Now you can create application using the new imagestream, oc new-app --image-stream=nodejs --name app ","title":"Play with OpenShift Internal Docker Registry and imagestreams"},{"content":" Day1 I reached venue (LifeLong Learning Institute, Singapore) at around 10.30 am and volunteers were all set to welcome us with smile and welcome kit :) Venue was awesome with multiple conf rooms and good facilities.\nHarish Pillay(Red Hat) \u0026amp; Damini Satya (SalesForce) Kicked off the event with Awesome Keynote with introducing FOSSASIA, stats about FOSSASIA and schedule as well. Followed by Keynote, Teo Ser Luck (Member of Parliament, Singapore) expressed his thoughts about OpenSource and how it is helping Singapore Governance and Economy. Now it was time for Diamler (Mercedes Benz) folks which was one the sponsors to Conference as well. They explained about how OpenSource is helping Mercedes to become Super Smart Car using technologies like containers, Kubernetes, OpenShift and BlockChain. Next Sponsor Keynote was by Google\u0026rsquo;s Kazunori Sato. it was about How Google Cloud is solving real life problems using AI/ML, it was quite interesting. There was Awesome Exhibition Area where all Sponsors had a booths and school students were demonstrating their exciting projects. I have written detail description of Exhibition Area at the end of this Blog. I met lots of people there including folks from platform.sh which provides PaaS platform, gstar.ai which is interesting startup which combines power of NLP (Natural Language Processing) and BlockChain. Day2 I attended talk by Jan Peuker (Google) which was about system as choreographed behaviour with Kubernetes, whereas he explained about Kubernetes works and he also talked about it\u0026rsquo;s relavance with Borg. At the end of the day, Sudhir Varma (Red Hat) about his work which is making easy Desktop Application easy with tools like DevSuite. During day 2, I met Chris from CNCF. I demoed Kedge, Kompsoe \u0026amp; Kubely to him. He was pretty impressed with projects. Also, I met a web developer. His perception was like, I am web developer, why should I care about containers. Then, I headed for the HonestBees Office (45 mins away from Conference venue) for Local Singapore Kubernetes Meetup. After having snacks, we started with meetup. MEETUP Seth Vargo (Google), started the first session of meetup with his talk, \u0026ldquo;Secret Management in Kubernetes using Vault\u0026rdquo; where he discussed about how we can effectively use vault for secrets in Kubernetes with cool demo. Me talked about Kedge, Kompose \u0026amp; My Personal project Kubely. People loved those project, there were some question as well about how we are reducing number of lines in Kedge without compromising on it\u0026rsquo;s Power. But what I understood, People are happy with Helm. Last session was by Michel Bright (Containous) about Traefik Proxy in Kubernetes. At the end, we took awesome group photo of all meetup attendees. Day3 At 12 pm, all conference attendees gathered at ground floor for group photo. Here\u0026rsquo;s one of the group photo:\nAfter that, at 1 pm, it was time for my talk :) Then I talked to Ria Bhatia(works on virtual kubelet, amazing project) from Microsoft about kedge, She liked the idea of it.\nIn the evening, there was social event with Live Music, Magic Show, Indian Classical Dance Performance along with good veg food and drinks. That was amazing.\nDay4 This was last day at conference. I attended DevSevOps session by Chris van tuin (Who is Red Hatter). After his talk, I catched him in Speaker lounge and showed him kedge, He kinda liked it. Later on, he asked about OSIO access as well. I also presented Kedge at Centos DOJO event. Along with me Kamesh presented talk on Serverles with OpenWhisk. In afternoon session, Chandan \u0026amp; Janki presented a talk on RDO OpenStack \u0026amp; Packaging as well. FOSSASIA EXHIBITION HALL There were some students from ITE polytechnic, Singapore who created some awesome gadgets at their Maker Labs, like Light sabber and Thor Hammer (Electromagnetic Based) This was SUSI, an OpenSouce voice Assistant. It was so amazing. (I am gonna try that. Don\u0026rsquo;t tell Alexa ;) This kids were doing DIY Keychain workshop. I did it well ;) Meanwhile, at conference :) (He is gentoo guy) There was continuous supply of coffee to awake developers by those amazing people :D Made some friends at FOSSASIA :) He Serves for his nation and works on OpenSource on weekends (salute) CNCF COO Awesome Hosts Damini Satya (Salesforce) Harish Pillay (Red Hat) Lovely Person behind FOSSASIA :) Hong Phuc dang Important Links FOSSASIA Talk Videos: https://www.youtube.com/watch?v=M4cMeKYS-0w\u0026list=PLzZVLecTsGpKb1P8J5WLrONMLby5Dn1EV FOSSASIA Photo Gallery: FOSSASIA Social Night Photo Gallery: Thanks FOSSASIA for such amazing conference :) Looking Forward to FOSSASIA Summit 2019 ","permalink":"http://localhost:1313/blog/fossasia18/","summary":"Day1 I reached venue (LifeLong Learning Institute, Singapore) at around 10.30 am and volunteers were all set to welcome us with smile and welcome kit :) Venue was awesome with multiple conf rooms and good facilities.\nHarish Pillay(Red Hat) \u0026amp; Damini Satya (SalesForce) Kicked off the event with Awesome Keynote with introducing FOSSASIA, stats about FOSSASIA and schedule as well. Followed by Keynote, Teo Ser Luck (Member of Parliament, Singapore) expressed his thoughts about OpenSource and how it is helping Singapore Governance and Economy.","title":"FOSSASIA Summit 2018-Event Report"},{"content":"Make sure sshd service is running on your host.\n$ sudo systemctl status sshd â— sshd.service - OpenSSH server daemon Loaded: loaded (/usr/lib/systemd/system/sshd.service; disabled; vendor preset: disabled) Active: active (running) since Wed 2017-12-13 18:23:09 IST; 4min 14s ago Docs: man:sshd(8) man:sshd_config(5) Main PID: 11319 (sshd) Tasks: 1 (limit: 4915) Memory: 4.9M CPU: 333ms CGroup: /system.slice/sshd.service â””â”€11319 /usr/sbin/sshd -D Dec 13 18:23:09 localhost.localdomain systemd[1]: Starting OpenSSH server daemon... Dec 13 18:23:09 localhost.localdomain sshd[11319]: Server listening on 0.0.0.0 port 22. Dec 13 18:23:09 localhost.localdomain sshd[11319]: Server listening on :: port 22. Dec 13 18:23:09 localhost.localdomain systemd[1]: Started OpenSSH server daemon. Create\n","permalink":"http://localhost:1313/blog/hostmount-minishift/","summary":"Make sure sshd service is running on your host.\n$ sudo systemctl status sshd â— sshd.service - OpenSSH server daemon Loaded: loaded (/usr/lib/systemd/system/sshd.service; disabled; vendor preset: disabled) Active: active (running) since Wed 2017-12-13 18:23:09 IST; 4min 14s ago Docs: man:sshd(8) man:sshd_config(5) Main PID: 11319 (sshd) Tasks: 1 (limit: 4915) Memory: 4.9M CPU: 333ms CGroup: /system.slice/sshd.service â””â”€11319 /usr/sbin/sshd -D Dec 13 18:23:09 localhost.localdomain systemd[1]: Starting OpenSSH server daemon... Dec 13 18:23:09 localhost.","title":"Mounting host folders in minishift"},{"content":"supports PV of type hostpath.\nminikube is configured to persist files stored under following directories.\n/data /var/lib/localkube /var/lib/docker /tmp/hostpath_pv /tmp/hostpath-provisioner any other directory will not persist the data after reboot.\nTo mount host directory inside minikube,\n$ minikube mount /host-mount-path:/vm-mount-path for example,\n$ minikube mount ~/mount-dir:/mountexample Mounting /home/user/mount-dir/ into /mountexample on the minikubeVM This daemon process needs to stay alive for the mount to still be accessible... ufs starting This process has to stay open, so open another terminal (if you want more than one mountpath, open one more terminal follow the above procedure)\nMounting inside container,\n--- apiVersion: v1 kind: Pod metadata: name: centos spec: containers: - name: centos image: centos args: - bash stdin: true stdinOnce: true tty: true workingDir: \u0026#34;/mountexample\u0026#34; volumeMounts: - mountPath: \u0026#34;/mountexample\u0026#34; name: host-mount volumes: - name: host-mount hostPath: path: \u0026#34;/mountexample\u0026#34; Another way is to create PV using following YAML and then you can map your host directory path to this directory path,\napiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi hostPath: path: /data/pv0001/ Run kubectl create -f \u0026lt;above yaml file\u0026gt; to create PV.\nAs of now, only hostpath and accessmode (read write once) are supported.\nReference: https://kubernetes.io/docs/getting-started-guides/minikube/ ","permalink":"http://localhost:1313/blog/hostmount-minikube/","summary":"supports PV of type hostpath.\nminikube is configured to persist files stored under following directories.\n/data /var/lib/localkube /var/lib/docker /tmp/hostpath_pv /tmp/hostpath-provisioner any other directory will not persist the data after reboot.\nTo mount host directory inside minikube,\n$ minikube mount /host-mount-path:/vm-mount-path for example,\n$ minikube mount ~/mount-dir:/mountexample Mounting /home/user/mount-dir/ into /mountexample on the minikubeVM This daemon process needs to stay alive for the mount to still be accessible... ufs starting This process has to stay open, so open another terminal (if you want more than one mountpath, open one more terminal follow the above procedure)","title":"Mounting host folders in minikube"},{"content":"I am very bad at git and I always forget the steps. It\u0026rsquo;s always better to write it somewhere. Then why not a blog ?\nTo split your last or recent commit, simply do,\n$ git reset HEAD~ But I wanted to break 3rd commit and split it into two commits, then I did as following way,\n$ git rebase -i HEAD~3 if you dont know the number you can also mention SHA1 of that commit as well,\n$ git rebase master -i xyz^ Then rebase screen will appear, replace pick with edit and save it and do,\n$ git reset HEAD~ Now Commit the individual files and create as much commit as you want and finally do,\n$ git rebase --continue Happy Hacking !!!\nReference: https://stackoverflow.com/questions/6217156/break-a-previous-commit-into-multiple-commits ","permalink":"http://localhost:1313/blog/git/","summary":"I am very bad at git and I always forget the steps. It\u0026rsquo;s always better to write it somewhere. Then why not a blog ?\nTo split your last or recent commit, simply do,\n$ git reset HEAD~ But I wanted to break 3rd commit and split it into two commits, then I did as following way,\n$ git rebase -i HEAD~3 if you dont know the number you can also mention SHA1 of that commit as well,","title":"How to split commits in git"},{"content":"I was watching video about kubectl by janakiram and surprising I found there is also world of commands rather than create, delete, get. kubectl has lots of interesting easter eggs.\nSome of the cool things I found as below,\nList pod along with node name on which they are running kubectl get pods -o wide If you want yaml or json configurations of your application(maybe pod,deployment or service,etc) kubectl get pod web -o=yaml/json CLI hacks to retrieve minimal information (In this case, pod name and node name) kubectl get pod -o wide | awk {\u0026#39;print $1\u0026#34; \u0026#34; $7\u0026#39;} | column -t you can directly edit configurations kubectl edit pod/web You can mention editor of your choice using KUBE_EDITOR variable, KUBE_EDITOR=\u0026#34;sublime\u0026#34; kubectl edit pod/web If we want to get any specific thing from configurations kubectl get pods web -o jsonpath={.spec.containers[*].name} cordon will mark node as unschedulable kubectl cordon \u0026lt;ip-of-node\u0026gt; uncordon will mark node as schedulable again kubectl uncordon \u0026lt;ip-of-node\u0026gt; The given node will be marked unschedulable to prevent new pods from arriving. \u0026lsquo;drain\u0026rsquo; evicts the pods . Otherwise, The \u0026lsquo;drain\u0026rsquo; evicts or deletes all pods except replicationcontrollers pods, etc kubectl drain \u0026lt;node\u0026gt; Drain node \u0026ldquo;foo\u0026rdquo;, even if there are pods not managed by a ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet on it kubectl drain \u0026lt;node\u0026gt; --force Create proxy server between localhost and kubernetes API server kubectl proxy you can mention port of your choice as well, kubectl proxy --port=8000 We can forward local ports to pod (useful for debugging frontend application) kubectl port-forward \u0026lt;pod_name\u0026gt; \u0026lt;host_port\u0026gt;:\u0026lt;container_port\u0026gt; Copy files to and from container to our machine kubectl cp \u0026lt;file\u0026gt; \u0026lt;podname\u0026gt;:\u0026lt;path\u0026gt; Sometime you need shortcuts to increase your speed, productivity something like po for pods, deploy for deployments, you can find all shortcuts here, kubectl explain Thanks janakiram for awesome tutorial.\nReference https://www.youtube.com/watch?v=BadzJOlSn24 https://twitter.com/janakiramm ","permalink":"http://localhost:1313/blog/kubectl/","summary":"I was watching video about kubectl by janakiram and surprising I found there is also world of commands rather than create, delete, get. kubectl has lots of interesting easter eggs.\nSome of the cool things I found as below,\nList pod along with node name on which they are running kubectl get pods -o wide If you want yaml or json configurations of your application(maybe pod,deployment or service,etc) kubectl get pod web -o=yaml/json CLI hacks to retrieve minimal information (In this case, pod name and node name) kubectl get pod -o wide | awk {\u0026#39;print $1\u0026#34; \u0026#34; $7\u0026#39;} | column -t you can directly edit configurations kubectl edit pod/web You can mention editor of your choice using KUBE_EDITOR variable, KUBE_EDITOR=\u0026#34;sublime\u0026#34; kubectl edit pod/web If we want to get any specific thing from configurations kubectl get pods web -o jsonpath={.","title":"Easter eggs in kubectl"},{"content":"Static Pods are managed by kubelet on specific minion. As they are not associated with any controller, APIServer has no control over it.\nOne of the use case for static pod might be for storage like gluster on each minion or maybe for os level debugging on OS like atomic host.\nFor testing it, I referred kelseyhightower\u0026rsquo;s standalone kubelet guide.\nTo install kubelet, we can either download kubelet binary and set it up manually or we can get it from kubernetes-node package,\nsudo dnf install -y kubernetes-node Now, we need to add Path to to the directory containing pod manifest files to run, or the path to a single pod manifest file.\nOpen /etc/kubernetes/kubelet and add --pod-manifest-path=\u0026lt;manifest_path\u0026gt;, In my case, it\u0026rsquo;s /home/vagrant\nso it will look like,\nKUBELET_ARGS=\u0026#34;--cgroup-driver=systemd --pod-manifest-path=/home/vagrant/\u0026#34; You can mention --manifest-url=\u0026lt;URL\u0026gt; as well, so kubelet will download that manifest file from URL and will apply the changes.\nAlso, set --allow-privileged flag to true in /etc/kubernetes/config\nKUBE_ALLOW_PRIV=\u0026#34;--allow-privileged=true\u0026#34; Reload the daemon\nsystemctl daemon-reload Start the service\nsudo systemctl restart kubelet Enable the service\nsudo systemctl enable kubelet Check whether kubelet is running or not,\nsudo systemctl status kubelet Running a static pod I am taking example from official documentation,\napiVersion: v1 kind: Pod metadata: name: static-web labels: role: myrole spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 protocol: TCP As soon as, we put this manifests in /home/vagrant, kubelet will automatically detect it and will start the containers.\nYou can check running containers by docker ps,\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES bd4e42a51f4c docker.io/nginx@sha256:aa1c5b5f864508ef5ad472c45c8d3b6ba34e5c0fb34aaea24acf4b0cee33187e \u0026#34;nginx -g \u0026#39;daemon ...\u0026#34; 4 minutes ago Up 4 minutes k8s_web_static-web-127.0.0.1_default_2bdef00423eeb60e6edec38d373da61d_0 c0c6c1106515 gcr.io/google_containers/pause-amd64:3.0 \u0026#34;/pause\u0026#34; 5 minutes ago Up 5 minutes k8s_POD_static-web-127.0.0.1_default_2bdef00423eeb60e6edec38d373da61d_0 For more, you can follow kelsey hightower\u0026rsquo;s tutorial.\nReference: Static Pods Standalone Kubelet Tutorial by kelsey hightower issue 10093 on kubernetes ","permalink":"http://localhost:1313/blog/static-pod/","summary":"Static Pods are managed by kubelet on specific minion. As they are not associated with any controller, APIServer has no control over it.\nOne of the use case for static pod might be for storage like gluster on each minion or maybe for os level debugging on OS like atomic host.\nFor testing it, I referred kelseyhightower\u0026rsquo;s standalone kubelet guide.\nTo install kubelet, we can either download kubelet binary and set it up manually or we can get it from kubernetes-node package,","title":"Running static pod using standalone kubelet on fedora"},{"content":" Example Use Cases When might you want to use a multi-stage build? It allows you to do an entire pipeline within a single build, rather than having to script the pipeline externally. Hereâ€™s a few examplesâ€¦\nJava apps using WAR files\nFirst stage uses a container with Maven to compile, test, and build the war file Second stage copies the built war file into an image with the app server (Wildfly, Tomcat, Jetty, etc.) Golang apps with standalone binaries\nFirst stage uses a container with golang SDK to build the binary Second stage copies the binary into an image Node.js app needing processed JavaScript for client\nFirst stage uses a Node container, installs dev dependencies, and performs a build (maybe compiling Typescript, Webpack-ify, etc.) Second stage also uses a Node container, installs only prod dependencies (like Express), and copies the distributable from stage one Here, we will take simple hello-world golang program to illustrate single-stage build vs multi-stage build:\n$ cat main.go package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello World!\u0026#34;) } Single stage build Usually for static type langauge as golang, derive Dockerfile from golang SDk, add source, do a build and push it to dockerhub, unfortuantely size was very huge. for example, basic golang:alpine image is of size 257 MB, it increases further more.\nworkaround was : Derive from a Golang base image with the whole runtime/SDK (Dockerfile.build) Add source code Produce a statically-linked binary Copy the static binary from the image to the host (docker create, docker cp) Derive from SCRATCH or some other light-weight image such as alpine (Dockerfile) Add the binary back in Push a tiny image to the Docker Hub\nthat means having two separate Dockerfiles and shell script to control it\nDockerfile.singlestage\nFROM golang:alpine WORKDIR /app ADD . /app RUN cd /app \u0026amp;\u0026amp; go build -o app ENTRYPOINT ./app now build the dockerfile and run it using following commands, $ docker build -t surajnarwade/go-singlestage-app . $ docker run --rm surajnarwade/go-singlestage-app You can check docker image size which is only 259MB: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE go-singlestage-app latest bb2594c6d2fd 3 days ago 259MB Multistage build It removes hassle of maintaining multiple files, so now we have to maintain only one Dockerfile\nsyntax is very simple, we will give label to stages as\nFROM \u0026lt;image\u0026gt; as \u0026lt;label\u0026gt; and then use it in another stage to copy artifacts as, COPY --from=\u0026lt;label\u0026gt; whichever is last FROM statement, that is the final base image Dockerfile.multistage # build stage FROM golang:alpine AS build-env ADD . /src RUN cd /src \u0026amp;\u0026amp; go build -o app # final stage FROM alpine WORKDIR /app COPY --from=build-env /src/app /app/ ENTRYPOINT ./app now build the dockerfile and run it using following commands,\n$ docker build -t surajnarwade/go-multistage-app . $ docker run --rm surajnarwade/go-multistage-app you can see docker image size is reduced which is only approximately 6 MB: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE go-multistage-app latest 975ef40d39ee 3 days ago 5.52MB Reference: https://blog.alexellis.io/mutli-stage-docker-builds/ https://github.com/appleboy/docker-multi-stage-build https://blog.mikesir87.io/2017/03/introducing-docker-multi-stage-builds/ ","permalink":"http://localhost:1313/blog/multi-staging/","summary":"Example Use Cases When might you want to use a multi-stage build? It allows you to do an entire pipeline within a single build, rather than having to script the pipeline externally. Hereâ€™s a few examplesâ€¦\nJava apps using WAR files\nFirst stage uses a container with Maven to compile, test, and build the war file Second stage copies the built war file into an image with the app server (Wildfly, Tomcat, Jetty, etc.","title":"Multistage build in Docker"},{"content":"May times, We have to create demos for projects for which we use various desktop screen recorders along with normal earphones with mic for audio. But, It causes a lot of white/background noise in the video results into degraded video quality.\nAnother case, where we record demos, talks from meetups. In this cases, we are not equipped with smart gadgets but only normal mic or mobile phone due to which lots of white noise occurs in a video.\nI surfed on the internet and found some relevant solutionsm which we will discuss today.\nFirst a fall, we need to separate audio from video, for which we need to run following commands,\nInstall ffmpeg package, $ sudo dnf install ffmpeg -y ``` * Run this command for separating audio from video, $ ffmpeg -i video.mp4 -vn -ac 2 -ar 44100 -ab 320k -f mp3 output.mp3\nIt will create `output.mp3` as audio file. Install audacity for audio processing,\n$ sudo dnf install audacity -y Open output.mp3 audio file in audacity, select any area of noise where there\u0026rsquo;s no talking, (low frequency)\nClick Effects \u0026gt; Noise Reduction \u0026gt; Get noise profile\nNow Select whole audio (Ctrl + A) and goto Effects \u0026gt; Noise Reduction \u0026gt; OK\nNow goto File \u0026gt; Export Audio and save it in format of your choice.\nYo, white noise is removed, now club this audio with your video using your favourite video editors, (I prefer OpenShot)\n*Happy Hacking !!!* ","permalink":"http://localhost:1313/blog/videoguideline/","summary":"May times, We have to create demos for projects for which we use various desktop screen recorders along with normal earphones with mic for audio. But, It causes a lot of white/background noise in the video results into degraded video quality.\nAnother case, where we record demos, talks from meetups. In this cases, we are not equipped with smart gadgets but only normal mic or mobile phone due to which lots of white noise occurs in a video.","title":"Remove white noise from Videos"},{"content":"It was beautiful Saturday morning and we were at yet another chapter of kubernetes Pune meetup. Meetup had a good turnaround of about 30 people, even it was the weekend.\nHarshal Shah started the Session on \u0026ldquo;Lifecycle of a Pod\u0026rdquo;, where he explained in detail about various states of pods, liveness and readiness probes as well as restart policies. That was really helpful as we get to know what actually happens to pod while deploying and undeploying the application.\nNext, in the queue, there was a talk on â€œMaking Kubernetes simpler with Komposeâ€ by Pradeepto and Suraj which was a mind blowing thing for some attendees. Pradeepto spoke about the problem statement that Kompose is trying to solve followed by Komposeâ€™s journey to where it is now. He mentioned how we at Red Hat Developer Engineering became a part of it. Then he elaborated the architecture of Kompose and gave the stage to Suraj.\nFollowing by a short discussion regarding Kompose and its features, Suraj demoed Kompose by deploying few applications and showed how easy it was to deploy microservices on Kubernetes using Kompose. He explained how Kompose converts artifacts for various providers. He also covered various scenarios and edge cases. Along with that, he explained the features of Kompose such as labels which are used for ingress(not supported in docker-compose) as well as build-push support. You can find his demo examples here.\nWe got a lot of questions around Kompose and its features. After the demo, Pradeepto concluded the talk with future for the Kompose and quick introduction to kedgeproject, also he urged attendees to try out both projects and give feedbacks or file issues.\nBy the way, Kedge is a simple and easy way to define and deploy applications to Kubernetes by writing very concise application definitions. Please give it a spin.\nAfter a heavy discussion and demos, we had short snacks break over a tasty samosa and cup of tea. This break seems to be good opportunity to interact with people and networking.\nLast but not the least, Rahul Mahale shared his experience with deploying Rails app using kubernetes. He also discussed problems he faced and how he overcome it with the help of tools such as Prometheus, graphana, etc.\nThanks to all speakers for making this meetup as a great place to learn, Prathmesh and Bigbinary for the venue and constant support for hosting the Kubernetes Meetup Pune. Please stay tuned to the meetup page, we will have our next meetup on August 5th possibly at Red Hat Pune office. See you there.\n","permalink":"http://localhost:1313/blog/k8s-pune-meetup/","summary":"It was beautiful Saturday morning and we were at yet another chapter of kubernetes Pune meetup. Meetup had a good turnaround of about 30 people, even it was the weekend.\nHarshal Shah started the Session on \u0026ldquo;Lifecycle of a Pod\u0026rdquo;, where he explained in detail about various states of pods, liveness and readiness probes as well as restart policies. That was really helpful as we get to know what actually happens to pod while deploying and undeploying the application.","title":"Kubernetes Pune Meetup - July 2017"},{"content":"I went through internet yesterday regarding best practices with golang, I found go-tools written by Dominik Honnef which were pretty interesting and can be . I found some of them useful for me, so are they listed below.\nstatic check applies tons of static analysis checks.\nit needs go 1.6 or later,\ngo get honnef.co/go/tools/cmd/staticcheck syntax is: $ statickcheck [pkg] or [directory] I tried it for kompose with some extra hacks: $ for pkg in $(go list -f \u0026#39;{{ join .Deps \u0026#34;\\n\u0026#34;}}\u0026#39; . | grep \u0026#39;kompose/[^vendor]\u0026#39;); do staticcheck \u0026#34;$pkg\u0026#34;; done cmd/completion.go:48:2: empty branch (SA9003) pkg/transformer/kubernetes/kubernetes_test.go:71:6: identical expressions on the left and right side of the \u0026#39;!=\u0026#39; operator (SA4000) gosimple it gives suggestions for simplifying your code.\nit needs go 1.6 or later:\ngo get honnef.co/go/tools/cmd/gosimple syntax is: $ gosimple [pkg] or [directory] I tried it as follows: $ for pkg in $(go list -f \u0026#39;{{ join .Deps \u0026#34;\\n\u0026#34;}}\u0026#39; . | grep \u0026#39;kompose/[^vendor]\u0026#39;); do gosimple \u0026#34;$pkg\u0026#34;; done pkg/transformer/utils.go:219:2: \u0026#39;if err != nil { return err }; return nil\u0026#39; can be simplified to \u0026#39;return err\u0026#39; (S1013) pkg/transformer/utils_test.go:115:4: the argument is already a string, there\u0026#39;s no need to use fmt.Sprintf (S1025) pkg/transformer/kubernetes/k8sutils.go:346:3: should replace loop with volumes = append(volumes, TmpVolumes...) (S1011) pkg/transformer/kubernetes/k8sutils.go:349:3: should replace loop with volumesMount = append(volumesMount, TmpVolumesMount...) (S1011) pkg/transformer/kubernetes/k8sutils.go:414:6: should omit comparison to bool constant, can be simplified to service.Privileged (S1002) pkg/transformer/kubernetes/k8sutils_test.go:206:7: should omit comparison to bool constant, can be simplified to !hostPid (S1002) pkg/transformer/kubernetes/k8sutils_test.go:243:8: should omit comparison to bool constant, can be simplified to hostPid (S1002) pkg/transformer/kubernetes/k8sutils_test.go:275:5: should omit comparison to bool constant, can be simplified to !output (S1002) pkg/transformer/kubernetes/k8sutils_test.go:284:5: should omit comparison to bool constant, can be simplified to output (S1002) pkg/transformer/kubernetes/k8sutils_test.go:293:5: should omit comparison to bool constant, can be simplified to output (S1002) pkg/transformer/kubernetes/kubernetes_test.go:183:68: should omit comparison to bool constant, can be simplified to !*template.Spec.Containers[0].SecurityContext.Privileged (S1002) pkg/utils/docker/client.go:31:2: \u0026#39;if err != nil { return client, err }; return client, nil\u0026#39; can be simplified to \u0026#39;return client, err\u0026#39; (S1013) unused it checks for unused constants, variables, functions and types.\nit needs go 1.6 or later\ngo get honnef.co/go/tools/cmd/unused syntax is: $ unused [pkg] or [directory] again, I tried it on kompose, purposefully I have added some unused type and function: $ for pkg in $(go list -f \u0026#39;{{ join .Deps \u0026#34;\\n\u0026#34;}}\u0026#39; . | grep \u0026#39;kompose/[^vendor]\u0026#39;); do unused \u0026#34;$pkg\u0026#34;; done pkg/kobject/kobject.go:126:6: type abc is unused (U1000) pkg/kobject/kobject.go:130:6: func hello is unused (U1000) Happy Hacking !!! References: https://dave.cheney.net/2014/09/14/go-list-your-swiss-army-knife https://github.com/dominikh/go-tools ","permalink":"http://localhost:1313/blog/gotools/","summary":"I went through internet yesterday regarding best practices with golang, I found go-tools written by Dominik Honnef which were pretty interesting and can be . I found some of them useful for me, so are they listed below.\nstatic check applies tons of static analysis checks.\nit needs go 1.6 or later,\ngo get honnef.co/go/tools/cmd/staticcheck syntax is: $ statickcheck [pkg] or [directory] I tried it for kompose with some extra hacks: $ for pkg in $(go list -f \u0026#39;{{ join .","title":"tools to adopt best practices in golang"},{"content":"Yesterday, I was working one of the Kompose issue, and I was working on map of string to struct, while iterating over a map I wanted to change elements of struct, so I tried similar to this,\npackage main import \u0026#34;fmt\u0026#34; type Animal struct { count int } func main() { m := map[string]Animal{\u0026#34;cat\u0026#34;: Animal{2}, \u0026#34;dog\u0026#34;: Animal{3}, \u0026#34;mouse\u0026#34;: Animal{5}} fmt.Println(m) m[\u0026#34;dog\u0026#34;].count = 4 fmt.Println(m) } so I got this error,\ntmp/sandbox728133053/main.go:12: cannot assign to struct field m[\u0026#34;dog\u0026#34;].count in map After googling for some time, I found this solution and I tried \u0026amp; it worked as below:\npackage main import \u0026#34;fmt\u0026#34; type Animal struct { count int } func main() { m := map[string]Animal{\u0026#34;cat\u0026#34;: Animal{2}, \u0026#34;dog\u0026#34;: Animal{3}, \u0026#34;mouse\u0026#34;: Animal{5}} fmt.Println(m) var x = m[\u0026#34;dog\u0026#34;] x.count = 4 m[\u0026#34;dog\u0026#34;] = x fmt.Println(m) } I found one more way to do this by storing pointers as shown below:\npackage main import \u0026#34;fmt\u0026#34; type Animal struct { count int } func main() { m := map[string]*Animal{\u0026#34;cat\u0026#34;: \u0026amp;Animal{2}, \u0026#34;dog\u0026#34;: \u0026amp;Animal{3}, \u0026#34;mouse\u0026#34;: \u0026amp;Animal{5}} fmt.Printf(\u0026#34;%#v\\n\u0026#34;,m[\u0026#34;dog\u0026#34;]) m[\u0026#34;dog\u0026#34;].count = 4 fmt.Printf(\u0026#34;%#v\u0026#34;, m[\u0026#34;dog\u0026#34;]) } Happy Hacking !!!\nReference: https://github.com/golang/go/issues/3117 ","permalink":"http://localhost:1313/blog/golang_workaround/","summary":"Yesterday, I was working one of the Kompose issue, and I was working on map of string to struct, while iterating over a map I wanted to change elements of struct, so I tried similar to this,\npackage main import \u0026#34;fmt\u0026#34; type Animal struct { count int } func main() { m := map[string]Animal{\u0026#34;cat\u0026#34;: Animal{2}, \u0026#34;dog\u0026#34;: Animal{3}, \u0026#34;mouse\u0026#34;: Animal{5}} fmt.Println(m) m[\u0026#34;dog\u0026#34;].count = 4 fmt.Println(m) } so I got this error,","title":"Golang workaround for cannot assign to struct field in map"},{"content":"A golang hands-on workshop was organized under meetup group Practical Data Science on May 6th, 2017. It was run up event for Devconf India 2017. Around 60 people turned up. Baiju\u0026rsquo;s, who was workshop instructor, started with introduction of golang.\nHe carried audience through the setup of golang SDK, basic golang semantics, datatypes, control structures, concurrency and other advanced constructs.\nWorkshop course material can be found here.\nMe(SurajN), SurajD and Zeeshan volunteered for the event. We helped attendees for installing golang and also helped them while they were solving exercises given by Baiju.\nAt around 1:15pm, we broke off for lunch.\nPeople were eager to learn golang and the workshop was very interactive.\nPost lunch Bargava explained how golang and Data community converge and what could be the possible future of Data Science in his view.\nBaiju\u0026rsquo;s passion for teaching golang was immense and that inspired folks to follow along.\nMeanwhile, SurajD announced Kubernetes 101 meetup which is going to happen on 20th May. Feel free to ping us if you have a venue for this event.\nThanks to Bargava \u0026amp; Baiju for organizing such an awesome workshop, thanks to Red Hat for sponsoring Venue and food.\nStay tuned for upcoming meetups.\n*Happy Hacking !!!* ","permalink":"http://localhost:1313/blog/goworkshop/","summary":"A golang hands-on workshop was organized under meetup group Practical Data Science on May 6th, 2017. It was run up event for Devconf India 2017. Around 60 people turned up. Baiju\u0026rsquo;s, who was workshop instructor, started with introduction of golang.\nHe carried audience through the setup of golang SDK, basic golang semantics, datatypes, control structures, concurrency and other advanced constructs.\nWorkshop course material can be found here.\nMe(SurajN), SurajD and Zeeshan volunteered for the event.","title":"The GO Workshop by Baiju"},{"content":" Recently, I installed latest docker on my machine. But I noticed lots of new features, commands and aliases to the existing commands and felt it was worth posting.\nDocker version that I installed is 17.03.0-ce, Click here to know how to install Docker\nAs we can see in docker help, commands are grouped as management commands under names like container, network, image, etc as per their purpose. $ docker --help Usage:\tdocker COMMAND A self-sufficient runtime for containers ... ... Management Commands: container Manage containers image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker volume Manage volumes Commands: ... ... Run \u0026#39;docker COMMAND --help\u0026#39; for more information on a command. For example, Under docker container, all commands related to managing to containers are classified. $ docker container --help Usage:\tdocker container COMMAND Manage containers Options: --help Print usage Commands: attach Attach to a running container commit Create a new image from a container\u0026#39;s changes cp Copy files/folders between a container and the local filesystem create Create a new container ... ... wait Block until one or more containers stop, then print their exit codes Run \u0026#39;docker container COMMAND --help\u0026#39; for more information on a command. There is new feature here, which makes plugin management in docker easier, now we can easily install or delete or get information about plugins. $ docker plugin --help Usage:\tdocker plugin COMMAND Manage plugins Options: --help Print usage Commands: create Create a plugin from a rootfs and configuration. Plugin data directory must contain config.json and rootfs directory. disable Disable a plugin enable Enable a plugin inspect Display detailed information on one or more plugins install Install a plugin ls List plugins push Push a plugin to a registry rm Remove one or more plugins set Change settings for a plugin upgrade Upgrade an existing plugin New Aliases: docker container ls is same as docker ps $ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 66d17f3bc07e hello-world \u0026#34;/hello\u0026#34; About a minute ago Exited (0) About a minute ago ecstatic_lamarr docker image ls is same as docker images # docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE fedora latest 0047cca29c6f 5 days ago 230 MB hello-world latest 48b5124b2768 2 months ago 1.84 kB New docker system command added: docker system df - Show docker disk Usage # docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 1 0 1.84 kB 1.84 kB (100%) Containers 0 0 0 B 0 B Local Volumes 0 0 0 B 0 B docker system events - Get real time events (like oc get events in openshift)\ndocker system info - Displays System-wide information\ndocker system prune - Removes unused data\nWARNING! This will remove: - all stopped containers - all volumes not used by at least one container - all networks not used by at least one container - all dangling images Are you sure you want to continue? [y/N] New prune feature added: docker container prune - Remove all stopped containers\ndocker image prune - Remove unused images\ndocker network prune - Remove all unused networks\ndocker volume prune - Remove all unused volumes\nabove all things can be achieved by using one command also,\ndocker system prune (WARNING!, be careful with this command.) #Reference:\nClick here for more command line reference. Thanks, Happy Hacking !!! ","permalink":"http://localhost:1313/blog/whats-new-in-docker/","summary":"Recently, I installed latest docker on my machine. But I noticed lots of new features, commands and aliases to the existing commands and felt it was worth posting.\nDocker version that I installed is 17.03.0-ce, Click here to know how to install Docker\nAs we can see in docker help, commands are grouped as management commands under names like container, network, image, etc as per their purpose. $ docker --help Usage:\tdocker COMMAND A self-sufficient runtime for containers .","title":"What's new in Docker ?"},{"content":"Hello guys, its been so long, I havn\u0026rsquo;t written any post. Recently, I have migrated my blog from Pelican(python based static site generator) to Hugo(GO based static site generator). Today, I will write about it. You can explore my blog at surajnarwade.github.io\nInstalling Hugo .\nInstall hugo $ go get -v github.com/spf13/hugo Check whether it is properly installed or not $ hugo help Using Hugo .\nWe need to tell hugo to generate blog skeleton, type following command for that: $ hugo new site myblog myblog directory will be created, checkout into directory: $ cd myblog $ tree . â”œâ”€â”€ archetypes â”œâ”€â”€ config.toml â”œâ”€â”€ content â”œâ”€â”€ data â”œâ”€â”€ layouts â”œâ”€â”€ static â””â”€â”€ themes Now its time to write to post. following command will create a post with predefined skeleton. you can create your own archtypes for this. Edit the post with markdown syntax. Click here for markdown cheatsheet. $ hugo new post/good-to-great.md Compile the blog using following command and our website will be generated under public/ directory. $ hugo Hugo comes with server while reloads your browser live as soon as there\u0026rsquo;s change in a file so, you will get a instant feedback. $ hugo server ... .... Watching for changes in /home/snarwade/myblog/{data,content,layouts,static,themes} Serving pages from memory Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop Now Open localhost:1313 on your browser to see the website. you can make changes to blogpost file to see live changes.\nI have used nix theme for my blog, but there are bunch of beautiful themes for hugo from which you can choose, Click here to get browse hugo themes.\n$ cd themes $ git clone https://github.com/LordMathis/hugo-theme-nix Update config.toml file languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;\u0026lt;your title\u0026gt;\u0026#34; baseURL = \u0026#34;https://\u0026lt;username\u0026gt;.github.io/\u0026#34; theme = \u0026#34;your theme name\u0026#34; disqusShortname = \u0026#34;your disqus shortname\u0026#34; [params] GithubID = \u0026#34;your_github\u0026#34; GitlabId = \u0026#34;your_gitlab\u0026#34; TwitterID = \u0026#34;your_twitter\u0026#34; LinkedInID = \u0026#34;your_linkedin\u0026#34; GoogleplusID = \u0026#34;your_googleplus\u0026#34; FacebookID = \u0026#34;your_facebook\u0026#34; InstagramID = \u0026#34;your instagram\u0026#34; Name = \u0026#34;your_name\u0026#34; HeaderUsername = \u0026#34;username\u0026#34; HeaderHostname = \u0026#34;hostname\u0026#34; Email = \u0026#34;your_email\u0026#34; About = \u0026#34;info_about_you\u0026#34; ProfilePicture = \u0026#34;profile_picture_url\u0026#34; GoogleAnalytics = \u0026#34;your_google_analytics_id\u0026#34; Deploying blog to Github Create repository named as .github.io, this will be the URL for your blog. Here, Master branch will serve contents for the blog. $ git clone git@github.com:\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io.git Copy the contents from public directory to repository and push the contents to github $ cp -r public/* \u0026lt;username\u0026gt;.github.io/ $ cd \u0026lt;username\u0026gt;.github.io $ git add --all $ git commit -m \u0026#39;First commit\u0026#39; $ git push You will have your own new blog at \u0026lt;username\u0026gt;.github.io Happy hacking || Happy writing !!!\n(Comments are most welcome)\n","permalink":"http://localhost:1313/blog/migration-to-hugo/","summary":"Hello guys, its been so long, I havn\u0026rsquo;t written any post. Recently, I have migrated my blog from Pelican(python based static site generator) to Hugo(GO based static site generator). Today, I will write about it. You can explore my blog at surajnarwade.github.io\nInstalling Hugo .\nInstall hugo $ go get -v github.com/spf13/hugo Check whether it is properly installed or not $ hugo help Using Hugo .\nWe need to tell hugo to generate blog skeleton, type following command for that: $ hugo new site myblog myblog directory will be created, checkout into directory: $ cd myblog $ tree .","title":"Migration to Hugo"},{"content":"Yesterday I was reading about magnum component in OpenStack, then I checked with installing OpenStack Newton by following RDO doc. but it seems that, packstack answer file is not ready with magnum component yet. but fortunately, I found a patch https://review.openstack.org/#/c/360388/ about adding magnum deployment in packstack, so I decided to test this patch via installing openstack-packstack through source as per discussion with Chandan Kumar and Javier PeÃ±a\nhave used centos 7 box for this purpose. Installation\n$ sudo yum install -y git python-pip $ git clone git://github.com/openstack/packstack.git then, I have to download that patchset, for that, I installed git-review\n$ sudo pip install git-review $ cd packstack Make sure you have configured your gerrit credentails on OpenStack Gerrit for reference, http://docs.openstack.org/infra/manual/developers.html\n$ git remote -s # It will ask for openstack gerrit username $ git review -d \u0026lt;change-id of patchset\u0026gt; $ git rebase -i master $ sudo python setup.py install installation of OpenStack-puppet-modules\n$ export GEM_HOME=/tmp/somedir $ sudo yum install rubygems -y $ gem install r10k We have used sudo -E, because it preserves environment variables as we declared GEM_HOME\n$ sudo -E /tmp/somedir/bin/r10k puppetfile install -v $ sudo cp -r packstack/puppet/modules/packstack /usr/share/openstack-puppet/modules Now it\u0026rsquo;s time to generate answer file and edit the contents: as my target was Magnum component,\n# packstack -d --gen-answer-file=answerfile.txt # Specify \u0026#39;y\u0026#39; to install OpenStack Container Service (magnum). [\u0026#39;y\u0026#39;, # \u0026#39;n\u0026#39;] CONFIG_MAGNUM_INSTALL=y Run the packstack by giving path of answerfile\n# packstack -d --answer-file=answerfile.txt In this way, I tested patch with packstack locally. I will write about Magnum component in next blog post.\nHappy Hacking!\n","permalink":"http://localhost:1313/blog/blog17/","summary":"Yesterday I was reading about magnum component in OpenStack, then I checked with installing OpenStack Newton by following RDO doc. but it seems that, packstack answer file is not ready with magnum component yet. but fortunately, I found a patch https://review.openstack.org/#/c/360388/ about adding magnum deployment in packstack, so I decided to test this patch via installing openstack-packstack through source as per discussion with Chandan Kumar and Javier PeÃ±a\nhave used centos 7 box for this purpose.","title":"How to install OpenStack-packstack from source and test individual patches locally!"},{"content":"Woo! It was so exciting, because It was my First [PyCon]( https://in.pycon.org/2016/`_) ,First conference, First flight and First time Delhi.\nWas so excited to meet all new people who were known over IRC,dgplug and social media volunteer(PyCon) only. We landed in the capital city at around 2 am in the morning (terrible experience in Indigo flight) . Then, we went to our Stay point Mulberry house (one of my memorable places in Delhi) :P Day1\nWe reached the venue early to have delicious breakfast (Since PyCon is known for its food too) and setting up for volunteering. I volunteered â€œDemystifying the Django REST Framework: Web Developmentâ€ By Haris Ibrahim K. V. The strength of the hall was quite good :). The workshop was really helpful as I get to know about Django also.\nThen, we headed up for lunch. In the afternoon session, I was the volunteer lead for â€œScaling Django with Kubernetes: Infrastructureâ€ by Saket Bhushan, I introduced Saket to the crowd and handed over the stage to him. I was aware of kubernetes, but got to know about how Django app is deployed with kubernetes as well as how to deploy kubernetes UI which helps in monitoring over the cluster. We also had hands-on session in the workshop. Devsprints also started on the Day1, there were lots of projects like pagure, ansible containers, sympy etc. Unfortunately, I was unable to attend the dev sprint due to some other work. The day ended with volunteer gathering, feedback session and next day planning :)\nDay2\nDay2 started with KG award(one of the prestigious awards in community ) felicitation ceremony to Dr Ajith Kumar B.P.\nNow It was time to encourage, appreciate the efforts of Python Express volunteers. We all gathered on the stage. It was an awesome feeling( huge round of applause) :). We, volunteers conducted Around 88 workshops in 11 states in this python month(10th august to 10th September). Thinking of such grand success, we have decided to run this python express for all 365 days.\nMuch awaited keynote was delivered by Baishampayan Ghose, known as B.G. is the CTO of Helpshift,Inc. He discussed lots of things regarding programmer, systems, etc some of his key points were:\n\u0026ldquo;Programmer think like bricklayersâ€ â€œThe world is not a program, it\u0026rsquo;s a system\u0026hellip; and it\u0026rsquo;s distributedâ€ .\nMeanwhile, I was at Red Hat booth too, we distributed lots of stickers as well as we also introduced RDO for the first time in a conference in India\nLots of geeks were gathered and crowded Red Hat booth, were curious about RedHat , RDOproject, solved some there queries too :)\nIt was around 4.30 pm and it was time for our 2nd keynote speaker, Van Lindberg , Vice President of RackSpace. Some of the Key points from his talk : We should discuss failure also because it also plays essential role in software development as failure is the source of GROWTH and responding to failure is a personal responsibility.\nThings that PyCon makes better:\nCome to Contribute Meet someone new Say Thanks! Dealing with failure:\nIncrease skill or capacity Increase domain knowledge Think more clearly and powerfully Avoiding Failure:\nMake our assumptions explicit Test our assumptions Embrace change Day 2 ended with PyCon India Dinner Party for volunteers and speakers at BBQ Nation Vasant Kunj, Delhi, It was full of fun and enjoyment. Meanwhile, I found something interesting near BBQ, MINI cooper on a wall :)\n.\nDay3\nStarted with the keynote of Andreas Mueller (Research Engineer at the NYU Center for Data Science)\nAs per Andreas,\nMachine learning is everywhere. He discussed how machine learning is important for user experience in online shopping. He also talked about the past, present, future also scikit-learn to package, how open source tools are changing machine learning In afternoon, we had dgplug staircase(actually ramp :P) meeting which takes place every year in PyCon India :), Kushal took feedback regarding summer training program. Lots of discussion happened regarding how dgplug can be better, IRC, why to contribute to opensource. Kushal introduced us to his mentor, Sirtaj Singh Kang who helped him to get started with python After that, Kushal presented Red Hat sponsored talk on â€œPython in Red Hat familyâ€, How python plays important role in Red Hatâ€™s Opensource projects like Anaconda, RDO, ansible, etc(Pythonistas by heart) Vote of thanks was given by anuvrat Parashar, and announcement of Next PyCon was done. Finally, we had #dgplug group photo, photo sessions and goodbye with lots of memories and a good stock of knowledge, new friends. Finally thanked to my social media volunteer colleagues Shashank Aryan and Shweta Suman.\nThanks, Sayan, Kushal for awesome clicks. Click here for photo collection from all volunteer and attendees\nClick for Kushal das\u0026rsquo;s and Sayan Chowdhary\u0026rsquo;s clicks.\n","permalink":"http://localhost:1313/blog/blog16/","summary":"Woo! It was so exciting, because It was my First [PyCon]( https://in.pycon.org/2016/`_) ,First conference, First flight and First time Delhi.\nWas so excited to meet all new people who were known over IRC,dgplug and social media volunteer(PyCon) only. We landed in the capital city at around 2 am in the morning (terrible experience in Indigo flight) . Then, we went to our Stay point Mulberry house (one of my memorable places in Delhi) :P Day1","title":"PyCon India 2016 - Weekend Iâ€™ll never forget"},{"content":"Python Pune July Meetup was held at Red Hat, Pune on 30th July 2016, this meetup was quite different than previous meetups because talks were given by Interns. Around 40 pythonistas were present for meetups, some were students and some were professionals. Topics for the meetup were as below:\nHow do I automate boring stuff using Python? by Suraj Narwade Writing unit tests for any Python Script by Sudhir Verma Static v/s Dynamically typed languages by Ganesh Kadam Regular Expressions in Python by Rahul Bajaj Meetup started with Rahul Bajaj\u0026rsquo;s topic regarding Regular Expressions. He explained about regex, it\u0026rsquo;s requirement,how to use it along with simple examples. Click here for Rahul\u0026rsquo;s slides\nRahul Talking about regex\n.\nI explained about how I automate daily boring stuffs as well as I explained how I automate Pycon India volunteering tasks (in which python script automatically tweets about talk proposal voting every 2 hour as well as retweet PyCon tweets automatically),\nLinks for my talk:\nScript for volunteering PyCon India Gist of all small scripts sildes Me talking about Automating Boring Stuff\n.\nChandan extended my topic and asked pythonistas to share their ideas regarding boring stuff in their day to day life. we collected those ideas in etherpad and will work on it soon. Click here for etherpad\nAs Unit Testing is known as important part of software development process, We can determine the working of code and how it will react to various inputs. It is an excellent way to detect defects and refactor the code. Sudhir explained unit test, its working along with simple examples.\nClick Here for his slides.\nSudhir Talking on Unit Testing\n. Last Talk was Static vs. Dynamic Languages by Ganesh Kadam , He talked about Why AsciiNema ported back to python from Golang as well how language should be chosen depend on requirement. He also explained difference between static and dynamic languages. Click here for his slides\nGanesh Talking about Static \u0026amp; Dynamic Languages\n.\nFinally Chandan told about Openstack community and took positive feedbacks regarding meetup from attendees and discussed about upcoming meetups. Thank You Red Hat Pune for organizing this meetup.\nPython Pune July Meetup Group Photo\n","permalink":"http://localhost:1313/blog/blog15/","summary":"Python Pune July Meetup was held at Red Hat, Pune on 30th July 2016, this meetup was quite different than previous meetups because talks were given by Interns. Around 40 pythonistas were present for meetups, some were students and some were professionals. Topics for the meetup were as below:\nHow do I automate boring stuff using Python? by Suraj Narwade Writing unit tests for any Python Script by Sudhir Verma Static v/s Dynamically typed languages by Ganesh Kadam Regular Expressions in Python by Rahul Bajaj Meetup started with Rahul Bajaj\u0026rsquo;s topic regarding Regular Expressions.","title":"It's our time Now !!! PythonPune July Meetup"},{"content":"Hello Everyone, From Past few days, I was studying Jenkins,\nhave read VMs or cloud instances can be provisioned as the slave for Jenkins, I thought it would be awesome to have more generic setup. So I dig more into the web and found Jenkins Docker Plugin. so that we can provision Jenkins slave on Docker host. and this Docker host can be anywhere on any machine or any cloud instance.I have experimented it and I managed to contributr Jenkins fedora slave image to Fedora Cloud, https://github.com/fedora-cloud/Fedora-Dockerfiles/tree/master/jenkins-slave Few Reasons for Container based slaves:\nEach image provides a clean environment in which a build can be run. They are relatively lightweight (compared to a full VM They can be saved/tagged to provide a record of the resulting build output What Jenkins docker plugin does ? it will able to use docker host so that it will dynamically provision slave run a single build tear down the slave\nSetting up the Docker host:\nMake sure that Firewall is disabled sudo systemctl stop firewalld sudo systemctl disable firewalld Install docker engine sudo dnf update -y sudo dnf install docker -y Replace following lines with \u0026ldquo;ExecStart\u0026rdquo; under [Service] section in /usr/lib/systemd/system/docker.service ExecStart= ExecStart=/usr/bin/docker daemon -H fd:// -H tcp://0.0.0.0:2376 Start Docker daemon Recently, I have submitted docker image in dockerhub, so you can pull that image using following command, docker pull surajnarwade/jenkins-fedora-slave Same image(Dockerfile) I requested to merge in Fedora-cloud, and the community happily accepted it, Hence my first Fedora Contribution :)\nClick Here to access my contributed Dockerfile on fedora cloud.\nDownload Jenkins Docker plugin in Jenkins, goto \u0026ldquo;Manage Jenkins\u0026rdquo;-\u0026gt;\u0026ldquo;Configure System\u0026rdquo;, Scroll Down to bottom of the page, you should find an option to \u0026ldquo;Add a new cloud\u0026rdquo;, Click on it and select Docker option.\nDocker URL should point to IP address of Docker host.\nPut name of Docker image in ID field.\nPut \u0026rsquo;label\u0026rsquo; Labels field, so that Jenkins projects identify this as docker cloud provider.\nIf everything sets up correctly, the \u0026ldquo;Test Connection\u0026rdquo; button should return the current version of Docker running on the host.\nPut label of Docker Cloud in Label Expression field in Project configurations, so now Jenkins should start up a new Docker container, run the build, and then shut down the container. Conclusion\nThe integration between Jenkins and Docker is still at development stage, so it may take some time and efforts to get them working nicely together. Capability to start new build from clean image is very important thing when it comes to point of reproducible builds.\n","permalink":"http://localhost:1313/blog/blog14/","summary":"Hello Everyone, From Past few days, I was studying Jenkins,\nhave read VMs or cloud instances can be provisioned as the slave for Jenkins, I thought it would be awesome to have more generic setup. So I dig more into the web and found Jenkins Docker Plugin. so that we can provision Jenkins slave on Docker host. and this Docker host can be anywhere on any machine or any cloud instance.","title":"Jenkins Docker Integration along with Fedora Contribution"},{"content":" Because you are new to python If you want to know python or want to get better at python, this is one of the best avenue to learn and hack. Make new friends having same interest, have a fun !!!\nBecause you are hiring Are you looking for good people like developers, admins, DevOps for your organization ? PyCon India is the best place where you can find around 1200 to 1350 of the best and brightest people which are pythonist and some of them will be interested in new opportunities. Sponsering PyCon india is the best way to get noticed by these pythonistas.\nClick here to Download Sponsership Prospectus\nBecause you want to meet pythonist whom you just know by IRC nick Come out of your cubicle/from your dark room and meet those people whom you just know by IRC nick, people who solved your queries once ! :) Meet new buddies, share ideas, interact with them, colloborate and work :)\nBecause you want to know what are the latest trends that pythonists are working on Excellent talks will be presented which will let you know about latest trends, also you will get to know various interesting, new things at PyCon india.\nclick here to vote for your favourite proposal\nBecause it\u0026rsquo;s time for Delhi Awesome place, crowdy, delicious street foods and it\u0026rsquo;s capital city :)\nLast but not the least, Because you want to taste Delicious Food Don\u0026rsquo;t miss delicious food, which will keep you up for whole day event :P\nFollow Pycon India on twitter for more upcoming updates\nIf you havn\u0026rsquo;t registered yet for PyCon India, Click here to grab your ticket\nMe ?\nSuraj Narwade\nProudly, OpenSource Contributor and Volunteer for PyCon India 2016\nIRC nick - snarwade\n","permalink":"http://localhost:1313/blog/blog13/","summary":"Because you are new to python If you want to know python or want to get better at python, this is one of the best avenue to learn and hack. Make new friends having same interest, have a fun !!!\nBecause you are hiring Are you looking for good people like developers, admins, DevOps for your organization ? PyCon India is the best place where you can find around 1200 to 1350 of the best and brightest people which are pythonist and some of them will be interested in new opportunities.","title":"Why You Should Attend PyCon India 2016?"},{"content":"When you request container from image,docker usually it looks for container image locally on a system, if its not there, it will download it to your system from somewhere, i.e. Docker Hub Regitry\nWhat is Private Docker Registry ?\nIf you want to keep your images private to yourself only, instead of going them over internet, you want to save to time by pushing and pulling them locally, at that point, Private Docker Registry comes into picture.\nNOTE :- Docker is not required for Docker registry Server.\nFollowing example shows how to deploy Docker registry on fedora(22 or later):\nSetting Up a Docker Registry in Fedora\nInstall docker registry package: $ dnf install docker-registry You may need to open TCP port 5000 to allow access to Docker registry service, assuming that you are using firewalld service. $ firewall-cmd --permanent --add-port=5000/tcp $ firewall-cmd --reload Start and Enable service and begin using it. $ systemctl start docker-registry $ systemctl enable docker-registry Check status of service by following command, if its active and running, then it means that docker registry is properly up and running. $ systemctl status docker-registry Allow access to registry for Docker Client\nAllow docker client to access registry by making changes in /etc/sysconfig/docker file as following lines (As We have not used any SSL security certificate here, hence we are adding it to insecure registry field) ADD_REGISTRY=\u0026#39;--add-registry localhost:5000\u0026#39; INSECURE_REGISTRY=\u0026#39;--insecure-registry localhost:5000\u0026#39; Now, restart docker service: $ systemctl restart docker Tag your private image, so that you can push it to local/private docker registry(For ex.image name is private-image) $ docker tag private-image localhost:5000/private-image:latest Push the private image into local docker registry by typing following commands: $ docker push localhost:5000/private-image:latest To make sure you can properly pull image from private registry, remove current image from system. $ docker rmi private-image localhost:5000/private-image:latest Now, Try to pull image from private registry. $ docker pull localhost:5000/private-image:latest Check Image is downloaded or not by following command \u0026amp; it should show output as below: $ docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE localhost:5000/private-image latest 91c95931e552 5 weeks ago 910 B Thank you !!!\n","permalink":"http://localhost:1313/blog/blog11/","summary":"When you request container from image,docker usually it looks for container image locally on a system, if its not there, it will download it to your system from somewhere, i.e. Docker Hub Regitry\nWhat is Private Docker Registry ?\nIf you want to keep your images private to yourself only, instead of going them over internet, you want to save to time by pushing and pulling them locally, at that point, Private Docker Registry comes into picture.","title":"How to Setup Private Docker Registry in Fedora/CentOS ?"},{"content":"Python Pune Meetup was held @ Red Hat, Pune and that was about Part III of Machine learning by Sudarshan Gadhave on 25th June 2016.\nThis time, along with Chandan, Me, ganesh and satyajit were the Event Host :)\nMeetup was specifically targeted as hackathon based on python pandas and numpy what we have covered in previous meetup.\nMeetup was titled as Data DOJO,(a bit weird) it means practicing and developing skills along with brainstorming ideas, real time problems to solve.\nAround 25 peoples were there, some were to new, some were intermediate for data science. But, most of the people were familiar with python. We at DATA DOJO, gave them opportunity to learn python-pandas, matplotlib, basic data science operations.\nFew Peoples presented their work on data sets, and got awesome goodies too :) we have used following data sets for example:\n[Adult data Set](Adult Data Setshttp://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data) Train Data set Links for work done by attendees as following:\nVikas Gaikwad Ketul Gupta Chaitanya Kukde Tanay Prabhudesai Congrats people :)\nThose who missed previous meetups can follow these links : https://github.com/sudarshan1413/python_pandas_meetup_2016\nOur next meetup will be last part of Data science series based on scikit-learn Stay Connected to Python Pune for upcoming meetups !!! If any suggestions are there, Feel free to contact us !!! Thank you !!!\nData Science Enthusiast at Meetup: .\n","permalink":"http://localhost:1313/blog/blog12/","summary":"Python Pune Meetup was held @ Red Hat, Pune and that was about Part III of Machine learning by Sudarshan Gadhave on 25th June 2016.\nThis time, along with Chandan, Me, ganesh and satyajit were the Event Host :)\nMeetup was specifically targeted as hackathon based on python pandas and numpy what we have covered in previous meetup.\nMeetup was titled as Data DOJO,(a bit weird) it means practicing and developing skills along with brainstorming ideas, real time problems to solve.","title":"Machine Learning Begins Here-Part III !!! #pythonpune"},{"content":"What is Pycon India ?\nPyCon India, the premier conference in India on using and developing the Python programming language is conducted annually by the Python developer community. It attracts the best Python programmers from across the country and abroad.\nWhen \u0026amp; Where it is happening ?\n23-25 Sepetember 2016\nConvention Centre Jawahar Lal Nehru University, New Mehrauli Road, Near Munirka, New Delhi - 110067\nWhy to Sponser Us ?\nAre you Pythonist, wanna talk, Submit proposal NOW !!!\nCFP for #pyconindia are open! Following is link to submit talk proposal as well as guidelines are also provided. Click here to submit your talk proposal\n(Last date for submission is 29th June, 2016)\nBefore you submit, Have a look at this:\nA Hypocriteâ€™s Guide to Public Speaking Pycon India 2015 talks For more contact\nIRC - #python-india Subscribe to Mailing List Twitter Facebook Who Am I ?\nPycon India Social Media Volunteer\n","permalink":"http://localhost:1313/blog/blog10/","summary":"What is Pycon India ?\nPyCon India, the premier conference in India on using and developing the Python programming language is conducted annually by the Python developer community. It attracts the best Python programmers from across the country and abroad.\nWhen \u0026amp; Where it is happening ?\n23-25 Sepetember 2016\nConvention Centre Jawahar Lal Nehru University, New Mehrauli Road, Near Munirka, New Delhi - 110067\nWhy to Sponser Us ?","title":"Pycon India 2016,New Delhi !!! Hurry Up guys !!!"},{"content":"Python Pune Meetup was held @ Amazatic Solutions and that was about Part II of Machine learning by Sudarshan Gadhave on 28th May 2016. Meetup was specifically targeted on python pandas as last meetup already covered basics of data science.\nSudarshan started from Required tools/libraries for machine learning :\nIpython numpy python-pandas python-scikit-learn matplotlib These are best opensource alternatives to softwares like MATLAB !!\nMachine learning is all about technical,business and statistics. if there is small data sets. that can be done manually, but if there are thousands of rows and columns in data stats, then what ? most of the time of Data Scientists is consumed to get the data,analyze the data,sepeartion of data,and cleaning of data, that means almost 70% of the total work in intial phase. Later comes the actual business logic and model building.\nwe have used titanic data sets for example (https://github.com/pcsanwald/kaggle-titanic/blob/master/train.csv).\nThen, Following things we have covered:\nNumpy basics Pandas Data Structures and basics Data Loading Data Wrangling (Clean, Transform, Merge) Data Aggregation and grouping operations Exploratory Data Analysis and Descriptive Statistics Plotting and Visualization Features/comarison of Numpy \u0026amp; pandas:\nnumpy:\nbuilt for fast array processing, vector operations 15 times faster then list made for scientific calculations python pandas :\nDatastructure series - 1d numpy array stores data. its like column in dataframe context series also has the index and the value We can change the values in series using index Dataframe is collection of series like excel sheet More the memory More data can be analysed in python pandas Click Here to know more about pandas from beginning\nWe also discussed about the Matplotlib,Matplotlib is the library used to mainly produce the charts related to the data. You can generate plots, histograms, power spectra, bar charts, errorcharts, scatterplots, etc, with just a few lines of code. I found this as good and free alternative to MATLAB.\nSudarshan Suggested to go through this book to know more \u0026amp; from very basics towards machine learning as well Data science: Python for data Analysis\nThose who missed meetup can follow these links : https://github.com/sudarshan1413/python_pandas_meetup_2016\nThank you Amazatic Solutions, and Stay Connected to Python Pune for upcoming meetups !!!\nSelfie @meetup @Amazatic by Sudarshan:\n.\n","permalink":"http://localhost:1313/blog/blog9/","summary":"Python Pune Meetup was held @ Amazatic Solutions and that was about Part II of Machine learning by Sudarshan Gadhave on 28th May 2016. Meetup was specifically targeted on python pandas as last meetup already covered basics of data science.\nSudarshan started from Required tools/libraries for machine learning :\nIpython numpy python-pandas python-scikit-learn matplotlib These are best opensource alternatives to softwares like MATLAB !!\nMachine learning is all about technical,business and statistics.","title":"Machine Learning Begins Here-Part II !!! @ #pythonpune"},{"content":"Kubernetes\nKubernetes is a system, developed by google, for managing containerized applications in a clustered environment. It provides basic mechanisms for deployment, maintenance and scaling of applications on public, private or hybrid setups. It also comes with self-healing features where containers can be auto provisioned, restarted or even replicated. It aims to provide a \u0026ldquo;platform for automating deployment, scaling, and operations of application containers across clusters of hosts\u0026rdquo;\n.\nIn this blog post, weâ€™ll install a Kubernetes cluster with three minions on Fedora 23, we will also see example on how to manage pods.\nPrerequisites ::\n$ systemctl stop firewalld $ systemctl disable firewalld Setting up the Kubernetes Master\n1.Install etcd and kubernetes using dnf ::\n$ dnf -y install etcd kubernetes 2.Configure etcd to listen all IP addresses, Following lines are to be uncommented and assigned with respective values in /etc/etcd/etcd.conf: ::\nETCD_NAME=default ETCD_DATA_DIR=\u0026#34;/var/lib/etcd/default.etcd\u0026#34; ETCD_LISTEN_CLIENT_URLS=\u0026#34;http://0.0.0.0:2379\u0026#34; ETCD_ADVERTISE_CLIENT_URLS=\u0026#34;http://localhost:2379 3.Configure Kubernetes API server by editing /etc/kubernetes/apiserver as below: ::\nKUBE_API_ADDRESS=\u0026#34;--address=0.0.0.0\u0026#34; KUBE_API_PORT=\u0026#34;--port=8080\u0026#34; KUBELET_PORT=\u0026#34;--kubelet_port=10250\u0026#34; KUBE_ETCD_SERVERS=\u0026#34;--etcd_servers=http://127.0.0.1:2379\u0026#34; KUBE_SERVICE_ADDRESSES=\u0026#34;--service-cluster-ip-range=10.254.0.0/16\u0026#34; KUBE_ADMISSION_CONTROL=\u0026#34;--admission_control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota\u0026#34; KUBE_API_ARGS=\u0026#34;\u0026#34; 4.We have to Start and enable these servies; etcd, kube-apiserver, kube-controller-manager and kube-scheduler: ::\n$ for SERVICES in etcd kube-apiserver kube-controller-manager kube-scheduler; do systemctl restart $SERVICES systemctl enable $SERVICES systemctl status $SERVICES done 5.We have to Define flannel network configuration in etcd. This configuration will be pulled by flannel service on minions, it is used for internetwork communication in containers: ::\n$ etcdctl mk /atomic.io/network/config \u0026#39;{\u0026#34;Network\u0026#34;:\u0026#34;172.17.0.0/16\u0026#34;}\u0026#39; .\nSetting up Kubernetes Minions (Nodes)\nNow follow following steps to configure minion1, minion2 and minion3 .\nInstall flannel and Kubernetes using dnf: :: $ dnf -y install flannel kubernetes Update the following line in /etc/sysconfig/flanneld to connect to the respective master(via flannel service): :: FLANNEL_ETCD=\u0026#34;http://10.3.3.171:2379\u0026#34; Update KUBE_MASTER in /etc/kubernetes/config to Kubernetes master API server for connection: :: KUBE_MASTER=\u0026#34;--master=http://10.3.13.171:8080\u0026#34; 4.Configure kubelet service by editing /etc/kubernetes/kubelet on each minion as following:\nMinion1: ::\nKUBELET_ADDRESS=\u0026#34;--address=0.0.0.0\u0026#34; KUBELET_PORT=\u0026#34;--port=10250\u0026#34; # change the hostname to this hostâ€™s IP address KUBELET_HOSTNAME=\u0026#34;--hostname_override=10.3.3.172\u0026#34; KUBELET_API_SERVER=\u0026#34;--api_servers=http://10.3.3.171:8080\u0026#34; KUBELET_ARGS=\u0026#34;\u0026#34; Minion2: ::\nKUBELET_ADDRESS=\u0026#34;--address=0.0.0.0\u0026#34; KUBELET_PORT=\u0026#34;--port=10250\u0026#34; # change the hostname to this hostâ€™s IP address KUBELET_HOSTNAME=\u0026#34;--hostname_override=10.3.3.173\u0026#34; KUBELET_API_SERVER=\u0026#34;--api_servers=http://10.3.3.171:8080\u0026#34; KUBELET_ARGS=\u0026#34;\u0026#34; Minion3: ::\nKUBELET_ADDRESS=\u0026#34;--address=0.0.0.0\u0026#34; KUBELET_PORT=\u0026#34;--port=10250\u0026#34; # change the hostname to this hostâ€™s IP address KUBELET_HOSTNAME=\u0026#34;--hostname_override=10.3.3.174\u0026#34; KUBELET_API_SERVER=\u0026#34;--api_servers=http://10.3.3.171:8080\u0026#34; KUBELET_ARGS=\u0026#34;\u0026#34; We have to Start and enable kube-proxy, kubelet, docker and flanneld services on each minion: :: $ for SERVICES in kube-proxy kubelet docker flanneld; do systemctl restart $SERVICES systemctl enable $SERVICES systemctl status $SERVICES done On each minion, you can check that you will have two new interfaces added, docker0 and flannel0. You can check different range of IP addresses on flannel0 interface on each minion, you can check it by following command on each minion: :: $ ip a | grep flannel | grep inet if all the services are started correctly, Hence, Everything is set now, we can check minion status as following: ::\n$ kubectl get nodes NAME STATUS AGE 10.3.3.172 Ready 10m 10.3.3.173 Ready 12m 10.3.3.174 Ready 15m We will create a simple pod definition like this below(pod is nothing but group of containers) ::\n#pod-nginx.yaml apiVersion: v1 kind: Pod metadata : name: nginx spec : containers: - name: nginx image: nginx ports: - containerPort: 80 Create a pod containing an nginx server (pod-nginx.yaml): ::\n$ kubectl create -f ./pod-nginx.yaml We can check output at: ::\n$ curl http://$(kubectl get pod nginx -o go-template={{.status.podIP}}) To List all pods: ::\n$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 2m To replicate the container, for example: ::\napiVersion: v1 kind: ReplicationController metadata : name: nginx spec : replicas: 3 selector: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 Then pods will be listed as,(containers will be spinned on all minions in cluster way) ::\n$ sudo kubectl get pods NAME READY STATUS RESTARTS AGE nginx-1z3lw 1/1 Running 0 3m nginx-343jk 1/1 Running 0 3m nginx-s9gt8 1/1 Running 0 3m We can Delete the pod by name as following: ::\n$ kubectl delete pod nginx We can check all logs(events) as following : ::\n$ kubectl get events Thank You !!! For any query or any suggestion, Comments are most welcome !!!\n","permalink":"http://localhost:1313/blog/blog8/","summary":"Kubernetes\nKubernetes is a system, developed by google, for managing containerized applications in a clustered environment. It provides basic mechanisms for deployment, maintenance and scaling of applications on public, private or hybrid setups. It also comes with self-healing features where containers can be auto provisioned, restarted or even replicated. It aims to provide a \u0026ldquo;platform for automating deployment, scaling, and operations of application containers across clusters of hosts\u0026rdquo;\n.\nIn this blog post, weâ€™ll install a Kubernetes cluster with three minions on Fedora 23, we will also see example on how to manage pods.","title":"Installing Kubernetes Cluster with 3 minions on Fedora 23"},{"content":"Hello, after long time, I am writing for a good reason :P\nhad participated in RDO bug triage day on 18th and 19th of May, For those who dont know what is bug triage, Have a look at it: https://www.rdoproject.org/community/rdo-bugtriage/\n(I also came to know it just day before yesterday :-))\nAs a newbie to RDO community, ChandanKumar and Matthias Runge guided me to what to do there. So, I had created account on http://bugzilla.redhat.com, Then I joined #rdo, people out there also helped me in getting things done.\nSo I reviewed few bugs and then ChandanKumar told me, how to triaged it. Mostly I picked following bugs:\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1205645 https://bugzilla.redhat.com/show_bug.cgi?id=1195977 https://bugzilla.redhat.com/show_bug.cgi?id=1213545 https://bugzilla.redhat.com/show_bug.cgi?id=1212899 https://bugzilla.redhat.com/show_bug.cgi?id=1301158 Almost one bug for each component like cinder, glance, keystone and heat.\nfound one missing dependency So I got to fix that bug. So, ChandanKumar helped me to setup DLRN which maintains yum repositories of Openstack upstream (RDO). And for one bug https://bugzilla.redhat.com/show_bug.cgi?id=1212899, I have sent patch which is under review(https://review.rdoproject.org/r/#/c/1172/) right now. Once it approves, this will be my first contribution to RDO project. :-) have learnt alot in these two days as where is Openstack repositories is maintained and how dependencies are kept. Now, onwards, I will regularly participate in bug triage days. My next blog will be on how to setup DLRN to Contribute in RDO, Till then, Stay tuned. Be happy. Contribute to OPenSource. :P\n","permalink":"http://localhost:1313/blog/blog7/","summary":"Hello, after long time, I am writing for a good reason :P\nhad participated in RDO bug triage day on 18th and 19th of May, For those who dont know what is bug triage, Have a look at it: https://www.rdoproject.org/community/rdo-bugtriage/\n(I also came to know it just day before yesterday :-))\nAs a newbie to RDO community, ChandanKumar and Matthias Runge guided me to what to do there. So, I had created account on http://bugzilla.","title":"RDO BUG TRIAGE DAY - First step towards Contribution !!!"},{"content":"Data is present everywhere at malls, multiplexes, online shopping sites, dating sites, government intelligence almost everywhere that continuously needs to be monitor \u0026amp; analyse, which is very huge data, which cant be analysed by human, at that point machine learning came into picture.\nFor introduction to that #PythonPune added new chapter to its Pythonpune Meetup which was held on 30 April 2016 at Red Hat office, Magarpatta.\nAlmost 70-80 peoples were there(which was unexpected for us). :P\nVery few peoples which were present who know machine learning. Rest of people doesn\u0026rsquo;t know but interested to learn . I was also interested to learn machine learning after watching blockbuster \u0026ldquo;person of Interest\u0026rdquo;\nSpeakers of meetup were Satish Patil and Sudarshan Gadhave who gave Introduction on Machine Learning. Satish is a founder and chief data scientist of Lemoxo Technologies, where he advises companies large and small on their data strategy. He holds a Ph.D. from the University of Minnesota, USA. Over the last 4 years, Satish has helped global pharma, e-commerce giants \u0026amp; startups to solve complex data problems to drive the meaningful business outcome. His core competency lies in applied math and statistical modelling, machine learning, deep learning and data visualisation. Satish is passionate about applying math, technology, design thinking and cognitive science to better understand, predict and improve business functions. Sudarshan works @ NEC as data scientist.\nMachine learning is all about technical,business and statistics. In this session, we learnt basics of machine learning and how to implement machine learning algorithms on your data sets using Python and Scikit-Learn. also we learnt: The Black Box of Machine Learning,features,tupules,training and test data set,classification,clustering,pure \u0026amp; impure states,entropy,decision tree, supervised and unsupervised learning, market basket analysis, data pre-processing, K means algorithm. we have used titanic data sets for example (https://github.com/pcsanwald/kaggle-titanic/blob/master/train.csv).\nRequired tools/libraries for machine learning :\nnumpy python-pandas python-scikit-learn ipython ChandanKumar talked about Fedora Labs. Fedora labs have the selection of curated bundles of purpose-driven software and content as curated and maintained by members of the Fedora Community. These may be installed as standalone full versions of Fedora or as add-ons to existing Fedora installations. From fedora-labs, we were focused on Fedora Scientific (open source software computing) which comes with featured application like ipython, pandas, Gnuplot, Matplotlib, R , latex, etc can be useful for machine learning.\nTo download fedora Scientific, Click Here.\nIf you need any help, you can ping on #fedora-science channel on Freenode in IRC.\nRed Hat was sponser for food, beverages, and venue. Thanks to Satish Patil and Sudarshan Gadhave for conducting an interesting workshop! We hope to see more such workshops by you in upcoming meetups.\nThanks to ChandanKumar, Praveen Kumar for organising such interesting meetups !!!\nStill from Meetup :\nSatish Patil delivering Session on Machine Learning:\nSudarshan Gadhave Talking about Machine Learning:\nChandanKumar talking about Fedora Scientific and OpenSource Contribution:\n","permalink":"http://localhost:1313/blog/blog6/","summary":"Data is present everywhere at malls, multiplexes, online shopping sites, dating sites, government intelligence almost everywhere that continuously needs to be monitor \u0026amp; analyse, which is very huge data, which cant be analysed by human, at that point machine learning came into picture.\nFor introduction to that #PythonPune added new chapter to its Pythonpune Meetup which was held on 30 April 2016 at Red Hat office, Magarpatta.\nAlmost 70-80 peoples were there(which was unexpected for us).","title":"Machine Learning Begins Here !!! @ #pythonpune"},{"content":"April Fedora meetup was dedicated to â€œUnit testingâ€.\nAs I am into DevOps, usually i had seen various automated tests that was carried out by jenkins are nothing but unit test but i was always wondered what is this?? Kushal gave us brief idea about unit test :\nunit testing is a method by which individual units of source code, sets of one or more computer program modules together are tested to determine if they are fit for use.\nas well unit test gives you opportunity to scrutinize your own code for proper operation.\nAnd also he gave an example of unit test for â€˜catâ€™ command, for that we cloned https://github.com/kushaldas/tunirtests ( as we had to used some functions from these files )\nimport unittest from testutils import system class Firetest(unittest.TestCase): def test_cat(self): out,err,retcode= system(\u0026#34;cat /etc/fedora-release\u0026#34;) out = out.decode(\u0026#39;utf-8\u0026#39;) self.assertIn(\u0026#39;Fedora\u0026#39;,out) if __name__ == \u0026#39;__main__\u0026#39; : unittest.main() Then, Kushal and Pravin gave different test cases to each one of us, I got test case to check selinux context of file ( i.e. restorecon command ) whether we are getting same result after \u0026lsquo;restorecon\u0026rsquo; or not:\nimport unittest from testutils import system class Firetest(unittest.TestCase) : def test_selinux(self): out,err,retcode= system(\u0026#34;ls -lZ /var/www/html/index.html\u0026#34;) out = out.decode(\u0026#39;utf-8\u0026#39;) out1,err1,retcode1= system(\u0026#34;restorecon -v /var/www/html/index.html\u0026#34;) out2,err2,retcode2= system(\u0026#34;ls -lZ /var/www/html/index.html\u0026#34;) out2 = out2.decode(\u0026#39;utf-8\u0026#39;) self.assertEqual(out,out2) if __name__ == \u0026#39;__main__\u0026#39; : unittest.main() As I was new to fedora meetup, i got connected with new friends too.\nThis experiment of meetup was awesome as it was kind of interactive, rather than just one person talking and others are just listening.\nforget to mention above, we had tasty snacks and drinks too !!! :P Thank you, ChandanKumar, Kushal Das, PravinKumar and all !!! for more regarding unit tests, read here : http://pymbook.readthedocs.org/en/latest/testing.html#unit-testing\n","permalink":"http://localhost:1313/blog/blog5/","summary":"April Fedora meetup was dedicated to â€œUnit testingâ€.\nAs I am into DevOps, usually i had seen various automated tests that was carried out by jenkins are nothing but unit test but i was always wondered what is this?? Kushal gave us brief idea about unit test :\nunit testing is a method by which individual units of source code, sets of one or more computer program modules together are tested to determine if they are fit for use.","title":"April Fedora Meetup - Unit Testing Revealed !!!"},{"content":"Yesterday, I was sending patch for review on gerrit, I commmited the code as follows, Everything works fine ;) ,\n[user@localhost project]$ git commit --amend [project d9593ab] \u0026lt;commit message here\u0026gt; Date: Thu Apr 14 17:34:53 2016 +0530 1. file changed, 75 insertions(+) create mode 100644 project/file.py While I was pushing code to gerrit for review, Suddenly Heavy errorfall occurs with :o following error:\n[user@localhost project]$ git review develop remote: Processing changes: refs: 1, done. To ssh://user@xxxxxxxxxxxxxxxxxxxxxxxx.com:22/project ! [remote rejected] HEAD -\u0026gt; refs/publish/develop/project (not Signed-off-by author/committer/uploader in commit message footer) error: failed to push some refs to \u0026#39;ssh://user@xxxxxxxxxxxxxxxxxxxxxxxxx.com:22/project\u0026#39; After trying different methods for half an hour, we have checked commit which was as below:\n\u0026lt;commit messgae here\u0026gt; Change-Id: yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy # Please enter the commit message for your changes. Lines starting # with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. # # Date: Thu Apr 14 17:34:53 2016 +0530 # # On branch project # Your branch is ahead of \u0026#39;origin/develop\u0026#39; by 1 commit. # (use \u0026#34;git push\u0026#34; to publish your local commits) # # Changes to be committed: # new file: project/file.py After that we had removed Change-Id for patch set and added Signed-off-by line along with username and email id as shown below:\n\u0026lt;commit message here\u0026gt; Signed-off-by: username \u0026lt;userid@xyz.com\u0026gt; # Please enter the commit message for your changes. Lines starting # with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. # # Date: Thu Apr 14 17:34:53 2016 +0530 # # On branch project # Your branch is ahead of \u0026#39;origin/develop\u0026#39; by 1 commit. # (use \u0026#34;git push\u0026#34; to publish your local commits) # # Changes to be committed: # new file: project/file.py And yippee, it works !!! :-) :-)\n[user@localhost project]$ git review develop remote: Processing changes: new: 1, refs: 1, done remote: remote: New Changes: remote: https://xxxxxxxxxxxxxxxxxxxx.com/gerrit/00000 \u0026lt;commit message here\u0026gt; remote: To ssh://user@xxxxxxxxxxxxxxxxxxxxxx.com:22/project - [new branch] HEAD -\u0026gt; refs/publish/develop/project Thank you, ChandanKumar ++\n","permalink":"http://localhost:1313/blog/blog4/","summary":"Yesterday, I was sending patch for review on gerrit, I commmited the code as follows, Everything works fine ;) ,\n[user@localhost project]$ git commit --amend [project d9593ab] \u0026lt;commit message here\u0026gt; Date: Thu Apr 14 17:34:53 2016 +0530 1. file changed, 75 insertions(+) create mode 100644 project/file.py While I was pushing code to gerrit for review, Suddenly Heavy errorfall occurs with :o following error:\n[user@localhost project]$ git review develop remote: Processing changes: refs: 1, done.","title":"Gerrit Error not Signed-off-by author/........ Resolved"},{"content":"I was listening to song â€˜We love Raleighâ€™ by Brian Totty (so-called Red Hat anthem) :P. â€˜So amazing when youâ€™re riding on a ray of sunshine now youâ€™re feeling like you might be einsteinâ€™ Almost more than one month is spent at Red hat, being surrounded by all OpenSource contributors, peoples dedicated to technologies are here. So, things are becoming cool \u0026amp; awesome, I got to know new things every single day !!!\nThen I got a task to create Jenkins job using JJb for the unit test using nosetest, pylint tests, I got stuck with one issue regarding master-slave handshake whereas Rishabh gave me only hint EXPLORE THE SLAVE that was pretty awesome one. only one hint lead to clear all concepts around it. Thanks Rishabh for it.\non 1st march, Communication guidelines session was taken by Kushal das.In this session , He told us about Netiquettes and other communication guidelines while having conversation on IRC or mailing list.\nI also read \u0026ldquo;2015 Open Source Yearbook\u0026rdquo; in free time. I got lots of ways to use ownCloud in creative ways, awesome Open hardware projects like Raspberry projects. Also, I got to know that even facebook also OpenSourcing many of their projects like HHVM, React, RocksDB, etc. I suggest you, go through it.\nFriday, 11 March, I had attended Tech Talk on Jenkins Job Builder by Anisha Narang, It was quite useful as she spoke about macros in Jenkins as well how to use the various plugins that was very useful in my projects. Thank you, Anisha :)\nIn those days, I had read Ansible thoroughly Then I wrote ansible playbook , I found ansible quite beautiful tool for automation as well as it\u0026rsquo;s very simple to use.\nTuesday 15th march, Delightful Pizza was there for Lunch, \u0026amp; reason was for the farewell of Robin Cain (USA), she also shared her past experiences with us and also guided us for our next journey.\nIn mid of March, Jenkins gates were down somehow :( :( nothing gonna worked for few days. So those days, I worked on Pelican, ansible, JJB \u0026amp; tasty Raspberry Pi ;-) :p\n26 March, I was unable to attend python meetup because I had to travel to my hometown but, fortunately, I got lucky to Attend Docker birthday \u0026amp; meetup at Unnati Computers, Aurangabad. Pushpendra Chavan told briefly about docker, containers and Ashutosh Sir gave a demo of how to run a webserver using docker.\n31 march, Open Source Session was organized by Kushal Das, Rupali Talwatkar. Kushal Das, Pravin Kumar, Chandan Kumar and Rupali Talwatkar shared their experiences that how they started contribution to Opensource with us which were very much motivating. they had shared very basic things like what is FAS account, how to open it, how to find your project for contribution. Kushal also told about why the blog is important and how it can be beneficial to maintain digital identity.\nI heard one good quote: Wissen ist Macht. (Knowledge is power) (obviously deutsch ;-) )\n","permalink":"http://localhost:1313/blog/blog3/","summary":"I was listening to song â€˜We love Raleighâ€™ by Brian Totty (so-called Red Hat anthem) :P. â€˜So amazing when youâ€™re riding on a ray of sunshine now youâ€™re feeling like you might be einsteinâ€™ Almost more than one month is spent at Red hat, being surrounded by all OpenSource contributors, peoples dedicated to technologies are here. So, things are becoming cool \u0026amp; awesome, I got to know new things every single day !","title":"So amazing when you're riding on a ray of sunshine....."},{"content":"Finally Interns family completed !\nIt was 8 February, I came at office bit early ;) .\nSeema welcomed us(new joinees) \u0026amp; took care of all documentation process. Then, Manoj takes us for tour of whole office. It was awesome( Especially 5th floor) and then introduced me to Manager Devang Parikh, Mentor Rishabh Das and Team Lead Shrink ;)\nThen, Rishabh helped me to setup all things \u0026amp; told me about workflow in brief \u0026amp; also about CI/CD, Jenkins, Ansible, Openstack etc., it was completely new for me ( except Openstack). Surprisingly I got know about kick-off meeting and excel sheet with burger list :O :P \u0026amp; this is how first day ended with kick off with delicious burgers. At second day, there was introduction with team member,(again with surprise) delicious donuts :P Shrink introduced me to team members Rohan, Kaustubh, Neeti, Swati, Suprith, Nidhi \u0026amp; ChandanKumar\nMy interns colleagues also helped me a lot to tune up with environment Ganesh Kadam, Sudhir Verma, Snehal Karale and Ankush Behl, helped me to set up IRC \u0026amp; other stuff, I got know about pymbook, automate the boring stuff and learn python the hard way to brush up python programming.\nFriday 19th February 2016, I also attended \u0026ldquo;OpenStack Nova component - 2\u0026rdquo; ET Tech Talk Session by Pratik Bandarkar In that session, He gave more insights on Nova Operations like offline and online migration, evacuation, some known issues, troubleshooting, and future features.\nPrimarily, I have learnt things like Jenkins job management using web interface as well with JJB, Master slave setup on openstack. Based on things that I learn, I have done those tasks:\n1.Jenkins slave setup(Installed Jenkins and added an OpenStack Instance as a slave to Jenkins) 2.Created jenkins job to run pylint on git repo.\nCreated jenkins job to run unittest on git repo. Everyday is full of surprise, someday is terrible someday is incredible but one thing is common I learn something every single day and that is the most awesome thing. One interesting thing I learnt here is if you do things on your own, you will fail for some time it will take some time but nothing is impossible,\nThat reminds me one german proverb : Aus Schaden wird man klug. (Failure makes smart)\n","permalink":"http://localhost:1313/blog/blog2/","summary":"Finally Interns family completed !\nIt was 8 February, I came at office bit early ;) .\nSeema welcomed us(new joinees) \u0026amp; took care of all documentation process. Then, Manoj takes us for tour of whole office. It was awesome( Especially 5th floor) and then introduced me to Manager Devang Parikh, Mentor Rishabh Das and Team Lead Shrink ;)\nThen, Rishabh helped me to setup all things \u0026amp; told me about workflow in brief \u0026amp; also about CI/CD, Jenkins, Ansible, Openstack etc.","title":"Finally Interns family completed !"},{"content":"On 3rd January 2016, I was practising in my institute for my RHCSA in OpenStack certification, it was around 11:45 pm and suddenly, My Instructor Ashutosh Sir told me regarding internship interview at Red Hat on 10th January. I started preparing for interview immediately . (First Job interview, little bit tension, few days left)\nSurprisingly, there was a call from Red Hat ( First Telephonic interview by Rishabh Das) on 7 Jan 2016 in morning, for a while I thought it was from GSS team, I cracked it and I got script( it was shell \u0026amp; python script) to write for second round of screening, I completed it and submitted on next day itself(one day before deadline because there was exam scheduled on next day)\nOn 9th of January, My RHCSA in OpenStack exam was there I was bit worried because I have to come to Pune for internship interview process on 10th January conducted by GSS team.\nI cleared both Aptitude and group discussion round and then interview was taken by Piyush Yaduvanshi (I came to know about my exam result just before entering room for interview, was happy because no matter what happens there, but I was Red Hat certified System Administrator in OpenStack now :P) , \u0026amp; was awesome!\nOn 11 January, Again Rishabh interviewed me regarding script that what I wrote for second round and day was gone in worry worry worry\u0026hellip;\nOn next day morning, Soni Lonari called me and told that I was selected and should join from 8 February, immediately I asked my friend (Abhijeet Sadawarte-ATSE) who is in GSS team that will I get Red Hat email-id and then Happiness was that I can never explain\u0026hellip;and journey beginsâ€¦\nAus den Augen, Aus dem Sinn. (Out of sight, out of mind)\n","permalink":"http://localhost:1313/blog/blog1/","summary":"On 3rd January 2016, I was practising in my institute for my RHCSA in OpenStack certification, it was around 11:45 pm and suddenly, My Instructor Ashutosh Sir told me regarding internship interview at Red Hat on 10th January. I started preparing for interview immediately . (First Job interview, little bit tension, few days left)\nSurprisingly, there was a call from Red Hat ( First Telephonic interview by Rishabh Das) on 7 Jan 2016 in morning, for a while I thought it was from GSS team, I cracked it and I got script( it was shell \u0026amp; python script) to write for second round of screening, I completed it and submitted on next day itself(one day before deadline because there was exam scheduled on next day)","title":"Happiness is when @gmail.com changing to @redhat.com"},{"content":"Bio I am a Platform Engineer at Natwest Boxed, where I work on AWS, Kubernetes and cloud native stuff.\nReach out to me Social Media Handle Twitter @surajincloud YouTube @surajincloud LinkedIn surajnarwade Github surajnarwade Linktree surajincloud Email surajincloud.service@gmail.com ","permalink":"http://localhost:1313/about/","summary":"Bio I am a Platform Engineer at Natwest Boxed, where I work on AWS, Kubernetes and cloud native stuff.\nReach out to me Social Media Handle Twitter @surajincloud YouTube @surajincloud LinkedIn surajnarwade Github surajnarwade Linktree surajincloud Email surajincloud.service@gmail.com ","title":"About me"}]